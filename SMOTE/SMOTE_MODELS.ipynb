{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "426A8UYY22we"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSo9kPdR24O-",
        "outputId": "52bf555f-21ad-4eea-8938-eb9997e2ec03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting KNN analysis with SMOTE for 3-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
            "Best Parameters with SMOTE: {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance'}\n",
            "Best F1 Score from cross-validation: 0.6041771094402673\n",
            "Per-fold metrics saved to: smote-results/knn-smote-fold-results-3-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.5318077803203661\n",
            "Final Recall: 0.7041666666666666\n",
            "Final Accuracy: 0.857062706270627\n",
            "Final F1 Score: 0.6041771094402673\n",
            "Final MCC: 0.5281142717308954\n",
            "KNN analysis completed for 3-folds with SMOTE. Results saved to: smote-results/knn-results-3-folds.csv\n",
            "Starting KNN analysis with SMOTE for 5-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
            "Best Parameters with SMOTE: {'knn__metric': 'cosine', 'knn__n_neighbors': 3, 'knn__weights': 'distance'}\n",
            "Best F1 Score from cross-validation: 0.566845388365725\n",
            "Per-fold metrics saved to: smote-results/knn-smote-fold-results-5-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.4680361305361306\n",
            "Final Recall: 0.7466666666666667\n",
            "Final Accuracy: 0.8239344262295081\n",
            "Final F1 Score: 0.566845388365725\n",
            "Final MCC: 0.49252268091839924\n",
            "KNN analysis completed for 5-folds with SMOTE. Results saved to: smote-results/knn-results-5-folds.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (make_scorer, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef)\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "###############################################################################\n",
        "# Utility functions\n",
        "\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def getDataPoints(path):\n",
        "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
        "    dataPointsList = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory does not exist: {path}\")\n",
        "        return dataPointsList\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dataPointName in files:\n",
        "            if dataPointName.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, dataPointName)\n",
        "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
        "                    dp = fileIn.read().strip()\n",
        "                    if dp:  # Ensure the document is not empty\n",
        "                        dataPointsList.append(dp)\n",
        "                    else:\n",
        "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
        "\n",
        "    if len(dataPointsList) == 0:\n",
        "        print(f\"No valid documents found in directory: {path}\")\n",
        "\n",
        "    return dataPointsList\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"Plots confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Flaky', 'Flaky'], yticklabels=['Non-Flaky', 'Flaky'])\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Main Function with Pipeline and GridSearchCV for KNN\n",
        "\n",
        "def flastKNNWithPipeline(outDir, flakyZip, nonFlakyZip, extractDir, n_splits):\n",
        "    v0 = time.perf_counter()\n",
        "\n",
        "    # Extract the zip files\n",
        "    flakyDir = os.path.join(extractDir, 'flaky')\n",
        "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
        "    os.makedirs(flakyDir, exist_ok=True)\n",
        "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
        "\n",
        "    extract_zip(flakyZip, flakyDir)\n",
        "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
        "\n",
        "    dataPointsFlaky = getDataPoints(flakyDir)\n",
        "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
        "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
        "\n",
        "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
        "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
        "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
        "\n",
        "    if len(dataPoints) == 0:\n",
        "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
        "\n",
        "    # Create labels: 1 for flaky, 0 for non-flaky\n",
        "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
        "\n",
        "    vecTime = time.perf_counter() - v0\n",
        "\n",
        "    scoring = {\n",
        "        'precision': make_scorer(precision_score, zero_division=1),\n",
        "        'recall': make_scorer(recall_score, zero_division=1),\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'f1': make_scorer(f1_score, zero_division=1),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Define a pipeline with CountVectorizer, SMOTE, and KNN\n",
        "    pipeline = ImbPipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
        "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
        "        ('knn', KNeighborsClassifier())                    # KNN classifier\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'knn__n_neighbors': [3, 5, 7, 9, 11, 15, 20],            # Number of neighbors for KNN\n",
        "        'knn__weights': ['uniform', 'distance'],                 # Uniform or distance-based weighting\n",
        "        'knn__metric': ['euclidean', 'cosine']                   # Distance metrics\n",
        "    }\n",
        "\n",
        "    # Setup cross-validation strategy\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setup GridSearchCV with the pipeline and parameter grid\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
        "\n",
        "    # Fit the GridSearchCV to the data\n",
        "    grid_search.fit(dataPoints, dataLabelsList)\n",
        "\n",
        "    # Step 1: Retrieve the best parameters and score from cross-validation\n",
        "    best_params = grid_search.best_params_\n",
        "    best_f1_cv = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
        "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
        "\n",
        "    # Step 2: Extract the cross-validation results and print final metrics\n",
        "    results = grid_search.cv_results_\n",
        "\n",
        "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
        "    fold_metrics = []\n",
        "    for idx in range(len(results['params'])):\n",
        "      fold_metrics.append({\n",
        "        'n_neighbors': results['params'][idx].get('knn__n_neighbors'),\n",
        "        'weights': results['params'][idx].get('knn__weights'),\n",
        "        'metric': results['params'][idx].get('knn__metric'),\n",
        "        'accuracy': results['mean_test_accuracy'][idx],\n",
        "        'precision': results['mean_test_precision'][idx],\n",
        "        'recall': results['mean_test_recall'][idx],\n",
        "        'f1': results['mean_test_f1'][idx],\n",
        "        'mcc': results['mean_test_mcc'][idx],\n",
        "        'preparationTime': vecTime / len(dataPoints)  # Calculate the preparation time per document\n",
        "    })\n",
        "\n",
        "    # Save fold-wise metrics to CSV\n",
        "    df_folds = pd.DataFrame(fold_metrics)\n",
        "    outFile = os.path.join(outDir, f\"knn-smote-fold-results-{n_splits}-folds.csv\")\n",
        "    df_folds.to_csv(outFile, index=False)\n",
        "\n",
        "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
        "\n",
        "    # Step 3: Extract final metrics based on the cross-validation results\n",
        "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
        "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
        "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
        "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
        "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
        "\n",
        "    # Step 4: Print final metrics (cross-validation averages)\n",
        "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
        "    print(f\"Final Precision: {final_precision}\")\n",
        "    print(f\"Final Recall: {final_recall}\")\n",
        "    print(f\"Final Accuracy: {final_accuracy}\")\n",
        "    print(f\"Final F1 Score: {final_f1}\")\n",
        "    print(f\"Final MCC: {final_mcc}\")\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    outFile = os.path.join(outDir, f\"knn-results-{n_splits}-folds.csv\")\n",
        "    with open(outFile, \"w\") as f:\n",
        "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")\n",
        "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")\n",
        "\n",
        "    print(f\"KNN analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
        "\n",
        "###############################################################################\n",
        "# Main Execution for Both 3-Fold and 5-Fold\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters setup\n",
        "    flakyZip = \"cleaned_flaky_files.zip\"\n",
        "    largerNonFlakyZip = \"all_nonflaky_files.zip\"\n",
        "\n",
        "    outDir = \"smote-results\"\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    extractDir = \"smote-extracted\"\n",
        "    os.makedirs(extractDir, exist_ok=True)\n",
        "\n",
        "    # Run KNN with Pipeline and GridSearchCV using 3-fold cross-validation\n",
        "    print(\"Starting KNN analysis with SMOTE for 3-fold cross-validation...\")\n",
        "    flastKNNWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=3)\n",
        "\n",
        "    # Run KNN with Pipeline and GridSearchCV using 5-fold cross-validation\n",
        "    print(\"Starting KNN analysis with SMOTE for 5-fold cross-validation...\")\n",
        "    flastKNNWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgawEyBJ1TwQ"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Iv6Zd64eTyU",
        "outputId": "6ec34e92-0351-4a5e-9fdc-0824f05778e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SVM analysis with SMOTE for 3-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Best Parameters with SMOTE: {'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
            "Best F1 Score from cross-validation: 0.7206861239119303\n",
            "Per-fold metrics saved to: smote-results/svm-smote-fold-results-3-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.8680555555555555\n",
            "Final Recall: 0.6402777777777778\n",
            "Final Accuracy: 0.9235313531353135\n",
            "Final F1 Score: 0.7206861239119303\n",
            "Final MCC: 0.698389106490695\n",
            "SVM analysis completed for 3-folds with SMOTE. Results saved to: smote-results/svm-results-3-folds.csv\n",
            "Starting SVM analysis with SMOTE for 5-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Parameters with SMOTE: {'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
            "Best F1 Score from cross-validation: 0.6963741366992141\n",
            "Per-fold metrics saved to: smote-results/svm-smote-fold-results-5-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.8392857142857142\n",
            "Final Recall: 0.611111111111111\n",
            "Final Accuracy: 0.9202732240437157\n",
            "Final F1 Score: 0.6963741366992141\n",
            "Final MCC: 0.6691593384052171\n",
            "SVM analysis completed for 5-folds with SMOTE. Results saved to: smote-results/svm-results-5-folds.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (make_scorer, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef)\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "###############################################################################\n",
        "# Utility functions\n",
        "\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def getDataPoints(path):\n",
        "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
        "    dataPointsList = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory does not exist: {path}\")\n",
        "        return dataPointsList\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dataPointName in files:\n",
        "            if dataPointName.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, dataPointName)\n",
        "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
        "                    dp = fileIn.read().strip()\n",
        "                    if dp:  # Ensure the document is not empty\n",
        "                        dataPointsList.append(dp)\n",
        "                    else:\n",
        "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
        "\n",
        "    if len(dataPointsList) == 0:\n",
        "        print(f\"No valid documents found in directory: {path}\")\n",
        "\n",
        "    return dataPointsList\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"Plots confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Flaky', 'Flaky'], yticklabels=['Non-Flaky', 'Flaky'])\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Main Function with Pipeline and GridSearchCV for SVM\n",
        "\n",
        "def flastSVMWithPipeline(outDir, flakyZip, nonFlakyZip, extractDir, n_splits):\n",
        "    v0 = time.perf_counter()\n",
        "\n",
        "    # Extract the zip files\n",
        "    flakyDir = os.path.join(extractDir, 'flaky')\n",
        "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
        "    os.makedirs(flakyDir, exist_ok=True)\n",
        "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
        "\n",
        "    extract_zip(flakyZip, flakyDir)\n",
        "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
        "\n",
        "    dataPointsFlaky = getDataPoints(flakyDir)\n",
        "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
        "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
        "\n",
        "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
        "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
        "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
        "\n",
        "    if len(dataPoints) == 0:\n",
        "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
        "\n",
        "    # Create labels: 1 for flaky, 0 for non-flaky\n",
        "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
        "\n",
        "    vecTime = time.perf_counter() - v0\n",
        "\n",
        "    scoring = {\n",
        "        'precision': make_scorer(precision_score, zero_division=1),\n",
        "        'recall': make_scorer(recall_score, zero_division=1),\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'f1': make_scorer(f1_score, zero_division=1),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Define a pipeline with CountVectorizer, SMOTE, and SVM\n",
        "    pipeline = ImbPipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
        "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
        "        ('svm', SVC(probability=True))                     # SVM classifier\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],                          # Regularization parameter\n",
        "        'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid']               # Kernel types\n",
        "    }\n",
        "\n",
        "    # Setup cross-validation strategy\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setup GridSearchCV with the pipeline and parameter grid\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
        "\n",
        "    # Fit the GridSearchCV to the data\n",
        "    grid_search.fit(dataPoints, dataLabelsList)\n",
        "\n",
        "    # Step 1: Retrieve the best parameters and score from cross-validation\n",
        "    best_params = grid_search.best_params_\n",
        "    best_f1_cv = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
        "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
        "\n",
        "    # Step 2: Extract the cross-validation results and print final metrics\n",
        "    results = grid_search.cv_results_\n",
        "\n",
        "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
        "    fold_metrics = []\n",
        "    for idx in range(len(results['params'])):\n",
        "      fold_metrics.append({\n",
        "        'C': results['params'][idx].get('svm__C'),\n",
        "        'kernel': results['params'][idx].get('svm__kernel'),\n",
        "        'accuracy': results['mean_test_accuracy'][idx],\n",
        "        'precision': results['mean_test_precision'][idx],\n",
        "        'recall': results['mean_test_recall'][idx],\n",
        "        'f1': results['mean_test_f1'][idx],\n",
        "        'mcc': results['mean_test_mcc'][idx],\n",
        "        'preparationTime': vecTime / len(dataPoints)  # Calculate the preparation time per document\n",
        "    })\n",
        "\n",
        "    # Save fold-wise metrics to CSV\n",
        "    df_folds = pd.DataFrame(fold_metrics)\n",
        "    outFile = os.path.join(outDir, f\"svm-smote-fold-results-{n_splits}-folds.csv\")\n",
        "    df_folds.to_csv(outFile, index=False)\n",
        "\n",
        "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
        "\n",
        "    # Step 3: Extract final metrics based on the cross-validation results\n",
        "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
        "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
        "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
        "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
        "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
        "\n",
        "    # Step 4: Print final metrics (cross-validation averages)\n",
        "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
        "    print(f\"Final Precision: {final_precision}\")\n",
        "    print(f\"Final Recall: {final_recall}\")\n",
        "    print(f\"Final Accuracy: {final_accuracy}\")\n",
        "    print(f\"Final F1 Score: {final_f1}\")\n",
        "    print(f\"Final MCC: {final_mcc}\")\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    outFile = os.path.join(outDir, f\"svm-results-{n_splits}-folds.csv\")\n",
        "    with open(outFile, \"w\") as f:\n",
        "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")\n",
        "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")\n",
        "\n",
        "    print(f\"SVM analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
        "\n",
        "###############################################################################\n",
        "# Main Execution for Both 3-Fold and 5-Fold\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters setup\n",
        "    flakyZip = \"cleaned_flaky_files.zip\"\n",
        "    largerNonFlakyZip = \"all_nonflaky_files.zip\"\n",
        "\n",
        "    outDir = \"smote-results\"\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    extractDir = \"smote-extracted\"\n",
        "    os.makedirs(extractDir, exist_ok=True)\n",
        "\n",
        "    # Run SVM with Pipeline and GridSearchCV using 3-fold cross-validation\n",
        "    print(\"Starting SVM analysis with SMOTE for 3-fold cross-validation...\")\n",
        "    flastSVMWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=3)\n",
        "\n",
        "    # Run SVM with Pipeline and GridSearchCV using 5-fold cross-validation\n",
        "    print(\"Starting SVM analysis with SMOTE for 5-fold cross-validation...\")\n",
        "    flastSVMWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XVnvyIqwWY"
      },
      "source": [
        "NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4RrJe6hqxpN",
        "outputId": "ba6e04a9-3f9e-4634-ee47-f75a86022ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Naive Bayes analysis with SMOTE for 3-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "Best Parameters with SMOTE: {'nb__alpha': 1.0}\n",
            "Best F1 Score from cross-validation: 0.6724137931034483\n",
            "Per-fold metrics saved to: smote-results/nb-smote-fold-results-3-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.7402319902319903\n",
            "Final Recall: 0.6180555555555555\n",
            "Final Accuracy: 0.9068316831683169\n",
            "Final F1 Score: 0.6724137931034483\n",
            "Final MCC: 0.6229824203893163\n",
            "Naive Bayes analysis completed for 3-folds with SMOTE. Results saved to: smote-results/nb-results-3-folds.csv\n",
            "Starting Naive Bayes analysis with SMOTE for 5-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Parameters with SMOTE: {'nb__alpha': 0.1}\n",
            "Best F1 Score from cross-validation: 0.6892436974789915\n",
            "Per-fold metrics saved to: smote-results/nb-smote-fold-results-5-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.7714285714285715\n",
            "Final Recall: 0.6333333333333334\n",
            "Final Accuracy: 0.9169398907103826\n",
            "Final F1 Score: 0.6892436974789915\n",
            "Final MCC: 0.6504601760622541\n",
            "Naive Bayes analysis completed for 5-folds with SMOTE. Results saved to: smote-results/nb-results-5-folds.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import (make_scorer, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef)\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "###############################################################################\n",
        "# Utility functions\n",
        "\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def getDataPoints(path):\n",
        "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
        "    dataPointsList = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory does not exist: {path}\")\n",
        "        return dataPointsList\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dataPointName in files:\n",
        "            if dataPointName.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, dataPointName)\n",
        "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
        "                    dp = fileIn.read().strip()\n",
        "                    if dp:  # Ensure the document is not empty\n",
        "                        dataPointsList.append(dp)\n",
        "                    else:\n",
        "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
        "\n",
        "    if len(dataPointsList) == 0:\n",
        "        print(f\"No valid documents found in directory: {path}\")\n",
        "\n",
        "    return dataPointsList\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"Plots confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Flaky', 'Flaky'], yticklabels=['Non-Flaky', 'Flaky'])\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Main Function with Pipeline and GridSearchCV for Naive Bayes\n",
        "\n",
        "def flastNBWithPipeline(outDir, flakyZip, nonFlakyZip, extractDir, n_splits):\n",
        "    v0 = time.perf_counter()\n",
        "\n",
        "    # Extract the zip files\n",
        "    flakyDir = os.path.join(extractDir, 'flaky')\n",
        "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
        "    os.makedirs(flakyDir, exist_ok=True)\n",
        "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
        "\n",
        "    extract_zip(flakyZip, flakyDir)\n",
        "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
        "\n",
        "    dataPointsFlaky = getDataPoints(flakyDir)\n",
        "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
        "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
        "\n",
        "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
        "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
        "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
        "\n",
        "    if len(dataPoints) == 0:\n",
        "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
        "\n",
        "    # Create labels: 1 for flaky, 0 for non-flaky\n",
        "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
        "\n",
        "    vecTime = time.perf_counter() - v0\n",
        "\n",
        "    scoring = {\n",
        "        'precision': make_scorer(precision_score, zero_division=1),\n",
        "        'recall': make_scorer(recall_score, zero_division=1),\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'f1': make_scorer(f1_score, zero_division=1),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Define a pipeline with CountVectorizer, SMOTE, and Naive Bayes\n",
        "    pipeline = ImbPipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
        "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
        "        ('nb', MultinomialNB())                            # Naive Bayes classifier\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'nb__alpha': [0.001, 0.01, 0.1, 1.0, 10.0]          # Smoothing parameter for Naive Bayes\n",
        "    }\n",
        "\n",
        "    # Setup cross-validation strategy\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setup GridSearchCV with the pipeline and parameter grid\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
        "\n",
        "    # Fit the GridSearchCV to the data\n",
        "    grid_search.fit(dataPoints, dataLabelsList)\n",
        "\n",
        "    # Step 1: Retrieve the best parameters and score from cross-validation\n",
        "    best_params = grid_search.best_params_\n",
        "    best_f1_cv = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
        "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
        "\n",
        "    # Step 2: Extract the cross-validation results and print final metrics\n",
        "    results = grid_search.cv_results_\n",
        "\n",
        "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
        "    fold_metrics = []\n",
        "    for idx in range(len(results['params'])):\n",
        "      fold_metrics.append({\n",
        "        'alpha': results['params'][idx].get('nb__alpha'),  # Get the 'alpha' parameter for Naive Bayes\n",
        "        'accuracy': results['mean_test_accuracy'][idx],\n",
        "        'precision': results['mean_test_precision'][idx],\n",
        "        'recall': results['mean_test_recall'][idx],\n",
        "        'f1': results['mean_test_f1'][idx],\n",
        "        'mcc': results['mean_test_mcc'][idx],\n",
        "        'preparationTime': vecTime / len(dataPoints)  # Calculate the preparation time per document\n",
        "    })\n",
        "\n",
        "    # Save fold-wise metrics to CSV\n",
        "    df_folds = pd.DataFrame(fold_metrics)\n",
        "    outFile = os.path.join(outDir, f\"nb-smote-fold-results-{n_splits}-folds.csv\")\n",
        "    df_folds.to_csv(outFile, index=False)\n",
        "\n",
        "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
        "\n",
        "    # Step 3: Extract final metrics based on the cross-validation results\n",
        "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
        "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
        "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
        "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
        "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
        "\n",
        "    # Step 4: Print final metrics (cross-validation averages)\n",
        "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
        "    print(f\"Final Precision: {final_precision}\")\n",
        "    print(f\"Final Recall: {final_recall}\")\n",
        "    print(f\"Final Accuracy: {final_accuracy}\")\n",
        "    print(f\"Final F1 Score: {final_f1}\")\n",
        "    print(f\"Final MCC: {final_mcc}\")\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    outFile = os.path.join(outDir, f\"nb-results-{n_splits}-folds.csv\")\n",
        "    with open(outFile, \"w\") as f:\n",
        "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")\n",
        "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")\n",
        "\n",
        "    print(f\"Naive Bayes analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
        "\n",
        "###############################################################################\n",
        "# Main Execution for Both 3-Fold and 5-Fold\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters setup\n",
        "    flakyZip = \"cleaned_flaky_files.zip\"\n",
        "    largerNonFlakyZip = \"all_nonflaky_files.zip\"\n",
        "\n",
        "    outDir = \"smote-results\"\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    extractDir = \"smote-extracted\"\n",
        "    os.makedirs(extractDir, exist_ok=True)\n",
        "\n",
        "    # Run Naive Bayes with Pipeline and GridSearchCV using 3-fold cross-validation\n",
        "    print(\"Starting Naive Bayes analysis with SMOTE for 3-fold cross-validation...\")\n",
        "    flastNBWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=3)\n",
        "\n",
        "    # Run Naive Bayes with Pipeline and GridSearchCV using 5-fold cross-validation\n",
        "    print(\"Starting Naive Bayes analysis with SMOTE for 5-fold cross-validation...\")\n",
        "    flastNBWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsuiaiIrrEoB"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpfQzUp1rLhW",
        "outputId": "e8138e44-3707-4e96-c302-5ab4dfda4edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Decision Tree analysis with SMOTE for 3-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
            "Best Parameters with SMOTE: {'dt__criterion': 'gini', 'dt__max_depth': None, 'dt__max_features': None, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 2}\n",
            "Best F1 Score from cross-validation: 0.8915925101965146\n",
            "Per-fold metrics saved to: smote-results/dt-smote-fold-results-3-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.9166666666666666\n",
            "Final Recall: 0.8736111111111112\n",
            "Final Accuracy: 0.9667986798679867\n",
            "Final F1 Score: 0.8915925101965146\n",
            "Final MCC: 0.8744409272444104\n",
            "Decision Tree analysis completed for 3-folds with SMOTE. Results saved to: smote-results/dt-results-3-folds.csv\n",
            "Starting Decision Tree analysis with SMOTE for 5-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n",
            "Best Parameters with SMOTE: {'dt__criterion': 'gini', 'dt__max_depth': None, 'dt__max_features': None, 'dt__min_samples_leaf': 1, 'dt__min_samples_split': 2}\n",
            "Best F1 Score from cross-validation: 0.8918716577540108\n",
            "Per-fold metrics saved to: smote-results/dt-smote-fold-results-5-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.93\n",
            "Final Recall: 0.8711111111111112\n",
            "Final Accuracy: 0.966775956284153\n",
            "Final F1 Score: 0.8918716577540108\n",
            "Final MCC: 0.8780415116902793\n",
            "Decision Tree analysis completed for 5-folds with SMOTE. Results saved to: smote-results/dt-results-5-folds.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (make_scorer, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef)\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "###############################################################################\n",
        "# Utility functions\n",
        "\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def getDataPoints(path):\n",
        "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
        "    dataPointsList = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory does not exist: {path}\")\n",
        "        return dataPointsList\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dataPointName in files:\n",
        "            if dataPointName.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, dataPointName)\n",
        "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
        "                    dp = fileIn.read().strip()\n",
        "                    if dp:  # Ensure the document is not empty\n",
        "                        dataPointsList.append(dp)\n",
        "                    else:\n",
        "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
        "\n",
        "    if len(dataPointsList) == 0:\n",
        "        print(f\"No valid documents found in directory: {path}\")\n",
        "\n",
        "    return dataPointsList\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"Plots confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Flaky', 'Flaky'], yticklabels=['Non-Flaky', 'Flaky'])\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Main Function with Pipeline and GridSearchCV for Decision Tree\n",
        "\n",
        "def flastDTWithPipeline(outDir, flakyZip, nonFlakyZip, extractDir, n_splits):\n",
        "    v0 = time.perf_counter()\n",
        "\n",
        "    # Extract the zip files\n",
        "    flakyDir = os.path.join(extractDir, 'flaky')\n",
        "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
        "    os.makedirs(flakyDir, exist_ok=True)\n",
        "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
        "\n",
        "    extract_zip(flakyZip, flakyDir)\n",
        "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
        "\n",
        "    dataPointsFlaky = getDataPoints(flakyDir)\n",
        "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
        "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
        "\n",
        "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
        "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
        "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
        "\n",
        "    if len(dataPoints) == 0:\n",
        "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
        "\n",
        "    # Create labels: 1 for flaky, 0 for non-flaky\n",
        "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
        "\n",
        "    vecTime = time.perf_counter() - v0\n",
        "\n",
        "    scoring = {\n",
        "        'precision': make_scorer(precision_score, zero_division=1),\n",
        "        'recall': make_scorer(recall_score, zero_division=1),\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'f1': make_scorer(f1_score, zero_division=1),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Define a pipeline with CountVectorizer, SMOTE, and Decision Tree\n",
        "    pipeline = ImbPipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
        "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
        "        ('dt', DecisionTreeClassifier(random_state=42))    # Decision Tree classifier\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'dt__criterion': ['gini', 'entropy'],               # Function to measure the quality of a split\n",
        "        'dt__max_depth': [None, 10, 30, 50, 100],       # Maximum depth of the tree\n",
        "        'dt__min_samples_split': [2, 5, 10],                    # Minimum number of samples required to split a node\n",
        "        'dt__min_samples_leaf': [1, 2, 5, 10],                     # Minimum number of samples required to be at a leaf node\n",
        "        'dt__max_features': [None, 'sqrt', 'log2']          # Number of features to consider when looking for the best split\n",
        "    }\n",
        "\n",
        "    # Setup cross-validation strategy\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setup GridSearchCV with the pipeline and parameter grid\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
        "\n",
        "    # Fit the GridSearchCV to the data\n",
        "    grid_search.fit(dataPoints, dataLabelsList)\n",
        "\n",
        "    # Step 1: Retrieve the best parameters and score from cross-validation\n",
        "    best_params = grid_search.best_params_\n",
        "    best_f1_cv = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
        "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
        "\n",
        "    # Step 2: Extract the cross-validation results and print final metrics\n",
        "    results = grid_search.cv_results_\n",
        "\n",
        "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
        "    fold_metrics = []\n",
        "    for idx in range(len(results['params'])):\n",
        "      fold_metrics.append({\n",
        "        'criterion': results['params'][idx].get('dt__criterion'),\n",
        "        'max_depth': results['params'][idx].get('dt__max_depth'),\n",
        "        'min_samples_split': results['params'][idx].get('dt__min_samples_split'),\n",
        "        'min_samples_leaf': results['params'][idx].get('dt__min_samples_leaf'),\n",
        "        'max_features': results['params'][idx].get('dt__max_features'),\n",
        "        'accuracy': results['mean_test_accuracy'][idx],\n",
        "        'precision': results['mean_test_precision'][idx],\n",
        "        'recall': results['mean_test_recall'][idx],\n",
        "        'f1': results['mean_test_f1'][idx],\n",
        "        'mcc': results['mean_test_mcc'][idx],\n",
        "        'preparationTime': vecTime / len(dataPoints)\n",
        "    })\n",
        "    # Save fold-wise metrics to CSV\n",
        "    df_folds = pd.DataFrame(fold_metrics)\n",
        "    outFile = os.path.join(outDir, f\"dt-smote-fold-results-{n_splits}-folds.csv\")\n",
        "    df_folds.to_csv(outFile, index=False)\n",
        "\n",
        "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
        "\n",
        "    # Step 3: Extract final metrics based on the cross-validation results\n",
        "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
        "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
        "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
        "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
        "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
        "\n",
        "    # Step 4: Print final metrics (cross-validation averages)\n",
        "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
        "    print(f\"Final Precision: {final_precision}\")\n",
        "    print(f\"Final Recall: {final_recall}\")\n",
        "    print(f\"Final Accuracy: {final_accuracy}\")\n",
        "    print(f\"Final F1 Score: {final_f1}\")\n",
        "    print(f\"Final MCC: {final_mcc}\")\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    outFile = os.path.join(outDir, f\"dt-results-{n_splits}-folds.csv\")\n",
        "    with open(outFile, \"w\") as f:\n",
        "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")\n",
        "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")\n",
        "\n",
        "    print(f\"Decision Tree analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
        "\n",
        "###############################################################################\n",
        "# Main Execution for Both 3-Fold and 5-Fold\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters setup\n",
        "    flakyZip = \"cleaned_flaky_files.zip\"\n",
        "    largerNonFlakyZip = \"all_nonflaky_files.zip\"\n",
        "\n",
        "    outDir = \"smote-results\"\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    extractDir = \"smote-extracted\"\n",
        "    os.makedirs(extractDir, exist_ok=True)\n",
        "\n",
        "    # Run Decision Tree with Pipeline and GridSearchCV using 3-fold cross-validation\n",
        "    print(\"Starting Decision Tree analysis with SMOTE for 3-fold cross-validation...\")\n",
        "    flastDTWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=3)\n",
        "\n",
        "    # Run Decision Tree with Pipeline and GridSearchCV using 5-fold cross-validation\n",
        "    print(\"Starting Decision Tree analysis with SMOTE for 5-fold cross-validation...\")\n",
        "    flastDTWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4nSGTn1rKre"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4FhS5bwzrQDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3cbc25-8903-48db-e865-6e62c8c3529b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Random Forest analysis with SMOTE for 3-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters with SMOTE: {'rf__criterion': 'entropy', 'rf__max_depth': 30, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 10}\n",
            "Best F1 Score from cross-validation: 0.8534562211981567\n",
            "Per-fold metrics saved to: smote-results/rf-smote-fold-results-3-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.9045177045177045\n",
            "Final Recall: 0.8083333333333332\n",
            "Final Accuracy: 0.9568646864686468\n",
            "Final F1 Score: 0.8534562211981567\n",
            "Final MCC: 0.8302348933402532\n",
            "Random Forest analysis completed for 3-folds with SMOTE. Results saved to: smote-results/rf-smote-results-3-folds.csv\n",
            "Starting Random Forest analysis with SMOTE for 5-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters with SMOTE: {'rf__criterion': 'gini', 'rf__max_depth': 30, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 50}\n",
            "Best F1 Score from cross-validation: 0.8325421396628826\n",
            "Per-fold metrics saved to: smote-results/rf-smote-fold-results-5-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.8605555555555554\n",
            "Final Recall: 0.8111111111111111\n",
            "Final Accuracy: 0.950327868852459\n",
            "Final F1 Score: 0.8325421396628826\n",
            "Final MCC: 0.8057316396502678\n",
            "Random Forest analysis completed for 5-folds with SMOTE. Results saved to: smote-results/rf-smote-results-5-folds.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (make_scorer, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef)\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "###############################################################################\n",
        "# Utility functions\n",
        "\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def getDataPoints(path):\n",
        "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
        "    dataPointsList = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory does not exist: {path}\")\n",
        "        return dataPointsList\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dataPointName in files:\n",
        "            if dataPointName.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, dataPointName)\n",
        "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
        "                    dp = fileIn.read().strip()\n",
        "                    if dp:  # Ensure the document is not empty\n",
        "                        dataPointsList.append(dp)\n",
        "                    else:\n",
        "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
        "\n",
        "    if len(dataPointsList) == 0:\n",
        "        print(f\"No valid documents found in directory: {path}\")\n",
        "\n",
        "    return dataPointsList\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"Plots confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Flaky', 'Flaky'], yticklabels=['Non-Flaky', 'Flaky'])\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Main Function with Pipeline and GridSearchCV for Random Forest\n",
        "\n",
        "def flastRFWithPipeline(outDir, flakyZip, nonFlakyZip, extractDir, n_splits):\n",
        "    v0 = time.perf_counter()\n",
        "\n",
        "    # Extract the zip files\n",
        "    flakyDir = os.path.join(extractDir, 'flaky')\n",
        "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
        "    os.makedirs(flakyDir, exist_ok=True)\n",
        "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
        "\n",
        "    extract_zip(flakyZip, flakyDir)\n",
        "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
        "\n",
        "    dataPointsFlaky = getDataPoints(flakyDir)\n",
        "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
        "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
        "\n",
        "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
        "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
        "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
        "\n",
        "    if len(dataPoints) == 0:\n",
        "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
        "\n",
        "    # Create labels: 1 for flaky, 0 for non-flaky\n",
        "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
        "\n",
        "    vecTime = time.perf_counter() - v0\n",
        "\n",
        "    scoring = {\n",
        "        'precision': make_scorer(precision_score, zero_division=1),\n",
        "        'recall': make_scorer(recall_score, zero_division=1),\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'f1': make_scorer(f1_score, zero_division=1),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Define a pipeline with CountVectorizer, SMOTE, and Random Forest\n",
        "    pipeline = ImbPipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
        "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
        "        ('rf', RandomForestClassifier(random_state=42))    # Random Forest classifier\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'rf__n_estimators': [10, 50, 100],                  # Number of trees in the forest\n",
        "        'rf__max_depth': [10, 30, 50],                      # Maximum depth of the tree\n",
        "        'rf__min_samples_split': [2, 5],                    # Minimum number of samples required to split a node\n",
        "        'rf__min_samples_leaf': [1, 2],                     # Minimum number of samples required to be at a leaf node\n",
        "        'rf__criterion': [\"gini\", \"entropy\"]                # Function to measure the quality of a split\n",
        "    }\n",
        "\n",
        "    # Setup cross-validation strategy\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setup GridSearchCV with the pipeline and parameter grid\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
        "\n",
        "    # Fit the GridSearchCV to the data\n",
        "    grid_search.fit(dataPoints, dataLabelsList)\n",
        "\n",
        "    # Step 1: Retrieve the best parameters and score from cross-validation\n",
        "    best_params = grid_search.best_params_\n",
        "    best_f1_cv = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
        "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
        "\n",
        "    # Step 2: Extract the cross-validation results and print final metrics\n",
        "    results = grid_search.cv_results_\n",
        "\n",
        "    # Prepare per-fold metrics for CSV along with the parameter combination\n",
        "    fold_metrics = []\n",
        "    for idx in range(len(results['params'])):\n",
        "        fold_metrics.append({\n",
        "            'max_depth': results['params'][idx].get('rf__max_depth'),\n",
        "            'criterion': results['params'][idx].get('rf__criterion'),\n",
        "            'min_samples_split': results['params'][idx].get('rf__min_samples_split'),\n",
        "            'min_samples_leaf': results['params'][idx].get('rf__min_samples_leaf'),\n",
        "            'n_estimators': results['params'][idx].get('rf__n_estimators'),\n",
        "            'accuracy': results['mean_test_accuracy'][idx],\n",
        "            'precision': results['mean_test_precision'][idx],\n",
        "            'recall': results['mean_test_recall'][idx],\n",
        "            'f1': results['mean_test_f1'][idx],\n",
        "            'mcc': results['mean_test_mcc'][idx],\n",
        "            'preparationTime': vecTime / len(dataPoints)\n",
        "        })\n",
        "\n",
        "    # Save fold-wise metrics to CSV\n",
        "    df_folds = pd.DataFrame(fold_metrics)\n",
        "    outFile = os.path.join(outDir, f\"rf-smote-fold-results-{n_splits}-folds.csv\")\n",
        "    df_folds.to_csv(outFile, index=False)\n",
        "\n",
        "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
        "\n",
        "    # Step 3: Extract final metrics based on the cross-validation results\n",
        "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
        "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
        "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
        "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
        "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
        "\n",
        "    # Step 4: Print final metrics (cross-validation averages)\n",
        "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
        "    print(f\"Final Precision: {final_precision}\")\n",
        "    print(f\"Final Recall: {final_recall}\")\n",
        "    print(f\"Final Accuracy: {final_accuracy}\")\n",
        "    print(f\"Final F1 Score: {final_f1}\")\n",
        "    print(f\"Final MCC: {final_mcc}\")\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    outFile = os.path.join(outDir, f\"rf-smote-results-{n_splits}-folds.csv\")\n",
        "    with open(outFile, \"w\") as f:\n",
        "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")\n",
        "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")\n",
        "\n",
        "    print(f\"Random Forest analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
        "\n",
        "###############################################################################\n",
        "# Main Execution for Both 3-Fold and 5-Fold\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters setup\n",
        "    flakyZip = \"cleaned_flaky_files.zip\"\n",
        "    largerNonFlakyZip = \"all_nonflaky_files.zip\"\n",
        "\n",
        "    outDir = \"smote-results\"\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    extractDir = \"smote-extracted\"\n",
        "    os.makedirs(extractDir, exist_ok=True)\n",
        "\n",
        "    # Run Random Forest with Pipeline and GridSearchCV using 3-fold cross-validation\n",
        "    print(\"Starting Random Forest analysis with SMOTE for 3-fold cross-validation...\")\n",
        "    flastRFWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=3)\n",
        "\n",
        "    # Run Random Forest with Pipeline and GridSearchCV using 5-fold cross-validation\n",
        "    print(\"Starting Random Forest analysis with SMOTE for 5-fold cross-validation...\")\n",
        "    flastRFWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gAbHgcQrQac"
      },
      "source": [
        "XGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-PIAvGdprR8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7666562b-54dd-4017-a054-0d12c3b08fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting XGBoost analysis with SMOTE for 3-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
            "Best Parameters with SMOTE: {'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 150}\n",
            "Best F1 Score from cross-validation: 0.8956759715380406\n",
            "Per-fold metrics saved to: smote-results/xgb-smote-fold-results-3-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 0.9777777777777779\n",
            "Final Recall: 0.8319444444444445\n",
            "Final Accuracy: 0.9700990099009901\n",
            "Final F1 Score: 0.8956759715380406\n",
            "Final MCC: 0.8845242548867366\n",
            "XGBoost analysis completed for 3-folds with SMOTE. Results saved to: smote-results/xgb-results-3-folds.csv\n",
            "Starting XGBoost analysis with SMOTE for 5-fold cross-validation...\n",
            "Number of flaky documents: 47\n",
            "Number of non-flaky documents: 254\n",
            "Total number of documents: 301\n",
            "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
            "Best Parameters with SMOTE: {'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 150}\n",
            "Best F1 Score from cross-validation: 0.9100398053958425\n",
            "Per-fold metrics saved to: smote-results/xgb-smote-fold-results-5-folds.csv\n",
            "\n",
            "Final Cross-Validation Metrics:\n",
            "Final Precision: 1.0\n",
            "Final Recall: 0.8488888888888889\n",
            "Final Accuracy: 0.9767213114754097\n",
            "Final F1 Score: 0.9100398053958425\n",
            "Final MCC: 0.9060587243849911\n",
            "XGBoost analysis completed for 5-folds with SMOTE. Results saved to: smote-results/xgb-results-5-folds.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (make_scorer, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef)\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "###############################################################################\n",
        "# Utility functions\n",
        "\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def getDataPoints(path):\n",
        "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
        "    dataPointsList = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory does not exist: {path}\")\n",
        "        return dataPointsList\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dataPointName in files:\n",
        "            if dataPointName.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, dataPointName)\n",
        "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
        "                    dp = fileIn.read().strip()\n",
        "                    if dp:  # Ensure the document is not empty\n",
        "                        dataPointsList.append(dp)\n",
        "                    else:\n",
        "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
        "\n",
        "    if len(dataPointsList) == 0:\n",
        "        print(f\"No valid documents found in directory: {path}\")\n",
        "\n",
        "    return dataPointsList\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"Plots confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Flaky', 'Flaky'], yticklabels=['Non-Flaky', 'Flaky'])\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Main Function with Pipeline and GridSearchCV for XGBoost\n",
        "\n",
        "def flastXGBWithPipeline(outDir, flakyZip, nonFlakyZip, extractDir, n_splits):\n",
        "    v0 = time.perf_counter()\n",
        "\n",
        "    # Extract the zip files\n",
        "    flakyDir = os.path.join(extractDir, 'flaky')\n",
        "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
        "    os.makedirs(flakyDir, exist_ok=True)\n",
        "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
        "\n",
        "    extract_zip(flakyZip, flakyDir)\n",
        "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
        "\n",
        "    dataPointsFlaky = getDataPoints(flakyDir)\n",
        "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
        "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
        "\n",
        "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
        "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
        "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
        "\n",
        "    if len(dataPoints) == 0:\n",
        "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
        "\n",
        "    # Create labels: 1 for flaky, 0 for non-flaky\n",
        "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
        "\n",
        "    vecTime = time.perf_counter() - v0\n",
        "\n",
        "    scoring = {\n",
        "        'precision': make_scorer(precision_score, zero_division=1),\n",
        "        'recall': make_scorer(recall_score, zero_division=1),\n",
        "        'accuracy': make_scorer(accuracy_score),\n",
        "        'f1': make_scorer(f1_score, zero_division=1),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Define a pipeline with CountVectorizer, SMOTE, and XGBoost\n",
        "    pipeline = ImbPipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
        "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
        "        ('xgb', XGBClassifier(eval_metric='logloss', random_state=42))  # XGBoost classifier\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'xgb__n_estimators': [150, 100, 200, 300],                   # Number of boosting rounds\n",
        "        'xgb__max_depth': [3, 5, 7, 10],                        # Maximum depth of a tree\n",
        "        'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],                 # Learning rate\n",
        "\n",
        "    }\n",
        "\n",
        "    # Setup cross-validation strategy\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setup GridSearchCV with the pipeline and parameter grid\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
        "\n",
        "    # Fit the GridSearchCV to the data\n",
        "    grid_search.fit(dataPoints, dataLabelsList)\n",
        "\n",
        "    # Step 1: Retrieve the best parameters and score from cross-validation\n",
        "    best_params = grid_search.best_params_\n",
        "    best_f1_cv = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
        "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
        "\n",
        "    # Step 2: Extract the cross-validation results and print final metrics\n",
        "    results = grid_search.cv_results_\n",
        "\n",
        "    fold_metrics = []\n",
        "    for fold_idx in range(n_splits):\n",
        "        fold_metrics.append({\n",
        "            'fold': fold_idx + 1,\n",
        "            'accuracy': results[f'split{fold_idx}_test_accuracy'][grid_search.best_index_],\n",
        "            'precision': results[f'split{fold_idx}_test_precision'][grid_search.best_index_],\n",
        "            'recall': results[f'split{fold_idx}_test_recall'][grid_search.best_index_],\n",
        "            'f1': results[f'split{fold_idx}_test_f1'][grid_search.best_index_],\n",
        "            'mcc': results[f'split{fold_idx}_test_mcc'][grid_search.best_index_]\n",
        "        })\n",
        "\n",
        "    # Save fold-wise metrics to CSV\n",
        "    df_folds = pd.DataFrame(fold_metrics)\n",
        "    outFile = os.path.join(outDir, f\"xgb-smote-fold-results-{n_splits}-folds.csv\")\n",
        "    df_folds.to_csv(outFile, index=False)\n",
        "\n",
        "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
        "\n",
        "    # Step 3: Extract final metrics based on the cross-validation results\n",
        "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
        "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
        "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
        "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
        "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
        "\n",
        "    # Step 4: Print final metrics (cross-validation averages)\n",
        "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
        "    print(f\"Final Precision: {final_precision}\")\n",
        "    print(f\"Final Recall: {final_recall}\")\n",
        "    print(f\"Final Accuracy: {final_accuracy}\")\n",
        "    print(f\"Final F1 Score: {final_f1}\")\n",
        "    print(f\"Final MCC: {final_mcc}\")\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    outFile = os.path.join(outDir, f\"xgb-results-{n_splits}-folds.csv\")\n",
        "    with open(outFile, \"w\") as f:\n",
        "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")\n",
        "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")\n",
        "\n",
        "    print(f\"XGBoost analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
        "\n",
        "###############################################################################\n",
        "# Main Execution for Both 3-Fold and 5-Fold\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters setup\n",
        "    flakyZip = \"cleaned_flaky_files.zip\"\n",
        "    largerNonFlakyZip = \"all_nonflaky_files.zip\"\n",
        "\n",
        "    outDir = \"smote-results\"\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    extractDir = \"smote-extracted\"\n",
        "    os.makedirs(extractDir, exist_ok=True)\n",
        "\n",
        "    # Run XGBoost with Pipeline and GridSearchCV using 3-fold cross-validation\n",
        "    print(\"Starting XGBoost analysis with SMOTE for 3-fold cross-validation...\")\n",
        "    flastXGBWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=3)\n",
        "\n",
        "    # Run XGBoost with Pipeline and GridSearchCV using 5-fold cross-validation\n",
        "    print(\"Starting XGBoost analysis with SMOTE for 5-fold cross-validation...\")\n",
        "    flastXGBWithPipeline(outDir, flakyZip, largerNonFlakyZip, extractDir, n_splits=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract the best results from the CSV files of each model\n",
        "def extract_best_results(model_name, fold, csv_file):\n",
        "    \"\"\"\n",
        "    Extracts the best result from the CSV file for a model.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name: The name of the model (e.g., \"XGBoost\", \"Random Forest\", \"Decision Tree\")\n",
        "    - fold: Number of folds (e.g., 5 or 3)\n",
        "    - csv_file: The path to the CSV file containing the model's results\n",
        "\n",
        "    Returns:\n",
        "    A dictionary containing the best results for the model and fold.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(csv_file):\n",
        "        print(f\"CSV file for {model_name} ({fold}-fold) does not exist: {csv_file}\")\n",
        "        return None\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Get the row with the best F1 score\n",
        "    best_row = df.loc[df['f1'].idxmax()]\n",
        "\n",
        "    # Collect the best results into a dictionary\n",
        "    best_results = {\n",
        "        'Model': model_name,\n",
        "        'Fold': f\"{fold}-fold\",\n",
        "        'Best Accuracy': best_row['accuracy'],\n",
        "        'Best Precision': best_row['precision'],\n",
        "        'Best Recall': best_row['recall'],\n",
        "        'Best F1 Score': best_row['f1'],\n",
        "        'Best MCC': best_row['mcc'],\n",
        "        'Best Parameters': best_row.to_dict()  # Including all parameters\n",
        "    }\n",
        "\n",
        "    return best_results\n",
        "\n",
        "# Function to gather and print/save the best results from all SMOTE models\n",
        "def gather_best_results(models_results_dir, output_file):\n",
        "    \"\"\"\n",
        "    Gathers the best results from all SMOTE models and writes them to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - models_results_dir: Directory where the model result CSV files are stored for the SMOTE models.\n",
        "    - output_file: Path to the output CSV file to store the best results.\n",
        "    \"\"\"\n",
        "    # List of models and their corresponding result files for both 5-fold and 3-fold\n",
        "    models = {\n",
        "        'KNN': {'5-fold': 'knn-smote-fold-results-5-folds.csv', '3-fold': 'knn-smote-fold-results-3-folds.csv'},\n",
        "        'SVM': {'5-fold': 'svm-smote-fold-results-5-folds.csv', '3-fold': 'svm-smote-fold-results-5-folds.csv'},\n",
        "        'Naive Bayes': {'5-fold': 'nb-smote-fold-results-5-folds.csv', '3-fold': 'nb-smote-fold-results-3-folds.csv'},\n",
        "        'XGBoost': {'5-fold': 'xgb-smote-fold-results-5-folds.csv', '3-fold': 'xgb-smote-fold-results-3-folds.csv'},\n",
        "        'Random Forest': {'5-fold': 'rf-smote-fold-results-5-folds.csv', '3-fold': 'rf-smote-fold-results-3-folds.csv'},\n",
        "        'Decision Tree': {'5-fold': 'dt-smote-fold-results-5-folds.csv', '3-fold': 'dt-smote-fold-results-3-folds.csv'}\n",
        "    }\n",
        "\n",
        "    # Initialize an empty list to store the best results from each model and fold\n",
        "    best_results = []\n",
        "\n",
        "    # Iterate over each model and its result files for both 5-fold and 3-fold\n",
        "    for model_name, folds in models.items():\n",
        "        for fold, csv_file in folds.items():\n",
        "            full_csv_path = os.path.join(models_results_dir, csv_file)\n",
        "            best_result = extract_best_results(model_name, fold, full_csv_path)\n",
        "            if best_result:\n",
        "                best_results.append(best_result)\n",
        "\n",
        "    # Convert the list of best results into a DataFrame\n",
        "    best_results_df = pd.DataFrame(best_results)\n",
        "\n",
        "    # Save the best results to the output CSV file\n",
        "    best_results_df.to_csv(output_file, index=False)\n",
        "    print(f\"Best results saved to: {output_file}\")\n",
        "\n",
        "    # Print the best results as a table\n",
        "    print(f\"\\nBest Results from All SMOTE Models\")\n",
        "    print(best_results_df.to_string(index=False))\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Directories where the model result CSV files are stored\n",
        "    results_dir = '/content/smote-results'\n",
        "\n",
        "    # Path to the output CSV file where best results will be stored\n",
        "    output_file = \"best_results_smote.csv\"\n",
        "\n",
        "    # Gather and save the best results\n",
        "    gather_best_results(results_dir, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yxBRqTC43zV",
        "outputId": "1be31f34-0d7f-4e19-9a10-f22f8ca28330"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best results saved to: best_results_smote.csv\n",
            "\n",
            "Best Results from All SMOTE Models\n",
            "        Model        Fold  Best Accuracy  Best Precision  Best Recall  Best F1 Score  Best MCC                                                                                                                                                                                                                                                                                         Best Parameters\n",
            "          KNN 5-fold-fold       0.823934        0.468036     0.746667       0.566845  0.492523                                                  {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine', 'accuracy': 0.8239344262295081, 'precision': 0.4680361305361306, 'recall': 0.7466666666666667, 'f1': 0.566845388365725, 'mcc': 0.4925226809183992, 'preparationTime': 0.000352934375414}\n",
            "          KNN 3-fold-fold       0.857063        0.531808     0.704167       0.604177  0.528114                                              {'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean', 'accuracy': 0.857062706270627, 'precision': 0.5318077803203661, 'recall': 0.7041666666666666, 'f1': 0.6041771094402673, 'mcc': 0.5281142717308954, 'preparationTime': 0.0004331170897002}\n",
            "          SVM 5-fold-fold       0.920273        0.839286     0.611111       0.696374  0.669159                                                                                  {'C': 10.0, 'kernel': 'rbf', 'accuracy': 0.9202732240437156, 'precision': 0.8392857142857142, 'recall': 0.611111111111111, 'f1': 0.6963741366992141, 'mcc': 0.6691593384052171, 'preparationTime': 0.0003407861096331}\n",
            "          SVM 3-fold-fold       0.920273        0.839286     0.611111       0.696374  0.669159                                                                                  {'C': 10.0, 'kernel': 'rbf', 'accuracy': 0.9202732240437156, 'precision': 0.8392857142857142, 'recall': 0.611111111111111, 'f1': 0.6963741366992141, 'mcc': 0.6691593384052171, 'preparationTime': 0.0003407861096331}\n",
            "  Naive Bayes 5-fold-fold       0.916940        0.771429     0.633333       0.689244  0.650460                                                                                               {'alpha': 0.1, 'accuracy': 0.9169398907103826, 'precision': 0.7714285714285715, 'recall': 0.6333333333333334, 'f1': 0.6892436974789915, 'mcc': 0.6504601760622541, 'preparationTime': 0.0003397509402011}\n",
            "  Naive Bayes 3-fold-fold       0.906832        0.740232     0.618056       0.672414  0.622982                                                                                                {'alpha': 1.0, 'accuracy': 0.9068316831683167, 'precision': 0.7402319902319903, 'recall': 0.6180555555555555, 'f1': 0.6724137931034483, 'mcc': 0.6229824203893163, 'preparationTime': 0.000364616302325}\n",
            "      XGBoost 5-fold-fold       1.000000        1.000000     1.000000       1.000000  1.000000                                                                                                                                                                                                                  {'fold': 5.0, 'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'mcc': 1.0}\n",
            "      XGBoost 3-fold-fold       0.980000        0.933333     0.933333       0.933333  0.921569                                                                                                                                                     {'fold': 3.0, 'accuracy': 0.98, 'precision': 0.9333333333333332, 'recall': 0.9333333333333332, 'f1': 0.9333333333333332, 'mcc': 0.9215686274509804}\n",
            "Random Forest 5-fold-fold       0.950328        0.860556     0.811111       0.832542  0.805732     {'max_depth': 30, 'criterion': 'gini', 'min_samples_split': 2, 'min_samples_leaf': 1, 'n_estimators': 50, 'accuracy': 0.950327868852459, 'precision': 0.8605555555555554, 'recall': 0.8111111111111111, 'f1': 0.8325421396628826, 'mcc': 0.8057316396502678, 'preparationTime': 0.0003413665116284}\n",
            "Random Forest 3-fold-fold       0.956865        0.904518     0.808333       0.853456  0.830235 {'max_depth': 30, 'criterion': 'entropy', 'min_samples_split': 2, 'min_samples_leaf': 1, 'n_estimators': 10, 'accuracy': 0.9568646864686468, 'precision': 0.9045177045177044, 'recall': 0.8083333333333332, 'f1': 0.8534562211981567, 'mcc': 0.8302348933402532, 'preparationTime': 0.0004524303588042}\n",
            "Decision Tree 5-fold-fold       0.966776        0.930000     0.871111       0.891872  0.878042                 {'criterion': 'gini', 'max_depth': nan, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': nan, 'accuracy': 0.966775956284153, 'precision': 0.93, 'recall': 0.8711111111111112, 'f1': 0.8918716577540108, 'mcc': 0.8780415116902793, 'preparationTime': 0.0003436753023255}\n",
            "Decision Tree 3-fold-fold       0.966799        0.916667     0.873611       0.891593  0.874441  {'criterion': 'gini', 'max_depth': nan, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': nan, 'accuracy': 0.9667986798679868, 'precision': 0.9166666666666666, 'recall': 0.8736111111111112, 'f1': 0.8915925101965146, 'mcc': 0.8744409272444104, 'preparationTime': 0.0003409799335538}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}