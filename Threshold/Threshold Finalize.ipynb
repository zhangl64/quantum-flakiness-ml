{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014a5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents (larger combination): 45\n",
      "Number of non-flaky documents (larger combination): 243\n",
      "Total number of documents (larger combination): 288\n",
      "Shape of vectorized data (larger combination): (288, 11847)\n",
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Paths to your datasets\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "nonFlakyZip_larger = \"Dataset/nonflaky_files.zip\"   \n",
    "\n",
    "# Create directories\n",
    "outDirLarger = \"results/larger_nonflaky/\"\n",
    "os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract and read data for larger non-flaky combination\n",
    "flakyDirLarger = os.path.join(extractDirLarger, 'flaky')\n",
    "nonFlakyDirLarger = os.path.join(extractDirLarger, 'nonFlaky')\n",
    "os.makedirs(flakyDirLarger, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirLarger, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirLarger)\n",
    "extract_zip(nonFlakyZip_larger, nonFlakyDirLarger)\n",
    "\n",
    "dataPointsFlakyLarger = getDataPoints(flakyDirLarger)\n",
    "dataPointsNonFlakyLarger = getDataPoints(nonFlakyDirLarger)\n",
    "dataPointsLarger = dataPointsFlakyLarger + dataPointsNonFlakyLarger\n",
    "\n",
    "# Print the number of datasets for larger combination\n",
    "print(f\"Number of flaky documents (larger combination): {len(dataPointsFlakyLarger)}\")\n",
    "print(f\"Number of non-flaky documents (larger combination): {len(dataPointsNonFlakyLarger)}\")\n",
    "print(f\"Total number of documents (larger combination): {len(dataPointsLarger)}\")\n",
    "\n",
    "dataLabelsListLarger = np.array([1]*len(dataPointsFlakyLarger) + [0]*len(dataPointsNonFlakyLarger))\n",
    "\n",
    "# Vectorize data\n",
    "Z_larger = flastVectorization(dataPointsLarger)\n",
    "\n",
    "print(\"Shape of vectorized data (larger combination):\", Z_larger.shape)\n",
    "\n",
    "print(\"Data preprocessing completed.\")\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18238311",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30601c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Per-fold KNN analysis completed. Results saved to: knn_thresholds-thresholds-knn-results-per-fold.csv\n",
      "Averaged results saved to: knn_thresholds-thresholds-knn-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   pca__n_components  n_neighbors  weights  metric  fold  threshold  accuracy  \\\n",
      "0                150            3  uniform  cosine     1        0.1  0.758621   \n",
      "1                150            3  uniform  cosine     1        0.2  0.758621   \n",
      "2                150            3  uniform  cosine     1        0.3  0.758621   \n",
      "3                150            3  uniform  cosine     1        0.4  0.879310   \n",
      "4                150            3  uniform  cosine     1        0.5  0.879310   \n",
      "\n",
      "   precision    recall        f1       mcc  \n",
      "0   0.333333  0.555556  0.416667  0.290625  \n",
      "1   0.333333  0.555556  0.416667  0.290625  \n",
      "2   0.333333  0.555556  0.416667  0.290625  \n",
      "3   0.750000  0.333333  0.461538  0.447129  \n",
      "4   0.750000  0.333333  0.461538  0.447129  \n",
      "Averaged Results:\n",
      "   pca__n_components  n_neighbors   weights  metric  threshold  accuracy_mean  \\\n",
      "0                150            3  distance  cosine        0.1       0.854204   \n",
      "1                150            3  distance  cosine        0.2       0.864610   \n",
      "2                150            3  distance  cosine        0.3       0.868179   \n",
      "3                150            3  distance  cosine        0.4       0.878584   \n",
      "4                150            3  distance  cosine        0.5       0.875076   \n",
      "\n",
      "   accuracy_std  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0      0.031166        0.533217       0.090486     0.600000    0.168508   \n",
      "1      0.037400        0.568182       0.122390     0.600000    0.168508   \n",
      "2      0.039652        0.588889       0.133795     0.511111    0.168508   \n",
      "3      0.023753        0.734286       0.200408     0.355556    0.144871   \n",
      "4      0.022002        0.774286       0.233954     0.311111    0.144871   \n",
      "\n",
      "    f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0  0.555758  0.114102  0.476908  0.125255  \n",
      "1  0.575462  0.131355  0.501408  0.147215  \n",
      "2  0.542222  0.146228  0.470821  0.165204  \n",
      "3  0.464935  0.162247  0.448658  0.158913  \n",
      "4  0.423377  0.154581  0.427528  0.149471  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "def flastKNNWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization without PCA\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Define the pipeline with PCA and KNN\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('knn', KNeighborsClassifier(\n",
    "                n_neighbors=param_dict['n_neighbors'],\n",
    "                weights=param_dict['weights'],\n",
    "                metric=param_dict['metric'],\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(pipeline.named_steps['knn'], \"predict_proba\"):\n",
    "                y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = pipeline.named_steps['knn'].kneighbors(X_test_dense)\n",
    "                weights = pipeline.named_steps['knn']._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test_dense.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / pipeline.named_steps['knn'].n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'metric': param_dict['metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-knn-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold KNN analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['pca__n_components', 'n_neighbors', 'weights', 'metric', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-knn-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #param grid\n",
    "    param_grid = {\n",
    "        'pca__n_components': [150,180,200,220],  \n",
    "        'n_neighbors': [3, 5, 7, 9,11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['cosine', 'euclidean'],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"   \n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  \n",
    "    \n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/knn_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/knn_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis with threshold adjustments\n",
    "    print(\"Starting KNN analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastKNNWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, combination_label=\"knn_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52380d",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfccd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Per-fold SVM analysis completed. Results saved to: svm_thresholds-thresholds-svm-results-per-fold.csv\n",
      "Averaged results saved to: svm_thresholds-thresholds-svm-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   pca__n_components  svm__C svm__kernel  fold  threshold  accuracy  \\\n",
      "0                150    0.01      linear     1        0.1  0.793103   \n",
      "1                150    0.01      linear     1        0.2  0.913793   \n",
      "2                150    0.01      linear     1        0.3  0.948276   \n",
      "3                150    0.01      linear     1        0.4  0.948276   \n",
      "4                150    0.01      linear     1        0.5  0.896552   \n",
      "\n",
      "   precision    recall        f1       mcc  \n",
      "0   0.411765  0.777778  0.538462  0.456336  \n",
      "1   0.750000  0.666667  0.705882  0.657143  \n",
      "2   1.000000  0.666667  0.800000  0.792594  \n",
      "3   1.000000  0.666667  0.800000  0.792594  \n",
      "4   1.000000  0.333333  0.500000  0.544949  \n",
      "Averaged Results:\n",
      "   pca__n_components  svm__C svm__kernel  threshold  accuracy_mean  \\\n",
      "0                150    0.01      linear        0.1       0.781246   \n",
      "1                150    0.01      linear        0.2       0.909619   \n",
      "2                150    0.01      linear        0.3       0.902601   \n",
      "3                150    0.01      linear        0.4       0.909619   \n",
      "4                150    0.01      linear        0.5       0.902783   \n",
      "\n",
      "   accuracy_std  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0      0.049663        0.415755       0.054813     0.911111    0.121716   \n",
      "1      0.019698        0.702121       0.036890     0.733333    0.185924   \n",
      "2      0.042399        0.720556       0.186790     0.600000    0.230405   \n",
      "3      0.045152        0.891429       0.174262     0.488889    0.255797   \n",
      "4      0.043630        0.933333       0.149071     0.400000    0.255797   \n",
      "\n",
      "    f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0  0.567704  0.060260  0.514142  0.079142  \n",
      "1  0.707593  0.103336  0.661493  0.109769  \n",
      "2  0.642109  0.204357  0.598420  0.216152  \n",
      "3  0.595714  0.270091  0.597732  0.235823  \n",
      "4  0.526667  0.271211  0.553641  0.236713  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flastSVMWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization \n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Define the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('svm', SVC(\n",
    "                C=param_dict['svm__C'],\n",
    "                kernel=param_dict['svm__kernel'],\n",
    "                probability=True,  # Enable probability estimates\n",
    "                random_state=42\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'svm__C': param_dict['svm__C'],\n",
    "                    'svm__kernel': param_dict['svm__kernel'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-svm-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold SVM analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['pca__n_components', 'svm__C', 'svm__kernel', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-svm-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "    'pca__n_components': [150, 180, 200, 220],           # PCA components\n",
    "    'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],            # Regularization parameter C for SVM\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types for SVM\n",
    "}\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  \n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/svm_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/svm_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis with threshold adjustments\n",
    "    print(\"Starting SVM analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastSVMWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, combination_label=\"svm_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c77477",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1eee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c379083",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "794c3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Per-fold XGBoost analysis completed. Results saved to: xgb_thresholds-thresholds-xgb-results-per-fold.csv\n",
      "Averaged results saved to: xgb_thresholds-thresholds-xgb-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   pca__n_components  xgb__learning_rate  xgb__max_depth  xgb__n_estimators  \\\n",
      "0                180                0.01               3                 50   \n",
      "1                180                0.01               3                 50   \n",
      "2                180                0.01               3                 50   \n",
      "3                180                0.01               3                 50   \n",
      "4                180                0.01               3                 50   \n",
      "\n",
      "   fold  threshold  accuracy  precision    recall        f1       mcc  \n",
      "0     1        0.1  0.155172   0.155172  1.000000  0.268657  0.000000  \n",
      "1     1        0.2  0.896552   0.714286  0.555556  0.625000  0.572101  \n",
      "2     1        0.3  0.862069   0.666667  0.222222  0.333333  0.329935  \n",
      "3     1        0.4  0.879310   1.000000  0.222222  0.363636  0.440959  \n",
      "4     1        0.5  0.844828   1.000000  0.000000  0.000000  0.000000  \n",
      "Averaged Results:\n",
      "   pca__n_components  xgb__learning_rate  xgb__max_depth  xgb__n_estimators  \\\n",
      "0                180                0.01               3                 50   \n",
      "1                180                0.01               3                 50   \n",
      "2                180                0.01               3                 50   \n",
      "3                180                0.01               3                 50   \n",
      "4                180                0.01               3                 50   \n",
      "\n",
      "   threshold  accuracy_mean  accuracy_std  precision_mean  precision_std  \\\n",
      "0        0.1       0.156261      0.001491        0.156261       0.001491   \n",
      "1        0.2       0.830067      0.081730        0.506583       0.175154   \n",
      "2        0.3       0.854265      0.018818        0.666667       0.216025   \n",
      "3        0.4       0.857592      0.014989        1.000000       0.000000   \n",
      "4        0.5       0.843739      0.001491        1.000000       0.000000   \n",
      "\n",
      "   recall_mean  recall_std   f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0     1.000000    0.000000  0.270285  0.002230  0.000000  0.000000  \n",
      "1     0.622222    0.149071  0.546874  0.143096  0.457424  0.187059  \n",
      "2     0.244444    0.164804  0.316190  0.200611  0.278431  0.183541  \n",
      "3     0.088889    0.092962  0.152727  0.154599  0.211725  0.200663  \n",
      "4     0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import product\n",
    "from xgboost import XGBClassifier  # Import XGBoost classifier\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Function to run XGBoost analysis with PCA and threshold adjustments\n",
    "\n",
    "def flastXGBWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Define the pipeline with PCA and XGBoost\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                learning_rate=param_dict['xgb__learning_rate'],\n",
    "                max_depth=param_dict['xgb__max_depth'],\n",
    "                n_estimators=param_dict['xgb__n_estimators'],\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                verbosity=0,\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )),\n",
    "        ])\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'xgb__learning_rate': param_dict['xgb__learning_rate'],\n",
    "                    'xgb__max_depth': param_dict['xgb__max_depth'],\n",
    "                    'xgb__n_estimators': param_dict['xgb__n_estimators'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-xgb-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold XGBoost analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['pca__n_components', 'xgb__learning_rate', 'xgb__max_depth', 'xgb__n_estimators', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-xgb-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'pca__n_components': [180, 200, 220],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "        'xgb__max_depth': [3, 5, 7, 10],\n",
    "        'xgb__n_estimators': [50, 100, 200, 300],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  # Unbalanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/xgb_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/xgb_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform XGBoost analysis with threshold adjustments\n",
    "    print(\"Starting XGBoost analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastXGBWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5,\n",
    "        combination_label=\"xgb_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e68fb8",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43310b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'johnson_lindenstrauss_min_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 177\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Perform Random Forest analysis with threshold adjustments\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Random Forest analysis with threshold adjustments...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m df_results, grouped_metrics \u001b[38;5;241m=\u001b[39m flastRFWithThresholds(\n\u001b[0;32m    178\u001b[0m     outDir, flakyZip, nonFlakyZip, extractDir, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m    179\u001b[0m     combination_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf_thresholds\u001b[39m\u001b[38;5;124m\"\u001b[39m, param_grid\u001b[38;5;241m=\u001b[39mparam_grid)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis completed. Per-fold Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 42\u001b[0m, in \u001b[0;36mflastRFWithThresholds\u001b[1;34m(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo documents available for vectorization. Please check the input directories.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Vectorization without PCA\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m Z \u001b[38;5;241m=\u001b[39m flastVectorization(dataPoints)\n\u001b[0;32m     43\u001b[0m dataLabelsList \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataPointsFlaky) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataPointsNonFlaky))\n\u001b[0;32m     44\u001b[0m vecTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m v0\n",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m, in \u001b[0;36mflastVectorization\u001b[1;34m(dataPoints, dim, eps)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m         dim \u001b[38;5;241m=\u001b[39m johnson_lindenstrauss_min_dim(Z_full\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], eps\u001b[38;5;241m=\u001b[39meps)\n\u001b[0;32m     13\u001b[0m     srp \u001b[38;5;241m=\u001b[39m SparseRandomProjection(n_components\u001b[38;5;241m=\u001b[39mdim)\n\u001b[0;32m     14\u001b[0m     Z \u001b[38;5;241m=\u001b[39m srp\u001b[38;5;241m.\u001b[39mfit_transform(Z_full)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'johnson_lindenstrauss_min_dim' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d58f10d7",
   "metadata": {},
   "source": [
    "## Decision Tree (this took longer soooo long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e92196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Per-fold Decision Tree analysis completed. Results saved to: dt_thresholds-thresholds-dt-results-per-fold.csv\n",
      "Averaged results saved to: dt_thresholds-thresholds-dt-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "  dt__criterion  dt__max_depth  dt__min_samples_split  dt__min_samples_leaf  \\\n",
      "0          gini             10                      5                     2   \n",
      "1          gini             10                      5                     2   \n",
      "2          gini             10                      5                     2   \n",
      "3          gini             10                      5                     2   \n",
      "4          gini             10                      5                     2   \n",
      "\n",
      "  dt__max_features  fold  threshold  accuracy  precision    recall   f1  \\\n",
      "0             None     1        0.1  0.948276        1.0  0.666667  0.8   \n",
      "1             None     1        0.2  0.948276        1.0  0.666667  0.8   \n",
      "2             None     1        0.3  0.948276        1.0  0.666667  0.8   \n",
      "3             None     1        0.4  0.948276        1.0  0.666667  0.8   \n",
      "4             None     1        0.5  0.948276        1.0  0.666667  0.8   \n",
      "\n",
      "        mcc  \n",
      "0  0.792594  \n",
      "1  0.792594  \n",
      "2  0.792594  \n",
      "3  0.792594  \n",
      "4  0.792594  \n",
      "Averaged Results:\n",
      "  dt__criterion  dt__max_depth  dt__min_samples_split  dt__min_samples_leaf  \\\n",
      "0       entropy             10                      5                     2   \n",
      "1       entropy             10                      5                     2   \n",
      "2       entropy             10                      5                     2   \n",
      "3       entropy             10                      5                     2   \n",
      "4       entropy             10                      5                     2   \n",
      "\n",
      "  dt__max_features  threshold  accuracy_mean  accuracy_std  precision_mean  \\\n",
      "0             log2        0.1       0.263218      0.112794        0.166760   \n",
      "1             log2        0.2       0.829764      0.023410        0.416667   \n",
      "2             log2        0.3       0.829764      0.023410        0.416667   \n",
      "3             log2        0.4       0.833273      0.023748        0.450000   \n",
      "4             log2        0.5       0.833273      0.023748        0.450000   \n",
      "\n",
      "   precision_std  recall_mean  recall_std   f1_mean    f1_std  mcc_mean  \\\n",
      "0       0.026820     0.911111    0.092962  0.281403  0.040601  0.026909   \n",
      "1       0.372678     0.088889    0.049690  0.140466  0.080390  0.118230   \n",
      "2       0.372678     0.088889    0.049690  0.140466  0.080390  0.118230   \n",
      "3       0.370810     0.088889    0.049690  0.143497  0.081896  0.131331   \n",
      "4       0.370810     0.088889    0.049690  0.143497  0.081896  0.131331   \n",
      "\n",
      "    mcc_std  \n",
      "0  0.211725  \n",
      "1  0.143856  \n",
      "2  0.143856  \n",
      "3  0.146268  \n",
      "4  0.146268  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72871749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-fold results file for Naive Bayes does not exist: results\\nb_thresholds\\nb_thresholds-thresholds-nb-results-per-fold.csv\n",
      "Averaged results file for Naive Bayes does not exist: results\\nb_thresholds\\nb_thresholds-thresholds-nb-results-averaged.csv\n",
      "Best per-fold results saved to 'best_per_fold_results.csv'\n",
      "Best averaged results saved to 'best_averaged_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_193228\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the models with their directories and abbreviations\n",
    "models = {\n",
    "    'Decision Tree': {'dir': 'dt_thresholds', 'abbr': 'dt'},\n",
    "    'SVM': {'dir': 'svm_thresholds', 'abbr': 'svm'},\n",
    "    'KNN': {'dir': 'knn_thresholds', 'abbr': 'knn'},\n",
    "    'Random Forest': {'dir': 'rf_thresholds', 'abbr': 'rf'},\n",
    "    'Naive Bayes': {'dir': 'nb_thresholds', 'abbr': 'nb'},\n",
    "    'XGBoost': {'dir': 'xgb_thresholds', 'abbr': 'xgb'}\n",
    "}\n",
    "\n",
    "# Base directory where the results are stored\n",
    "base_results_dir = 'results'\n",
    "\n",
    "# Lists to store the best per-fold and averaged results\n",
    "best_per_fold_results = []\n",
    "best_averaged_results = []\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name, model_info in models.items():\n",
    "    model_dir = model_info['dir']\n",
    "    model_abbr = model_info['abbr']\n",
    "    # Paths to the per-fold and averaged results\n",
    "    per_fold_csv = os.path.join(base_results_dir, model_dir, f'{model_dir}-thresholds-{model_abbr}-results-per-fold.csv')\n",
    "    averaged_csv = os.path.join(base_results_dir, model_dir, f'{model_dir}-thresholds-{model_abbr}-results-averaged.csv')\n",
    "\n",
    "    # Check if the per-fold results file exists\n",
    "    if os.path.exists(per_fold_csv):\n",
    "        # Read the per-fold results\n",
    "        df_per_fold = pd.read_csv(per_fold_csv)\n",
    "        if not df_per_fold.empty:\n",
    "            # Find the row with the highest F1 score\n",
    "            best_row_per_fold = df_per_fold.loc[df_per_fold['f1'].idxmax()]\n",
    "            # Add model name to the results\n",
    "            best_row_per_fold['Model'] = model_name\n",
    "            # Append to the list\n",
    "            best_per_fold_results.append(best_row_per_fold)\n",
    "        else:\n",
    "            print(f\"Per-fold results for {model_name} are empty.\")\n",
    "    else:\n",
    "        print(f\"Per-fold results file for {model_name} does not exist: {per_fold_csv}\")\n",
    "\n",
    "    # Check if the averaged results file exists\n",
    "    if os.path.exists(averaged_csv):\n",
    "        # Read the averaged results\n",
    "        df_averaged = pd.read_csv(averaged_csv)\n",
    "        if not df_averaged.empty:\n",
    "            # Find the row with the highest mean F1 score\n",
    "            # Assuming the columns are named like 'f1_mean' for mean and 'f1_std' for std deviation\n",
    "            best_row_averaged = df_averaged.loc[df_averaged['f1_mean'].idxmax()]\n",
    "            # Add model name to the results\n",
    "            best_row_averaged['Model'] = model_name\n",
    "            # Append to the list\n",
    "            best_averaged_results.append(best_row_averaged)\n",
    "        else:\n",
    "            print(f\"Averaged results for {model_name} are empty.\")\n",
    "    else:\n",
    "        print(f\"Averaged results file for {model_name} does not exist: {averaged_csv}\")\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "if best_per_fold_results:\n",
    "    df_best_per_fold = pd.DataFrame(best_per_fold_results)\n",
    "    # Reorder columns to have 'Model' first\n",
    "    cols = ['Model'] + [col for col in df_best_per_fold.columns if col != 'Model']\n",
    "    df_best_per_fold = df_best_per_fold[cols]\n",
    "    # Save the best per-fold results to a CSV file\n",
    "    df_best_per_fold.to_csv('best_per_fold_results.csv', index=False)\n",
    "    print(\"Best per-fold results saved to 'best_per_fold_results.csv'\")\n",
    "else:\n",
    "    print(\"No per-fold results to save.\")\n",
    "\n",
    "if best_averaged_results:\n",
    "    df_best_averaged = pd.DataFrame(best_averaged_results)\n",
    "    # Reorder columns to have 'Model' first\n",
    "    cols = ['Model'] + [col for col in df_best_averaged.columns if col != 'Model']\n",
    "    df_best_averaged = df_best_averaged[cols]\n",
    "    # Save the best averaged results to a CSV file\n",
    "    df_best_averaged.to_csv('best_averaged_results.csv', index=False)\n",
    "    print(\"Best averaged results saved to 'best_averaged_results.csv'\")\n",
    "else:\n",
    "    print(\"No averaged results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec077ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
