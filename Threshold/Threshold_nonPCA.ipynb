{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5d9c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents (larger combination): 45\n",
      "Number of non-flaky documents (larger combination): 243\n",
      "Total number of documents (larger combination): 288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import SparseRandomProjection, johnson_lindenstrauss_min_dim\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Parameters setup\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "largerNonFlakyZip = \"Dataset/all_nonflaky_files.zip\"\n",
    "\n",
    "# Create directories\n",
    "outDirLarger = \"results/larger_nonflaky/\"\n",
    "os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "# Extract and read data once for equal combination\n",
    "os.makedirs(nonFlakyDirEqual, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vectorize data once\n",
    "\n",
    "# Extract and read data once for larger non-flaky combination\n",
    "flakyDirLarger = os.path.join(extractDirLarger, 'flaky')\n",
    "nonFlakyDirLarger = os.path.join(extractDirLarger, 'nonFlaky')\n",
    "os.makedirs(flakyDirLarger, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirLarger, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirLarger)\n",
    "\n",
    "dataPointsFlakyLarger = getDataPoints(flakyDirLarger)\n",
    "dataPointsNonFlakyLarger = getDataPoints(nonFlakyDirLarger)\n",
    "dataPointsLarger = dataPointsFlakyLarger + dataPointsNonFlakyLarger\n",
    "\n",
    "# Print the number of datasets for larger combination\n",
    "print(f\"Number of flaky documents (larger combination): {len(dataPointsFlakyLarger)}\")\n",
    "print(f\"Number of non-flaky documents (larger combination): {len(dataPointsNonFlakyLarger)}\")\n",
    "print(f\"Total number of documents (larger combination): {len(dataPointsLarger)}\")\n",
    "\n",
    "dataLabelsListLarger = np.array([1]*len(dataPointsFlakyLarger) + [0]*len(dataPointsNonFlakyLarger))\n",
    "\n",
    "Z_larger = flastVectorization(dataPointsLarger, dim=100, eps=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e0c0a",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18a52ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 50, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 100, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 10, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 20, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'gini'}\n",
      "Training with parameters: {'rf__n_estimators': 200, 'rf__max_depth': 30, 'rf__min_samples_split': 10, 'rf__min_samples_leaf': 5, 'rf__criterion': 'entropy'}\n",
      "Per-fold Random Forest analysis completed. Results saved to: rf_thresholds-thresholds-rf-results-per-fold.csv\n",
      "Averaged results saved to: rf_thresholds-thresholds-rf-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   rf__n_estimators rf__criterion  rf__max_depth  rf__min_samples_split  \\\n",
      "0                50          gini             10                      5   \n",
      "1                50          gini             10                      5   \n",
      "2                50          gini             10                      5   \n",
      "3                50          gini             10                      5   \n",
      "4                50          gini             10                      5   \n",
      "\n",
      "   rf__min_samples_leaf  fold  threshold  accuracy  precision    recall  \\\n",
      "0                     2     1        0.1  0.586207   0.258065  0.888889   \n",
      "1                     2     1        0.2  0.879310   0.571429  0.888889   \n",
      "2                     2     1        0.3  0.931034   0.857143  0.666667   \n",
      "3                     2     1        0.4  0.948276   1.000000  0.666667   \n",
      "4                     2     1        0.5  0.896552   1.000000  0.333333   \n",
      "\n",
      "         f1       mcc  \n",
      "0  0.400000  0.304502  \n",
      "1  0.695652  0.648496  \n",
      "2  0.750000  0.718276  \n",
      "3  0.800000  0.792594  \n",
      "4  0.500000  0.544949  \n",
      "Averaged Results:\n",
      "   rf__n_estimators rf__criterion  rf__max_depth  rf__min_samples_split  \\\n",
      "0                50       entropy             10                      5   \n",
      "1                50       entropy             10                      5   \n",
      "2                50       entropy             10                      5   \n",
      "3                50       entropy             10                      5   \n",
      "4                50       entropy             10                      5   \n",
      "\n",
      "   rf__min_samples_leaf  threshold  accuracy_mean  accuracy_std  \\\n",
      "0                     2        0.1       0.649607      0.068758   \n",
      "1                     2        0.2       0.895947      0.029435   \n",
      "2                     2        0.3       0.944465      0.018950   \n",
      "3                     2        0.4       0.927102      0.007465   \n",
      "4                     2        0.5       0.888869      0.009749   \n",
      "\n",
      "   precision_mean  precision_std  recall_mean  recall_std   f1_mean    f1_std  \\\n",
      "0        0.309707       0.040575     0.977778    0.049690  0.469344  0.050176   \n",
      "1        0.620000       0.073030     0.888889    0.078567  0.728882  0.067384   \n",
      "2        0.901010       0.136719     0.755556    0.121716  0.808413  0.068441   \n",
      "3        1.000000       0.000000     0.533333    0.049690  0.694505  0.044230   \n",
      "4        1.000000       0.000000     0.288889    0.060858  0.445455  0.074689   \n",
      "\n",
      "   mcc_mean   mcc_std  \n",
      "0  0.414365  0.066811  \n",
      "1  0.685080  0.082686  \n",
      "2  0.789208  0.070635  \n",
      "3  0.700128  0.036379  \n",
      "4  0.503115  0.057003  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "\n",
    "\n",
    "\n",
    "def flastRFWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization without PCA\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Define the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=param_dict['rf__n_estimators'],\n",
    "                criterion=param_dict['rf__criterion'],\n",
    "                max_depth=param_dict['rf__max_depth'],\n",
    "                min_samples_split=param_dict['rf__min_samples_split'],\n",
    "                min_samples_leaf=param_dict['rf__min_samples_leaf'],\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'rf__n_estimators': param_dict['rf__n_estimators'],\n",
    "                    'rf__criterion': param_dict['rf__criterion'],\n",
    "                    'rf__max_depth': param_dict['rf__max_depth'],\n",
    "                    'rf__min_samples_split': param_dict['rf__min_samples_split'],\n",
    "                    'rf__min_samples_leaf': param_dict['rf__min_samples_leaf'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-rf-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold Random Forest analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        [ 'rf__n_estimators', 'rf__criterion', 'rf__max_depth', 'rf__min_samples_split', 'rf__min_samples_leaf', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-rf-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #parameter grid\n",
    "    param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],                   # Number of trees in the forest\n",
    "    'rf__max_depth': [10, 20, 30],                        # Maximum depth of the tree\n",
    "    'rf__min_samples_split': [5, 10],                     # Minimum number of samples required to split a node\n",
    "    'rf__min_samples_leaf': [2, 5],                       # Minimum number of samples required at a leaf node\n",
    "    'rf__criterion': ['gini', 'entropy']        \n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  \n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/rf_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/rf_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform Random Forest analysis with threshold adjustments\n",
    "    print(\"Starting Random Forest analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastRFWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5,\n",
    "        combination_label=\"rf_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20d7c2",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "286c8e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Training with parameters: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Per-fold Decision Tree analysis completed. Results saved to: dt_thresholds-thresholds-dt-results-per-fold.csv\n",
      "Averaged results saved to: dt_thresholds-thresholds-dt-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "  dt__criterion  dt__max_depth  dt__min_samples_split  dt__min_samples_leaf  \\\n",
      "0          gini             10                      5                     2   \n",
      "1          gini             10                      5                     2   \n",
      "2          gini             10                      5                     2   \n",
      "3          gini             10                      5                     2   \n",
      "4          gini             10                      5                     2   \n",
      "\n",
      "  dt__max_features  fold  threshold  accuracy  precision    recall    f1  \\\n",
      "0             None     1        0.1  0.775862      0.375  0.666667  0.48   \n",
      "1             None     1        0.2  0.775862      0.375  0.666667  0.48   \n",
      "2             None     1        0.3  0.775862      0.375  0.666667  0.48   \n",
      "3             None     1        0.4  0.775862      0.375  0.666667  0.48   \n",
      "4             None     1        0.5  0.775862      0.375  0.666667  0.48   \n",
      "\n",
      "        mcc  \n",
      "0  0.374737  \n",
      "1  0.374737  \n",
      "2  0.374737  \n",
      "3  0.374737  \n",
      "4  0.374737  \n",
      "Averaged Results:\n",
      "  dt__criterion  dt__max_depth  dt__min_samples_split  dt__min_samples_leaf  \\\n",
      "0       entropy             10                      5                     2   \n",
      "1       entropy             10                      5                     2   \n",
      "2       entropy             10                      5                     2   \n",
      "3       entropy             10                      5                     2   \n",
      "4       entropy             10                      5                     2   \n",
      "\n",
      "  dt__max_features  threshold  accuracy_mean  accuracy_std  precision_mean  \\\n",
      "0             log2        0.1       0.805626      0.038882        0.388333   \n",
      "1             log2        0.2       0.805626      0.038882        0.388333   \n",
      "2             log2        0.3       0.805626      0.038882        0.388333   \n",
      "3             log2        0.4       0.809074      0.031897        0.392255   \n",
      "4             log2        0.5       0.809074      0.031897        0.392255   \n",
      "\n",
      "   precision_std  recall_mean  recall_std   f1_mean    f1_std  mcc_mean  \\\n",
      "0       0.068617     0.377778    0.201843  0.357021  0.111626  0.263242   \n",
      "1       0.068617     0.377778    0.201843  0.357021  0.111626  0.263242   \n",
      "2       0.068617     0.377778    0.201843  0.357021  0.111626  0.263242   \n",
      "3       0.065161     0.377778    0.201843  0.360440  0.115178  0.267569   \n",
      "4       0.065161     0.377778    0.201843  0.360440  0.115178  0.267569   \n",
      "\n",
      "    mcc_std  \n",
      "0  0.091004  \n",
      "1  0.091004  \n",
      "2  0.091004  \n",
      "3  0.095385  \n",
      "4  0.095385  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def flastDTWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization without PCA\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Define the pipeline with PCA and Decision Tree\n",
    "        pipeline = Pipeline([\n",
    "            ('dt', DecisionTreeClassifier(\n",
    "                criterion=param_dict['dt__criterion'],\n",
    "                max_depth=param_dict['dt__max_depth'],\n",
    "                min_samples_split=param_dict['dt__min_samples_split'],\n",
    "                min_samples_leaf=param_dict['dt__min_samples_leaf'],\n",
    "                max_features=param_dict['dt__max_features'],\n",
    "                random_state=42,\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(pipeline.named_steps['dt'], \"predict_proba\"):\n",
    "                y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "            else:\n",
    "                # If predict_proba is not available, use decision_function\n",
    "                y_scores = pipeline.decision_function(X_test_dense)\n",
    "                y_pred_proba = np.vstack([1 - y_scores, y_scores]).T\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'dt__criterion': param_dict['dt__criterion'],\n",
    "                    'dt__max_depth': param_dict['dt__max_depth'],\n",
    "                    'dt__min_samples_split': param_dict['dt__min_samples_split'],\n",
    "                    'dt__min_samples_leaf': param_dict['dt__min_samples_leaf'],\n",
    "                    'dt__max_features': param_dict['dt__max_features'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-dt-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold Decision Tree analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        [ 'dt__criterion', 'dt__max_depth', 'dt__min_samples_split', 'dt__min_samples_leaf', 'dt__max_features', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-dt-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "        # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "    'dt__max_depth': [10, 20, 30],                  # Maximum depth of the decision tree\n",
    "    'dt__min_samples_split': [5, 10],               # Minimum number of samples required to split a node\n",
    "    'dt__min_samples_leaf': [2, 5],                 # Minimum number of samples required at a leaf node\n",
    "    'dt__criterion': ['gini', 'entropy'],           # Function to measure the quality of a split\n",
    "    'dt__max_features': [None, 'sqrt', 'log2'],     # Controls how many features to consider for splits\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  # Unbalanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/dt_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/dt_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis with threshold adjustments\n",
    "    print(\"Starting Decision Tree analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastDTWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5,\n",
    "        combination_label=\"dt_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39958783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
