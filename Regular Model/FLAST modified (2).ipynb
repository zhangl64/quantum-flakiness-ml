{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e54f4a5-348d-4e3f-9b6f-74f4506a9992",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20573e37-1565-44bd-a94d-5c52b8d3f678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis for flaky vs smaller non-flaky files (47 each)...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe2 in position 37: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/h_pmxr992m143v567my_k4lm0000gn/T/ipykernel_23692/2888888278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Perform KNN analysis for the first combination (flaky vs smaller non-flaky)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting KNN analysis for flaky vs smaller non-flaky files (47 each)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mbest_params_5folds_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score_5folds_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflastKNNWithGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutDirEqual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflakyZip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonFlakyZip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractDirEqual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"equal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mbest_params_3folds_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score_3folds_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflastKNNWithGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutDirEqual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflakyZip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonFlakyZip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractDirEqual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"equal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rp/h_pmxr992m143v567my_k4lm0000gn/T/ipykernel_23692/2888888278.py\u001b[0m in \u001b[0;36mflastKNNWithGridSearchCV\u001b[0;34m(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mextract_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonFlakyZip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonFlakyDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mdataPointsFlaky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataPoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflakyDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mdataPointsNonFlaky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataPoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonFlakyDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mdataPoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataPointsFlaky\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataPointsNonFlaky\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rp/h_pmxr992m143v567my_k4lm0000gn/T/ipykernel_23692/2888888278.py\u001b[0m in \u001b[0;36mgetDataPoints\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataPointName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfileIn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileIn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Ensure the document is not empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mdataPointsList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe2 in position 37: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Vectorizes the data points using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# KNN with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "def flastKNNWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the KNN model\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Expanded parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'metric': ['cosine', 'euclidean'],  # Distance metrics\n",
    "        'weights': ['uniform', 'distance'],  # Neighbor weighting schemes\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  \n",
    "        'recall': make_scorer(recall_score),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score),\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-knn-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_neighbors,metric,weights,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['n_neighbors']},{param['metric']},{param['weights']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"KNN analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting KNN analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastKNNWithGridSearchCV(outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, \"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastKNNWithGridSearchCV(outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, \"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform KNN analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting KNN analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastKNNWithGridSearchCV(outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, \"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastKNNWithGridSearchCV(outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, \"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26a58c-f768-4863-ba1a-411cd77b6202",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4f8789a-f8d2-40c6-b5e9-e9f6a9f34129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7099346405228758\n",
      "SVM analysis completed for 5-folds. Results saved to: equal-params-svm-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7252136752136752\n",
      "SVM analysis completed for 3-folds. Results saved to: equal-params-svm-3-folds.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7099346405228758\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7252136752136752\n",
      "Starting SVM analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.658546365914787\n",
      "SVM analysis completed for 5-folds. Results saved to: larger-params-svm-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.6215384615384615\n",
      "SVM analysis completed for 3-folds. Results saved to: larger-params-svm-3-folds.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.658546365914787\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.6215384615384615\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Vectorizes the data points using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# SVM with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "def flastSVMWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the SVM model\n",
    "    svm = SVC()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score, zero_division=0),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=0),\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-svm-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"C,kernel,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints) \n",
    "            fo.write(f\"{param['C']},{param['kernel']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"SVM analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting SVM analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastSVMWithGridSearchCV(outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, \"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastSVMWithGridSearchCV(outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, \"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform SVM analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting SVM analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastSVMWithGridSearchCV(outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, \"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastSVMWithGridSearchCV(outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, \"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb67b0a-56b4-4182-9c98-09be0f352ae0",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdaa8f8-efd5-407d-853a-ebdcbaba8e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Naive Bayes analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 191\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Perform Naive Bayes analysis for the first combination (flaky vs smaller non-flaky)\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Naive Bayes analysis for flaky vs smaller non-flaky files (47 each)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 191\u001b[0m best_params_5folds_1, df_results_5folds_1 \u001b[38;5;241m=\u001b[39m flastThreshold(\n\u001b[0;32m    192\u001b[0m     outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, \u001b[38;5;241m5\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, combination_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m\"\u001b[39m, param_grid\u001b[38;5;241m=\u001b[39mparam_grid)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest results for 5-fold on equal combination:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_results_5folds_1)\n",
      "Cell \u001b[1;32mIn[1], line 100\u001b[0m, in \u001b[0;36mflastThreshold\u001b[1;34m(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid)\u001b[0m\n\u001b[0;32m     97\u001b[0m metrics_per_combination \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Manually iterate over all combinations of hyperparameters\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m product(\u001b[38;5;241m*\u001b[39mparam_grid\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Convert params from tuple to dictionary\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     param_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_grid\u001b[38;5;241m.\u001b[39mkeys(), params))\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_dict\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Vectorizes the data points using CountVectorizer without dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Naive Bayes with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "def flastNBWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization without Random Projection\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0], \n",
    "    }\n",
    "\n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score),\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(nb, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-nb-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"alpha,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['alpha']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Naive Bayes analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Naive Bayes analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Naive Bayes analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastNBWithGridSearchCV(outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, \"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastNBWithGridSearchCV(outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, \"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Naive Bayes analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Naive Bayes analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastNBWithGridSearchCV(outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, \"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastNBWithGridSearchCV(outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, \"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b751a1-1c35-4150-97cc-7b00124c835e",
   "metadata": {},
   "source": [
    "XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "568fb140-510b-40d7-9308-d5d36a53ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best Parameters: {'eta': 0.3, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best F1 Score: 0.7767320261437909\n",
      "XGBoost analysis completed for 5-folds. Results saved to: equal-params-xgb-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best Parameters: {'eta': 0.3, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best F1 Score: 0.7580697668594896\n",
      "XGBoost analysis completed for 3-folds. Results saved to: equal-params-xgb-3-folds.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'eta': 0.3, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best F1 Score: 0.7767320261437909\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'eta': 0.3, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best F1 Score: 0.7580697668594896\n",
      "Starting XGBoost analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best F1 Score: 0.5917697988286224\n",
      "XGBoost analysis completed for 5-folds. Results saved to: larger-params-xgb-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best F1 Score: 0.5422843822843822\n",
      "XGBoost analysis completed for 3-folds. Results saved to: larger-params-xgb-3-folds.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best F1 Score: 0.5917697988286224\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best F1 Score: 0.5422843822843822\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# XGBoost with GridSearchCV and Multiple Scoring Metrics including MCC\n",
    "\n",
    "def flastXGBWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(eval_metric=\"logloss\")\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'eta': [0.01, 0.1, 0.3, 0.5],  # Learning rate\n",
    "        'max_depth': [3, 5, 7, 10],  # Tree depth\n",
    "        'n_estimators': [50, 100, 200, 300],  # Number of boosting rounds\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  # Handle undefined precision\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),  # Handle undefined F1 score\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-xgb-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"eta,max_depth,n_estimators,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['eta']},{param['max_depth']},{param['n_estimators']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform XGBoost analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting XGBoost analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastXGBWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastXGBWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform XGBoost analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting XGBoost analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastXGBWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastXGBWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f8472-3d27-4443-893d-537f37dfbc4e",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1921b168-d230-4308-8c58-4d557ac68d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best F1 Score: 0.7642063492063492\n",
      "Random Forest analysis completed for 5-folds. Results saved to: equal-params-rf-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best F1 Score: 0.7433954933954934\n",
      "Random Forest analysis completed for 3-folds. Results saved to: equal-params-rf-3-folds.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best F1 Score: 0.7642063492063492\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best F1 Score: 0.7433954933954934\n",
      "Starting Random Forest analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Best F1 Score: 0.5147279191396839\n",
      "Random Forest analysis completed for 5-folds. Results saved to: larger-params-rf-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Best F1 Score: 0.48428383210991904\n",
      "Random Forest analysis completed for 3-folds. Results saved to: larger-params-rf-3-folds.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Best F1 Score: 0.5147279191396839\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Best F1 Score: 0.48428383210991904\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Random Forest with GridSearchCV and Multiple Scoring Metrics including MCC\n",
    "\n",
    "def flastRFWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define Random Forest model\n",
    "    rf_model = RandomForestClassifier()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 300, 500],  # Number of trees\n",
    "        'max_depth': [10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "        \"criterion\": [\"gini\", \"entropy\"],  # Function to measure the quality of a split\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  \n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(rf_model, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-rf-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_estimators,max_depth,min_samples_split,min_samples_leaf,criterion,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['n_estimators']},{param['max_depth']},{param['min_samples_split']},{param['min_samples_leaf']},{param['criterion']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Random Forest analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Random Forest analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Random Forest analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastRFWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastRFWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Random Forest analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Random Forest analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastRFWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastRFWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59ce10-674c-4a3b-bd92-4513197164b4",
   "metadata": {},
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c76c4f90-1da7-4ca7-b288-d6f2a43657a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 500, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "Best F1 Score: 0.7479713423831071\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: equal-params-dt-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 504 candidates, totalling 1512 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Best F1 Score: 0.73109243697479\n",
      "Decision Tree analysis completed for 3-folds. Results saved to: equal-params-dt-3-folds.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 500, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "Best F1 Score: 0.7479713423831071\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Best F1 Score: 0.73109243697479\n",
      "Starting Decision Tree analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 500, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best F1 Score: 0.5916683866529068\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: larger-params-dt-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 3 folds for each of 504 candidates, totalling 1512 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best F1 Score: 0.5779072082835524\n",
      "Decision Tree analysis completed for 3-folds. Results saved to: larger-params-dt-3-folds.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 500, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best F1 Score: 0.5916683866529068\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best F1 Score: 0.5779072082835524\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree with GridSearchCV and Multiple Scoring Metrics including MCC\n",
    "\n",
    "def flastDTWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define Decision Tree model\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "        'max_depth': [None, 10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 5, 10],  # Minimum number of samples required to be at a leaf node\n",
    "        'max_features': [None, 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  \n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(dt_model, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-dt-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"criterion,max_depth,min_samples_split,min_samples_leaf,max_features,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['criterion']},{param['max_depth']},{param['min_samples_split']},{param['min_samples_leaf']},{param['max_features']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastDTWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastDTWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Decision Tree analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastDTWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastDTWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbbf7ac-e0fc-4203-8b6d-793d28211d02",
   "metadata": {},
   "source": [
    "Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558f714-f7cf-4763-b225-5faa242c778f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6571dc7b",
   "metadata": {},
   "source": [
    "Get best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the best results from the CSV files of each model\n",
    "def extract_best_results(model_name, combination, fold, csv_file):\n",
    "    \"\"\"\n",
    "    Extracts the best result from the CSV file for a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model (e.g., \"KNN\", \"SVM\")\n",
    "    - combination: The combination of flaky and non-flaky files (e.g., \"equal\", \"larger\")\n",
    "    - fold: Number of folds (e.g., \"5-fold\" or \"3-fold\")\n",
    "    - csv_file: The path to the CSV file containing the model's results\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the best results for the model, combination, and fold.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold}) does not exist: {csv_file}\")\n",
    "        return None\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold}) is empty: {csv_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Get the row with the best F1 score\n",
    "    best_row = df.loc[df['f1'].idxmax()]\n",
    "    \n",
    "    # Extract metrics\n",
    "    accuracy = best_row['accuracy']\n",
    "    precision = best_row['precision']\n",
    "    recall = best_row['recall']\n",
    "    f1 = best_row['f1']\n",
    "    mcc = best_row.get('mcc', None)  # Get MCC if available\n",
    "    \n",
    "    # Collect parameters (exclude known metric columns)\n",
    "    metric_columns = ['accuracy', 'precision', 'recall', 'f1', 'mcc', 'preparationTime']\n",
    "    parameter_columns = [col for col in df.columns if col not in metric_columns]\n",
    "    parameters = {col: best_row[col] for col in parameter_columns}\n",
    "    \n",
    "    # Create a combined model name\n",
    "    combined_model_name = f\"{combination} {model_name}\"\n",
    "    \n",
    "    # Collect the best results into a dictionary\n",
    "    best_results = {\n",
    "        'Model': combined_model_name,\n",
    "        'Fold': fold,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'MCC': mcc,\n",
    "        'Parameters': parameters\n",
    "    }\n",
    "    \n",
    "    return best_results\n",
    "\n",
    "# Function to gather and print/save the best results from a single combination\n",
    "def gather_best_results_for_combination(models_results_dir, output_file, combination):\n",
    "    \"\"\"\n",
    "    Gathers the best results from all models for a specific combination (e.g., \"equal\", \"larger\") and writes them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - models_results_dir: Directory where the model result CSV files are stored for the combination.\n",
    "    - output_file: Path to the output CSV file to store the best results for the combination.\n",
    "    - combination: The combination name (e.g., \"equal\", \"larger\").\n",
    "    \"\"\"\n",
    "    # List of models and their corresponding result files for both 5-fold and 3-fold\n",
    "    models = {\n",
    "        'KNN': {'5-fold': 'params-knn-5-folds.csv', '3-fold': 'params-knn-3-folds.csv'},\n",
    "        'SVM': {'5-fold': 'params-svm-5-folds.csv', '3-fold': 'params-svm-3-folds.csv'},\n",
    "        'Naive Bayes': {'5-fold': 'params-nb-5-folds.csv', '3-fold': 'params-nb-3-folds.csv'},\n",
    "        'XGBoost': {'5-fold': 'params-xgb-5-folds.csv', '3-fold': 'params-xgb-3-folds.csv'},\n",
    "        'Random Forest': {'5-fold': 'params-rf-5-folds.csv', '3-fold': 'params-rf-3-folds.csv'},\n",
    "        'Decision Tree': {'5-fold': 'params-dt-5-folds.csv', '3-fold': 'params-dt-3-folds.csv'}\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the best results from each model and fold\n",
    "    best_results = []\n",
    "\n",
    "    # Iterate over each model and its result files for both 5-fold and 3-fold\n",
    "    for model_name, folds in models.items():\n",
    "        for fold_label, csv_file in folds.items():\n",
    "            # Adjust the filename to include the combination prefix (e.g., equal-params-xgb-5-folds.csv)\n",
    "            full_csv_file = f\"{combination}-{csv_file}\"\n",
    "            full_csv_path = os.path.join(models_results_dir, full_csv_file)\n",
    "            best_result = extract_best_results(model_name, combination, fold_label, full_csv_path)\n",
    "            if best_result:\n",
    "                best_results.append(best_result)\n",
    "\n",
    "    if not best_results:\n",
    "        print(f\"No best results found for combination {combination}.\")\n",
    "        return\n",
    "\n",
    "    # Convert the list of best results into a DataFrame\n",
    "    best_results_df = pd.DataFrame(best_results)\n",
    "    \n",
    "    # Reorder columns for clarity\n",
    "    columns = ['Model', 'Fold', 'Accuracy', 'Precision', 'Recall', 'F1', 'MCC', 'Parameters']\n",
    "    best_results_df = best_results_df[columns]\n",
    "    \n",
    "    # Save the best results to the output CSV file\n",
    "    best_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Best results for {combination} combination saved to: {output_file}\")\n",
    "    \n",
    "    # Print the best results as a table\n",
    "    print(f\"\\nBest Results from All Models for {combination} Combination:\")\n",
    "    print(best_results_df.to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories where the model result CSV files are stored for each combination\n",
    "    equal_results_dir = 'results/equal_flaky_nonflaky/'\n",
    "    larger_results_dir = 'results/larger_nonflaky/'\n",
    "\n",
    "    # Paths to the output CSV files where best results will be stored for each combination\n",
    "    equal_output_file = \"best_results_equal_combination.csv\"\n",
    "    larger_output_file = \"best_results_larger_combination.csv\"\n",
    "\n",
    "    # Gather and save the best results for the equal combination\n",
    "    gather_best_results_for_combination(equal_results_dir, equal_output_file, \"equal\")\n",
    "\n",
    "    # Gather and save the best results for the larger combination\n",
    "    gather_best_results_for_combination(larger_results_dir, larger_output_file, \"larger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d698ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined best results saved to: combined_best_results.csv\n",
      "\n",
      "Combined Best Results from All Models:\n",
      "                  Model   Fold  Accuracy  Precision   Recall       F1       MCC                                                                                                                           Parameters\n",
      "    equal Decision Tree 3-fold  0.563830   0.551724 0.680851 0.666667  0.131306 {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'threshold': 0.1}\n",
      "    equal Decision Tree 5-fold  0.478723   0.487179 0.808511 0.677165 -0.056614 {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2', 'threshold': 0.2}\n",
      "              equal KNN 3-fold  0.735215   0.796296 0.663889 0.709513  0.495859                                                                         {'n_neighbors': 3, 'metric': 'cosine', 'weights': 'uniform'}\n",
      "              equal KNN 5-fold  0.765497   0.788333 0.768889 0.769161  0.545171                                                                        {'n_neighbors': 3, 'metric': 'cosine', 'weights': 'distance'}\n",
      "      equal Naive Bayes 3-fold  0.766465   0.836219 0.662500 0.736180  0.547612                                                                                                                     {'alpha': 0.001}\n",
      "      equal Naive Bayes 5-fold  0.797661   0.833232 0.768889 0.790175  0.609148                                                                                                                     {'alpha': 0.001}\n",
      "    equal Random Forest 3-fold  0.777554   0.779762 0.768056 0.772625  0.556544                        {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "    equal Random Forest 5-fold  0.850292   0.873333 0.833333 0.847513  0.709358                           {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "              equal SVM 3-fold  0.744624   0.797199 0.683333 0.725214  0.506149                                                                                                        {'C': 100.0, 'kernel': 'rbf'}\n",
      "              equal SVM 5-fold  0.722807   0.753463 0.682222 0.709935  0.455411                                                                                                        {'C': 100.0, 'kernel': 'rbf'}\n",
      "          equal XGBoost 3-fold  0.734043   0.703704 0.808511 0.752475  0.473365                                                    {'learning_rate': 0.1, 'max_depth': 5.0, 'n_estimators': 200.0, 'threshold': 0.3}\n",
      "          equal XGBoost 5-fold  0.776596   0.782609 0.765957 0.774194  0.553317                                                     {'learning_rate': 0.1, 'max_depth': 3.0, 'n_estimators': 50.0, 'threshold': 0.5}\n",
      "not equal Decision Tree 3-fold  0.810631   0.391304 0.382979 0.425532  0.275150 {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'threshold': 0.5}\n",
      "not equal Decision Tree 5-fold  0.740864   0.281690 0.425532 0.450450  0.192159   {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'threshold': 0.3}\n",
      "          not equal KNN 3-fold  0.883762   0.742857 0.386111 0.503188  0.479011                                                                        {'n_neighbors': 3, 'metric': 'cosine', 'weights': 'distance'}\n",
      "          not equal KNN 5-fold  0.900492   0.871429 0.428889 0.537540  0.545645                                                                        {'n_neighbors': 5, 'metric': 'cosine', 'weights': 'distance'}\n",
      "  not equal Naive Bayes 3-fold  0.916931   0.781746 0.661111 0.716049  0.671153                                                                                                                       {'alpha': 0.1}\n",
      "  not equal Naive Bayes 5-fold  0.940219   0.866429 0.742222 0.792924  0.765654                                                                                                                     {'alpha': 0.001}\n",
      "not equal Random Forest 3-fold  0.890396   0.902778 0.343056 0.487747  0.510325                        {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "not equal Random Forest 5-fold  0.893661   0.779524 0.444444 0.563526  0.535607                            {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "          not equal SVM 3-fold  0.903597   0.794949 0.512500 0.621538  0.588593                                                                                                        {'C': 100.0, 'kernel': 'rbf'}\n",
      "          not equal SVM 5-fold  0.910328   0.831111 0.573333 0.658546  0.635199                                                                                                      {'C': 0.01, 'kernel': 'linear'}\n",
      "      not equal XGBoost 3-fold  0.900332   0.904762 0.404255 0.558824  0.564794                                                    {'learning_rate': 0.1, 'max_depth': 5.0, 'n_estimators': 200.0, 'threshold': 0.5}\n",
      "      not equal XGBoost 5-fold  0.897010   0.653846 0.723404 0.686869  0.626571                                                    {'learning_rate': 0.3, 'max_depth': 3.0, 'n_estimators': 100.0, 'threshold': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the best results from the CSV files of each model\n",
    "def extract_best_results(model_name, combination, fold, csv_file):\n",
    "    \"\"\"\n",
    "    Extracts the best result from the CSV file for a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model (e.g., \"KNN\", \"SVM\")\n",
    "    - combination: The combination of flaky and non-flaky files (e.g., \"equal\", \"larger\")\n",
    "    - fold: Number of folds (e.g., \"5-fold\" or \"3-fold\")\n",
    "    - csv_file: The path to the CSV file containing the model's results\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the best results for the model, combination, and fold.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold}) does not exist: {csv_file}\")\n",
    "        return None\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold}) is empty: {csv_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Get the row with the best F1 score\n",
    "    best_row = df.loc[df['f1'].idxmax()]\n",
    "    \n",
    "    # Extract metrics\n",
    "    accuracy = best_row['accuracy']\n",
    "    precision = best_row['precision']\n",
    "    recall = best_row['recall']\n",
    "    f1 = best_row['f1']\n",
    "    mcc = best_row.get('mcc', None)  # Get MCC if available\n",
    "    \n",
    "    # Collect parameters (exclude known metric columns)\n",
    "    metric_columns = ['accuracy', 'precision', 'recall', 'f1', 'mcc', 'preparationTime']\n",
    "    parameter_columns = [col for col in df.columns if col not in metric_columns]\n",
    "    parameters = {col: best_row[col] for col in parameter_columns}\n",
    "    \n",
    "    # Create a combined model name (e.g., 'equal KNN' or 'not equal KNN')\n",
    "    combination_label = 'not equal' if combination == 'larger' else 'equal'\n",
    "    combined_model_name = f\"{combination_label} {model_name}\"\n",
    "    \n",
    "    # Collect the best results into a dictionary\n",
    "    best_results = {\n",
    "        'Model': combined_model_name,\n",
    "        'Fold': fold,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'MCC': mcc,\n",
    "        'Parameters': parameters\n",
    "    }\n",
    "    \n",
    "    return best_results\n",
    "\n",
    "# Function to gather and print/save the best results from all models and combinations\n",
    "def gather_best_results(models_results_dirs, output_file):\n",
    "    \"\"\"\n",
    "    Gathers the best results from all models for both combinations and writes them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - models_results_dirs: Dictionary mapping combination names to their result directories.\n",
    "    - output_file: Path to the output CSV file to store the best results.\n",
    "    \"\"\"\n",
    "    # List of models and their corresponding result files for both 5-fold and 3-fold\n",
    "    models = {\n",
    "        'KNN': {'5-fold': 'params-knn-5-folds.csv', '3-fold': 'params-knn-3-folds.csv'},\n",
    "        'SVM': {'5-fold': 'params-svm-5-folds.csv', '3-fold': 'params-svm-3-folds.csv'},\n",
    "        'Naive Bayes': {'5-fold': 'params-nb-5-folds.csv', '3-fold': 'params-nb-3-folds.csv'},\n",
    "        'XGBoost': {'5-fold': 'params-xgb-5-folds.csv', '3-fold': 'params-xgb-3-folds.csv'},\n",
    "        'Random Forest': {'5-fold': 'params-rf-5-folds.csv', '3-fold': 'params-rf-3-folds.csv'},\n",
    "        'Decision Tree': {'5-fold': 'params-dt-5-folds.csv', '3-fold': 'params-dt-3-folds.csv'}\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the best results from each model, combination, and fold\n",
    "    best_results = []\n",
    "\n",
    "    # Iterate over each model, fold, and combination\n",
    "    for model_name, folds in models.items():\n",
    "        for fold_label, csv_file in folds.items():\n",
    "            for combination, results_dir in models_results_dirs.items():\n",
    "                # Adjust the filename to include the combination prefix (e.g., equal-params-xgb-5-folds.csv)\n",
    "                full_csv_file = f\"{combination}-{csv_file}\"\n",
    "                full_csv_path = os.path.join(results_dir, full_csv_file)\n",
    "                best_result = extract_best_results(model_name, combination, fold_label, full_csv_path)\n",
    "                if best_result:\n",
    "                    best_results.append(best_result)\n",
    "\n",
    "    if not best_results:\n",
    "        print(f\"No best results found.\")\n",
    "        return\n",
    "\n",
    "    # Convert the list of best results into a DataFrame\n",
    "    best_results_df = pd.DataFrame(best_results)\n",
    "    \n",
    "    # Reorder columns for clarity\n",
    "    columns = ['Model', 'Fold', 'Accuracy', 'Precision', 'Recall', 'F1', 'MCC', 'Parameters']\n",
    "    best_results_df = best_results_df[columns]\n",
    "    \n",
    "    # Add sorting helper columns\n",
    "    # Extract model name (e.g., 'KNN', 'SVM')\n",
    "    best_results_df['Model_Name'] = best_results_df['Model'].apply(lambda x: x.split(' ', 1)[1])\n",
    "    # Extract combination order (0 for 'equal', 1 for 'not equal')\n",
    "    best_results_df['Combination_Order'] = best_results_df['Model'].apply(lambda x: 0 if 'equal' in x else 1)\n",
    "    # Extract fold number (e.g., 5 or 3)\n",
    "    best_results_df['Fold_Number'] = best_results_df['Fold'].apply(lambda x: int(x.split('-')[0]))\n",
    "    \n",
    "    # Sort the DataFrame\n",
    "    best_results_df = best_results_df.sort_values(by=['Model_Name', 'Fold_Number', 'Combination_Order'])\n",
    "    \n",
    "    # Drop helper columns\n",
    "    best_results_df = best_results_df.drop(columns=['Model_Name', 'Combination_Order', 'Fold_Number'])\n",
    "    \n",
    "    # Save the best results to the output CSV file\n",
    "    best_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined best results saved to: {output_file}\")\n",
    "    \n",
    "    # Print the best results as a table\n",
    "    print(\"\\nCombined Best Results from All Models:\")\n",
    "    print(best_results_df.to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories where the model result CSV files are stored for each combination\n",
    "    models_results_dirs = {\n",
    "        'equal': 'results/equal_flaky_nonflaky/',\n",
    "        'larger': 'results/larger_nonflaky/'  # 'larger' will be labeled as 'not equal' in output\n",
    "    }\n",
    "\n",
    "    # Path to the output CSV file where best results will be stored\n",
    "    output_file = \"combined_best_results.csv\"\n",
    "\n",
    "    # Gather and save the combined best results\n",
    "    gather_best_results(models_results_dirs, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f338f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a63846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
