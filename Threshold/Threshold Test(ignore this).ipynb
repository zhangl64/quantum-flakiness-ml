{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ea354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "482b7176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Per-fold KNN analysis completed. Results saved to: knn_thresholds-thresholds-knn-results-per-fold.csv\n",
      "Averaged results saved to: knn_thresholds-thresholds-knn-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   n_neighbors  weights  metric  fold  threshold  accuracy  precision  \\\n",
      "0            3  uniform  cosine     1        0.1  0.866667   0.538462   \n",
      "1            3  uniform  cosine     1        0.2  0.866667   0.538462   \n",
      "2            3  uniform  cosine     1        0.3  0.866667   0.538462   \n",
      "3            3  uniform  cosine     1        0.4  0.900000   0.714286   \n",
      "4            3  uniform  cosine     1        0.5  0.900000   0.714286   \n",
      "\n",
      "     recall        f1       mcc  \n",
      "0  0.777778  0.636364  0.572158  \n",
      "1  0.777778  0.636364  0.572158  \n",
      "2  0.777778  0.636364  0.572158  \n",
      "3  0.555556  0.625000  0.574321  \n",
      "4  0.555556  0.625000  0.574321  \n",
      "Averaged Results:\n",
      "   n_neighbors   weights  metric  threshold  accuracy_mean  accuracy_std  \\\n",
      "0            3  distance  cosine        0.1       0.862712      0.038973   \n",
      "1            3  distance  cosine        0.2       0.866102      0.027260   \n",
      "2            3  distance  cosine        0.3       0.866045      0.036464   \n",
      "3            3  distance  cosine        0.4       0.886158      0.032798   \n",
      "4            3  distance  cosine        0.5       0.892881      0.028442   \n",
      "\n",
      "   precision_mean  precision_std  recall_mean  recall_std   f1_mean    f1_std  \\\n",
      "0        0.537754       0.103675     0.711111    0.168508  0.607287  0.114010   \n",
      "1        0.545253       0.076663     0.688889    0.164804  0.603205  0.095843   \n",
      "2        0.542273       0.116377     0.644444    0.213726  0.582518  0.144766   \n",
      "3        0.678968       0.196521     0.511111    0.099381  0.576615  0.115180   \n",
      "4        0.716825       0.173102     0.511111    0.099381  0.589683  0.106233   \n",
      "\n",
      "   mcc_mean   mcc_std  \n",
      "0  0.538181  0.137385  \n",
      "1  0.533845  0.114623  \n",
      "2  0.511285  0.169764  \n",
      "3  0.523222  0.143010  \n",
      "4  0.544440  0.126316  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction via PCA.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints).toarray()  # Convert to dense matrix for PCA\n",
    "\n",
    "    if dim <= 0:\n",
    "        # Automatically set the number of components based on the variance explained\n",
    "        pca = PCA(n_components=eps)  # Use 'eps' as the variance to explain\n",
    "        Z = pca.fit_transform(Z_full)\n",
    "    else:\n",
    "        # Use specified number of components\n",
    "        pca = PCA(n_components=dim)\n",
    "        Z = pca.fit_transform(Z_full)\n",
    "\n",
    "    return Z\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Modified KNN with Threshold Adjustment\n",
    "\n",
    "def flastKNNWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize KNeighborsClassifier with current hyperparameter combination\n",
    "        knn_model = KNeighborsClassifier(\n",
    "            n_neighbors=param_dict['n_neighbors'],\n",
    "            weights=param_dict['weights'],\n",
    "            metric=param_dict['metric'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            knn_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(knn_model, \"predict_proba\"):\n",
    "                y_pred_proba = knn_model.predict_proba(X_test)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = knn_model.kneighbors(X_test)\n",
    "                weights = knn_model._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / knn_model.n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'metric': param_dict['metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-knn-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold KNN analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['n_neighbors', 'weights', 'metric', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-knn-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['cosine', 'euclidean'],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/knn_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/knn_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis with threshold adjustments\n",
    "    print(\"Starting KNN analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastKNNWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"knn_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2f8ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Per-fold KNN analysis completed. Results saved to: knn_thresholds-thresholds-knn-results-per-fold.csv\n",
      "Averaged results saved to: knn_thresholds-thresholds-knn-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   n_neighbors  weights  metric  fold  threshold  accuracy  precision  \\\n",
      "0            3  uniform  cosine     1        0.1      0.75   0.285714   \n",
      "1            3  uniform  cosine     1        0.2      0.75   0.285714   \n",
      "2            3  uniform  cosine     1        0.3      0.75   0.285714   \n",
      "3            3  uniform  cosine     1        0.4      0.90   1.000000   \n",
      "4            3  uniform  cosine     1        0.5      0.90   1.000000   \n",
      "\n",
      "     recall        f1       mcc  \n",
      "0  0.444444  0.347826  0.209679  \n",
      "1  0.444444  0.347826  0.209679  \n",
      "2  0.444444  0.347826  0.209679  \n",
      "3  0.333333  0.500000  0.546119  \n",
      "4  0.333333  0.500000  0.546119  \n",
      "Averaged Results:\n",
      "   n_neighbors   weights  metric  threshold  accuracy_mean  accuracy_std  \\\n",
      "0            3  distance  cosine        0.1       0.662260      0.106102   \n",
      "1            3  distance  cosine        0.2       0.662260      0.106102   \n",
      "2            3  distance  cosine        0.3       0.665593      0.106536   \n",
      "3            3  distance  cosine        0.4       0.789322      0.090135   \n",
      "4            3  distance  cosine        0.5       0.796102      0.091316   \n",
      "\n",
      "   precision_mean  precision_std  recall_mean  recall_std   f1_mean    f1_std  \\\n",
      "0        0.229780       0.078454     0.511111    0.216595  0.310638  0.107284   \n",
      "1        0.229780       0.078454     0.511111    0.216595  0.310638  0.107284   \n",
      "2        0.232120       0.077909     0.511111    0.216595  0.312754  0.106772   \n",
      "3        0.333333       0.390868     0.200000    0.144871  0.224444  0.184659   \n",
      "4        0.313333       0.392711     0.177778    0.149071  0.201587  0.185076   \n",
      "\n",
      "   mcc_mean   mcc_std  \n",
      "0  0.154316  0.147964  \n",
      "1  0.154316  0.147964  \n",
      "2  0.157720  0.147038  \n",
      "3  0.133036  0.249931  \n",
      "4  0.121081  0.241114  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction via PCA.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints).toarray()  # Convert to dense matrix for PCA\n",
    "\n",
    "    if dim <= 0:\n",
    "        # Automatically set the number of components based on the variance explained\n",
    "        pca = PCA(n_components=eps)  # Use 'eps' as the variance to explain\n",
    "        Z = pca.fit_transform(Z_full)\n",
    "    else:\n",
    "        # Use specified number of components\n",
    "        pca = PCA(n_components=dim)\n",
    "        Z = pca.fit_transform(Z_full)\n",
    "\n",
    "    return Z\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Modified KNN with Threshold Adjustment\n",
    "\n",
    "def flastKNNWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize Pipeline with PCA and KNN\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=dim)),  # PCA step with dynamic components\n",
    "            ('knn', KNeighborsClassifier(\n",
    "                n_neighbors=param_dict['n_neighbors'],\n",
    "                weights=param_dict['weights'],\n",
    "                metric=param_dict['metric'],\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the pipeline model (PCA + KNN)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(pipeline.named_steps['knn'], \"predict_proba\"):\n",
    "                y_pred_proba = pipeline.named_steps['knn'].predict_proba(X_test)\n",
    "            else:\n",
    "                distances, indices = pipeline.named_steps['knn'].kneighbors(X_test)\n",
    "                weights = pipeline.named_steps['knn']._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / pipeline.named_steps['knn'].n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'metric': param_dict['metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-knn-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold KNN analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['n_neighbors', 'weights', 'metric', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-knn-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['cosine', 'euclidean'],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/knn_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/knn_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis with threshold adjustments\n",
    "    print(\"Starting KNN analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastKNNWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"knn_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b4a4a",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a22ec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NB analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'sigmoid'}\n",
      "Per-fold SVM analysis completed. Results saved to: NB_thresholds-thresholds-svm-results-per-fold.csv\n",
      "Averaged results saved to: NB_thresholds-thresholds-svm-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "      C  kernel  fold  threshold  accuracy  precision    recall        f1  \\\n",
      "0  0.01  linear     1        0.1  0.166667   0.152542  1.000000  0.264706   \n",
      "1  0.01  linear     1        0.2  0.866667   0.666667  0.222222  0.333333   \n",
      "2  0.01  linear     1        0.3  0.866667   1.000000  0.111111  0.200000   \n",
      "3  0.01  linear     1        0.4  0.866667   1.000000  0.111111  0.200000   \n",
      "4  0.01  linear     1        0.5  0.850000   1.000000  0.000000  0.000000   \n",
      "\n",
      "        mcc  \n",
      "0  0.054690  \n",
      "1  0.331954  \n",
      "2  0.309912  \n",
      "3  0.309912  \n",
      "4  0.000000  \n",
      "Averaged Results:\n",
      "      C  kernel  threshold  accuracy_mean  accuracy_std  precision_mean  \\\n",
      "0  0.01  linear        0.1       0.351243      0.156093        0.185429   \n",
      "1  0.01  linear        0.2       0.876102      0.033442        0.625458   \n",
      "2  0.01  linear        0.3       0.869492      0.036500        0.800000   \n",
      "3  0.01  linear        0.4       0.852712      0.025885        0.800000   \n",
      "4  0.01  linear        0.5       0.849435      0.012807        0.800000   \n",
      "\n",
      "   precision_std  recall_mean  recall_std   f1_mean    f1_std  mcc_mean  \\\n",
      "0       0.035228     0.933333    0.099381  0.307808  0.049359  0.154695   \n",
      "1       0.120180     0.466667    0.298142  0.494964  0.203159  0.458024   \n",
      "2       0.273861     0.200000    0.213726  0.280759  0.266368  0.294057   \n",
      "3       0.447214     0.066667    0.099381  0.112727  0.164844  0.134497   \n",
      "4       0.447214     0.022222    0.049690  0.040000  0.089443  0.050841   \n",
      "\n",
      "    mcc_std  \n",
      "0  0.111082  \n",
      "1  0.196873  \n",
      "2  0.264528  \n",
      "3  0.227660  \n",
      "4  0.146820  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        from sklearn.random_projection import SparseRandomProjection\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Modified SVM with Threshold Adjustment\n",
    "\n",
    "def flastNBWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "     # Initialize MultinomialNB with current hyperparameter combination\n",
    "        nb_model = MultinomialNB(\n",
    "            alpha=param_dict['alpha']\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            svm_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = svm_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'alpha': param_dict['alpha'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-nb-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold SVM analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['C', 'kernel', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-NB-result-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/NB_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/NB_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis with threshold adjustments\n",
    "    print(\"Starting NB analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastSVMWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"NB_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6b8a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Per-fold KNN analysis completed. Results saved to: knn_thresholds-thresholds-knn-results-per-fold.csv\n",
      "Averaged results saved to: knn_thresholds-thresholds-knn-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   n_neighbors  weights  metric  fold  threshold  accuracy  precision  \\\n",
      "0            3  uniform  cosine     1        0.1  0.833333   0.466667   \n",
      "1            3  uniform  cosine     1        0.2  0.833333   0.466667   \n",
      "2            3  uniform  cosine     1        0.3  0.833333   0.466667   \n",
      "3            3  uniform  cosine     1        0.4  0.883333   0.666667   \n",
      "4            3  uniform  cosine     1        0.5  0.883333   0.666667   \n",
      "\n",
      "     recall        f1       mcc  \n",
      "0  0.777778  0.583333  0.512020  \n",
      "1  0.777778  0.583333  0.512020  \n",
      "2  0.777778  0.583333  0.512020  \n",
      "3  0.444444  0.533333  0.482319  \n",
      "4  0.444444  0.533333  0.482319  \n",
      "Averaged Results:\n",
      "   n_neighbors   weights  metric  threshold  accuracy_mean  accuracy_std  \\\n",
      "0            3  distance  cosine        0.1       0.859605      0.046307   \n",
      "1            3  distance  cosine        0.2       0.866271      0.043918   \n",
      "2            3  distance  cosine        0.3       0.879605      0.060532   \n",
      "3            3  distance  cosine        0.4       0.902994      0.024789   \n",
      "4            3  distance  cosine        0.5       0.892938      0.022598   \n",
      "\n",
      "   precision_mean  precision_std  recall_mean  recall_std   f1_mean    f1_std  \\\n",
      "0        0.527273       0.118894     0.755556    0.198762  0.616190  0.132292   \n",
      "1        0.546061       0.124374     0.755556    0.198762  0.627368  0.130685   \n",
      "2        0.591439       0.205101     0.733333    0.243432  0.645197  0.193243   \n",
      "3        0.728225       0.114828     0.577778    0.164804  0.634048  0.123654   \n",
      "4        0.740000       0.173845     0.488889    0.126686  0.575107  0.095410   \n",
      "\n",
      "   mcc_mean   mcc_std  \n",
      "0  0.551195  0.165683  \n",
      "1  0.565243  0.161132  \n",
      "2  0.586334  0.233941  \n",
      "3  0.591240  0.129005  \n",
      "4  0.539763  0.110082  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d37bf2d8",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4942bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rf analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Per-fold rf analysis completed. Results saved to: rf_thresholds-params-rf-5-folds-Threshold.csv\n",
      "Averaged results saved to: rf_thresholds-thresholds-RF-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   n_estimators criterion  max_depth  min_samples_split  min_samples_leaf  \\\n",
      "0            10      gini         10                  2                 1   \n",
      "1            10      gini         10                  2                 1   \n",
      "2            10      gini         10                  2                 1   \n",
      "3            10      gini         10                  2                 1   \n",
      "4            10      gini         10                  2                 1   \n",
      "\n",
      "   fold  threshold  accuracy  precision    recall        f1       mcc  \n",
      "0     1        0.1  0.533333   0.243243  1.000000  0.391304  0.331207  \n",
      "1     1        0.2  0.666667   0.280000  0.777778  0.411765  0.307698  \n",
      "2     1        0.3  0.800000   0.333333  0.333333  0.333333  0.215686  \n",
      "3     1        0.4  0.800000   0.333333  0.333333  0.333333  0.215686  \n",
      "4     1        0.5  0.866667   0.600000  0.333333  0.428571  0.379980  \n",
      "Averaged Results:\n",
      "   n_estimators criterion  max_depth  min_samples_split  min_samples_leaf  \\\n",
      "0            10   entropy         10                  2                 1   \n",
      "1            10   entropy         10                  2                 1   \n",
      "2            10   entropy         10                  2                 1   \n",
      "3            10   entropy         10                  2                 1   \n",
      "4            10   entropy         10                  2                 1   \n",
      "\n",
      "   threshold  accuracy_mean  accuracy_std  precision_mean  precision_std  \\\n",
      "0        0.1       0.524972      0.081238        0.235276       0.038888   \n",
      "1        0.2       0.742260      0.068517        0.356857       0.092737   \n",
      "2        0.3       0.869379      0.052718        0.600556       0.301964   \n",
      "3        0.4       0.869379      0.052718        0.600556       0.301964   \n",
      "4        0.5       0.862768      0.040340        0.597619       0.319704   \n",
      "\n",
      "   recall_mean  recall_std   f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0     0.933333    0.060858  0.375240  0.053986  0.282377  0.095587  \n",
      "1     0.822222    0.149071  0.494720  0.106940  0.416280  0.144500  \n",
      "2     0.444444    0.272166  0.483229  0.242134  0.433707  0.267995  \n",
      "3     0.444444    0.272166  0.483229  0.242134  0.433707  0.267995  \n",
      "4     0.288889    0.201843  0.370163  0.219669  0.340021  0.238421  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        from sklearn.random_projection import SparseRandomProjection\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Modified RF with Threshold Adjustment\n",
    "\n",
    "def flastRFWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize KNeighborsClassifier with current hyperparameter combination\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=param_dict['n_estimators'],\n",
    "            criterion=param_dict['criterion'],\n",
    "            max_depth=param_dict['max_depth'],\n",
    "            min_samples_split=param_dict['min_samples_split'],\n",
    "            min_samples_leaf=param_dict['min_samples_leaf'],\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            rf_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(rf_model, \"predict_proba\"):\n",
    "                y_pred_proba = rf_model.predict_proba(X_test)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = rf_model.kneighbors(X_test)\n",
    "                weights = rf_model._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / rf_model.n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'n_estimators': param_dict['n_estimators'],\n",
    "                    'criterion': param_dict['criterion'],\n",
    "                    'max_depth': param_dict['max_depth'],\n",
    "                    'min_samples_split': param_dict['min_samples_split'],\n",
    "                    'min_samples_leaf': param_dict['min_samples_leaf'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-params-rf-{n_splits}-folds-Threshold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold rf analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "    ['n_estimators', 'criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'threshold']\n",
    ").agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'mcc': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-RF-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 300, 500],  # Number of trees\n",
    "        'max_depth': [10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "        'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    }\n",
    "\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/rf_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/rf_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform rf analysis with threshold adjustments\n",
    "    print(\"Starting rf analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastRFWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"rf_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbd924",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37360881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dt analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Per-fold dt analysis completed. Results saved to: dt_thresholds-params-dt-5-folds-Threshold.csv\n",
      "Averaged results saved to: dt_thresholds-thresholds-dt-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "  criterion  max_depth  min_samples_split  min_samples_leaf max_features  \\\n",
      "0      gini        NaN                  2                 1         None   \n",
      "1      gini        NaN                  2                 1         None   \n",
      "2      gini        NaN                  2                 1         None   \n",
      "3      gini        NaN                  2                 1         None   \n",
      "4      gini        NaN                  2                 1         None   \n",
      "\n",
      "   fold  threshold  accuracy  precision    recall        f1      mcc  \n",
      "0     1        0.1  0.866667        0.6  0.333333  0.428571  0.37998  \n",
      "1     1        0.2  0.866667        0.6  0.333333  0.428571  0.37998  \n",
      "2     1        0.3  0.866667        0.6  0.333333  0.428571  0.37998  \n",
      "3     1        0.4  0.866667        0.6  0.333333  0.428571  0.37998  \n",
      "4     1        0.5  0.866667        0.6  0.333333  0.428571  0.37998  \n",
      "Averaged Results:\n",
      "  criterion  max_depth  min_samples_split  min_samples_leaf max_features  \\\n",
      "0   entropy       10.0                  2                 1         log2   \n",
      "1   entropy       10.0                  2                 1         log2   \n",
      "2   entropy       10.0                  2                 1         log2   \n",
      "3   entropy       10.0                  2                 1         log2   \n",
      "4   entropy       10.0                  2                 1         log2   \n",
      "\n",
      "   threshold  accuracy_mean  accuracy_std  precision_mean  precision_std  \\\n",
      "0        0.1       0.819153      0.074994        0.414646       0.227570   \n",
      "1        0.2       0.819153      0.074994        0.414646       0.227570   \n",
      "2        0.3       0.822486      0.079708        0.447980       0.279902   \n",
      "3        0.4       0.822486      0.079708        0.447980       0.279902   \n",
      "4        0.5       0.832486      0.068004        0.459091       0.263828   \n",
      "\n",
      "   recall_mean  recall_std   f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0     0.422222    0.240883  0.412698  0.227389  0.310104  0.270001  \n",
      "1     0.422222    0.240883  0.412698  0.227389  0.310104  0.270001  \n",
      "2     0.400000    0.216595  0.412698  0.227389  0.316117  0.278490  \n",
      "3     0.400000    0.216595  0.412698  0.227389  0.316117  0.278490  \n",
      "4     0.400000    0.216595  0.417143  0.220122  0.328379  0.259259  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        from sklearn.random_projection import SparseRandomProjection\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Modified ft with Threshold Adjustment\n",
    "\n",
    "def flastdtWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize KNeighborsClassifier with current hyperparameter combination\n",
    "        dt_model = DecisionTreeClassifier(\n",
    "            criterion=param_dict['criterion'],\n",
    "            max_depth=param_dict['max_depth'],\n",
    "            min_samples_split=param_dict['min_samples_split'],\n",
    "            min_samples_leaf=param_dict['min_samples_leaf'],\n",
    "            max_features=param_dict['max_features'],\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            dt_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(dt_model, \"predict_proba\"):\n",
    "                y_pred_proba = dt_model.predict_proba(X_test)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = dt_model.kneighbors(X_test)\n",
    "                weights = dt_model._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / dt_model.n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'criterion': param_dict['criterion'],\n",
    "                    'max_depth': param_dict['max_depth'],\n",
    "                    'min_samples_split': param_dict['min_samples_split'],\n",
    "                    'min_samples_leaf': param_dict['min_samples_leaf'],\n",
    "                    'max_features': param_dict['max_features'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-params-dt-{n_splits}-folds-Threshold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold dt analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "    ['criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features', 'threshold']\n",
    ").agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'mcc': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-dt-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'max_depth': [None, 10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 5, 10],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/dt_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/dt_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform rf analysis with threshold adjustments\n",
    "    print(\"Starting dt analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastdtWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"dt_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b75422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5689656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73f05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a51a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5284e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bb31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ecaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef354c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85463d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1f878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a914fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1c183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21f8601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 3, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 3, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 3, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 3, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 5, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 5, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 5, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 5, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 7, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 7, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 7, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 7, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 9, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 9, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 9, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 9, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 11, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 11, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 11, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 11, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 15, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 15, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 15, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 15, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 20, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 20, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 20, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 56, 'knn__n_neighbors': 20, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 3, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 3, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 3, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 3, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 5, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 5, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 5, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 5, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 7, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 7, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 7, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 7, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 9, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 9, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 9, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 9, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 11, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 11, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 11, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 11, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 15, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 15, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 15, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 15, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 20, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 20, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 20, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 64, 'knn__n_neighbors': 20, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 3, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 3, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 3, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 3, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 5, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 5, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 5, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 5, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 7, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 7, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 7, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 7, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 9, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 9, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 9, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 9, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 11, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 11, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 11, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 11, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 15, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 15, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 15, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 15, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 20, 'knn__metric': 'cosine', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 20, 'knn__metric': 'cosine', 'knn__weights': 'distance'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 20, 'knn__metric': 'euclidean', 'knn__weights': 'uniform'}\n",
      "Training with parameters: {'pca__n_components': 72, 'knn__n_neighbors': 20, 'knn__metric': 'euclidean', 'knn__weights': 'distance'}\n",
      "Per-fold KNN analysis completed. Results saved to: knn_thresholds-thresholds-knn-results-per-fold.csv\n",
      "Averaged results saved to: knn_thresholds-thresholds-knn-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   pca_n_components  n_neighbors  weights  metric  fold  threshold  accuracy  \\\n",
      "0                56            3  uniform  cosine     1        0.1  0.850000   \n",
      "1                56            3  uniform  cosine     1        0.2  0.850000   \n",
      "2                56            3  uniform  cosine     1        0.3  0.850000   \n",
      "3                56            3  uniform  cosine     1        0.4  0.866667   \n",
      "4                56            3  uniform  cosine     1        0.5  0.866667   \n",
      "\n",
      "   precision    recall        f1       mcc  \n",
      "0   0.500000  0.666667  0.571429  0.490098  \n",
      "1   0.500000  0.666667  0.571429  0.490098  \n",
      "2   0.500000  0.666667  0.571429  0.490098  \n",
      "3   0.666667  0.222222  0.333333  0.331954  \n",
      "4   0.666667  0.222222  0.333333  0.331954  \n",
      "Averaged Results:\n",
      "   pca_n_components  n_neighbors   weights  metric  threshold  accuracy_mean  \\\n",
      "0                56            3  distance  cosine        0.1       0.846102   \n",
      "1                56            3  distance  cosine        0.2       0.849435   \n",
      "2                56            3  distance  cosine        0.3       0.852825   \n",
      "3                56            3  distance  cosine        0.4       0.899492   \n",
      "4                56            3  distance  cosine        0.5       0.896215   \n",
      "\n",
      "   accuracy_std  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0      0.040047        0.505672       0.081349     0.688889    0.164804   \n",
      "1      0.033708        0.508025       0.077613     0.666667    0.136083   \n",
      "2      0.034237        0.516667       0.079931     0.622222    0.126686   \n",
      "3      0.041612        0.719444       0.176886     0.577778    0.121716   \n",
      "4      0.040228        0.740952       0.203668     0.511111    0.099381   \n",
      "\n",
      "    f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0  0.572822  0.079301  0.500002  0.092311  \n",
      "1  0.570170  0.080397  0.493176  0.094013  \n",
      "2  0.559030  0.085478  0.479143  0.099994  \n",
      "3  0.636601  0.133115  0.586414  0.160327  \n",
      "4  0.601737  0.134158  0.558435  0.165137  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "###############################################################################\n",
    "# PCA and KNN with Threshold Adjustment\n",
    "\n",
    "def flastKNNWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization (PCA will be applied later in the pipeline)\n",
    "    countVec = CountVectorizer(stop_words=None)\n",
    "    Z_full = countVec.fit_transform(dataPoints).toarray()  # Convert to dense matrix\n",
    "\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Pipeline setup: PCA followed by KNN\n",
    "        pca = PCA(n_components=param_dict['pca__n_components'])\n",
    "        knn_model = KNeighborsClassifier(\n",
    "            n_neighbors=param_dict['knn__n_neighbors'],\n",
    "            weights=param_dict['knn__weights'],\n",
    "            metric=param_dict['knn__metric'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        pipeline = Pipeline([('pca', pca), ('knn', knn_model)])\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z_full, dataLabelsList)):\n",
    "            X_train, X_test = Z_full[train_index], Z_full[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the pipeline (PCA + KNN)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(pipeline.named_steps['knn'], \"predict_proba\"):\n",
    "                y_pred_proba = pipeline.predict_proba(X_test)\n",
    "            else:\n",
    "                # Handle the case if predict_proba is not available\n",
    "                distances, indices = pipeline.named_steps['knn'].kneighbors(X_test)\n",
    "                weights = pipeline.named_steps['knn']._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / pipeline.named_steps['knn'].n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'pca_n_components': param_dict['pca__n_components'],\n",
    "                    'n_neighbors': param_dict['knn__n_neighbors'],\n",
    "                    'weights': param_dict['knn__weights'],\n",
    "                    'metric': param_dict['knn__metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-knn-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold KNN analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['pca_n_components', 'n_neighbors', 'weights', 'metric', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-knn-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'pca__n_components': [math.floor(i*0.08*100) for i in range(7, 10)],  # Adjust based on dataset size\n",
    "        'knn__n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'knn__metric': ['cosine', 'euclidean'],\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/knn_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/knn_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis with threshold adjustments\n",
    "    print(\"Starting KNN analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastKNNWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5,\n",
    "        combination_label=\"knn_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25838750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68a23e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
