{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ac8440",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/h_pmxr992m143v567my_k4lm0000gn/T/ipykernel_40051/792669354.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim, SparseRandomProjection\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def computeResults(testLabels, predictLabels):\n",
    "    precision = precision_score(testLabels, predictLabels, zero_division=0)\n",
    "    recall = recall_score(testLabels, predictLabels, zero_division=0)\n",
    "    accuracy = accuracy_score(testLabels, predictLabels)\n",
    "    return precision, recall, accuracy\n",
    "\n",
    "###############################################################################\n",
    "# FLAST functions\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "def flastXGBClassification(trainData, trainLabels, testData, eta, max_depth, params):\n",
    "    t0 = time.perf_counter()\n",
    "    xgb_model = xgb.XGBClassifier(eta=eta, max_depth=max_depth, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    xgb_model.fit(trainData, trainLabels)\n",
    "    trainTime = time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    predictLabels = xgb_model.predict(testData)\n",
    "    testTime = time.perf_counter() - t0\n",
    "\n",
    "    return trainTime, testTime, predictLabels\n",
    "\n",
    "###############################################################################\n",
    "# FLAST XGB analysis\n",
    "\n",
    "def flastXGB(outDir, flakyZip, nonFlakyZip, extractDir, kf, dim, eps, eta, max_depth, params):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    if len(dataPoints) > 0:\n",
    "        print(f\"Sample document: {dataPoints[0]}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataPointsList = np.array([Z[i].toarray() for i in range(Z.shape[0])])\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Storage calculation\n",
    "    xgb_data = (dataPointsList, dataLabelsList)\n",
    "    pickleDumpXGB = os.path.join(outDir, f\"flast-xgb-eta{eta}-depth{max_depth}.pickle\")\n",
    "    with open(pickleDumpXGB, \"wb\") as pickleFile:\n",
    "        pickle.dump(xgb_data, pickleFile)\n",
    "    storage = os.path.getsize(pickleDumpXGB)\n",
    "    os.remove(pickleDumpXGB)\n",
    "\n",
    "    avgP, avgR, avgA = 0, 0, 0\n",
    "    avgTPrep, avgTPred = 0, 0\n",
    "    avgFlakyTrain, avgNonFlakyTrain, avgFlakyTest, avgNonFlakyTest = 0, 0, 0, 0\n",
    "    successFold, precisionFold = 0, 0\n",
    "\n",
    "    for trnIdx, tstIdx in kf.split(dataPointsList, dataLabelsList):\n",
    "        trainData, testData = dataPointsList[trnIdx], dataPointsList[tstIdx]\n",
    "        trainLabels, testLabels = dataLabelsList[trnIdx], dataLabelsList[tstIdx]\n",
    "        if sum(trainLabels) == 0 or sum(testLabels) == 0:\n",
    "            print(\"Skipping fold with no flaky/non-flaky examples...\")\n",
    "            continue\n",
    "\n",
    "        successFold += 1\n",
    "        avgFlakyTrain += sum(trainLabels)\n",
    "        avgNonFlakyTrain += len(trainLabels) - sum(trainLabels)\n",
    "        avgFlakyTest += sum(testLabels)\n",
    "        avgNonFlakyTest += len(testLabels) - sum(testLabels)\n",
    "\n",
    "        trainData = trainData.reshape((trainData.shape[0], -1))\n",
    "        testData = testData.reshape((testData.shape[0], -1))\n",
    "\n",
    "        trainTime, testTime, predictLabels = flastXGBClassification(trainData, trainLabels, testData, eta, max_depth, params)\n",
    "        preparationTime = (vecTime * len(trainData) / len(dataPoints)) + trainTime\n",
    "        predictionTime = (vecTime / len(dataPoints)) + (testTime / len(testData))\n",
    "        precision, recall, accuracy = computeResults(testLabels, predictLabels)\n",
    "\n",
    "        print(f\"Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}\")\n",
    "        if precision != \"-\":\n",
    "            precisionFold += 1\n",
    "            avgP += precision\n",
    "            avgA += accuracy\n",
    "        avgR += recall\n",
    "        avgTPrep += preparationTime\n",
    "        avgTPred += predictionTime\n",
    "\n",
    "    if precisionFold > 0:\n",
    "        avgP /= precisionFold\n",
    "        avgA /= precisionFold\n",
    "    else:\n",
    "        avgP = avgA = \"-\"\n",
    "    avgR /= successFold\n",
    "    avgTPrep /= successFold\n",
    "    avgTPred /= successFold\n",
    "    avgFlakyTrain /= successFold\n",
    "    avgNonFlakyTrain /= successFold\n",
    "    avgFlakyTest /= successFold\n",
    "    avgNonFlakyTest /= successFold\n",
    "\n",
    "    return avgFlakyTrain, avgNonFlakyTrain, avgFlakyTest, avgNonFlakyTest, avgP, avgR, avgA, storage, avgTPrep, avgTPred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup\n",
    "    flakyZip = \"C:/Users/kdeep/Downloads/Flakiness ML/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"C:/Users/kdeep/Downloads/Flakiness ML/reduced_nonflaky_files.zip\"\n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    numSplit = 10\n",
    "    kf = StratifiedKFold(n_splits=numSplit)\n",
    "\n",
    "    outFile = \"params-xgb.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"eta,max_depth,dim,eps,avgFlakyTrain,avgNonFlakyTrain,avgFlakyTest,avgNonFlakyTest,precision,recall,accuracy,storage,preparationTime,predictionTime\\n\")\n",
    "\n",
    "    # XGB parameters to vary\n",
    "    eta_values = [0.1, 0.3, 0.5]  # Learning rate\n",
    "    depth_values = [3, 5, 7]  # Tree depth\n",
    "    dim_values = [0, 100, 200]  # Dimensionality reduction\n",
    "    eps_values = [0.1, 0.3, 0.5]  # JL epsilon\n",
    "\n",
    "    for eta in eta_values:\n",
    "        for depth in depth_values:\n",
    "            for dim in dim_values:\n",
    "                for eps in eps_values:\n",
    "                    print(f\"eta={eta}, depth={depth}, dim={dim}, eps={eps}\")\n",
    "                    results = flastXGB(outDir, flakyZip, nonFlakyZip, extractDir, kf, dim, eps, eta, depth, {})\n",
    "                    print(f\"Results to be written: eta={eta}, depth={depth}, dim={dim}, eps={eps}, {results}\")\n",
    "                    with open(os.path.join(outDir, outFile), \"a\") as fo:\n",
    "                        fo.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(eta, depth, dim, eps, *results))\n",
    "\n",
    "    print(\"XGBoost analysis completed. Results saved to:\", outFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba92cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
