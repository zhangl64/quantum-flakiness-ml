{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a655df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Sample document: # -*- coding: utf-8 -*-\n",
      "\n",
      "# This code is part of Qiskit.\n",
      "#\n",
      "# (C) Copyright IBM 2020.\n",
      "#\n",
      "# This code is licensed under the Apache License, Version 2.0. You may\n",
      "# obtain a copy of this license in the LICENSE.txt file in the root directory\n",
      "# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n",
      "#\n",
      "# Any modifications or derivative works of this code must retain this\n",
      "# copyright notice, and modified files need to carry a notice indicating\n",
      "# that they have been altered from the originals.\n",
      "\n",
      "\"\"\"General utility functions for testing.\"\"\"\n",
      "\n",
      "from qiskit import QuantumCircuit\n",
      "from qiskit.providers.ibmq.ibmqbackend import IBMQBackend\n",
      "\n",
      "\n",
      "def most_busy_backend(provider):\n",
      "    \"\"\"Return the most busy backend for the provider given.\n",
      "\n",
      "    Return the most busy available backend for those that\n",
      "    have a `pending_jobs` in their `status`. Backends such as\n",
      "    local backends that do not have this are not considered.\n",
      "\n",
      "    Args:\n",
      "        provider (AccountProvider): IBM Q Experience account provider.\n",
      "\n",
      "    Returns:\n",
      "        IBMQBackend: the most busy backend.\n",
      "    \"\"\"\n",
      "    backends = provider.backends(simulator=False, operational=True)\n",
      "    return max([b for b in backends if b.configuration().n_qubits >= 5],\n",
      "               key=lambda b: b.status().pending_jobs)\n",
      "\n",
      "\n",
      "def get_large_circuit(backend: IBMQBackend) -> QuantumCircuit:\n",
      "    \"\"\"Return a slightly larger circuit that would run a bit longer.\n",
      "\n",
      "    Args:\n",
      "        backend: Backend on which the circuit will run.\n",
      "\n",
      "    Returns:\n",
      "        A larger circuit.\n",
      "    \"\"\"\n",
      "    n_qubits = min(backend.configuration().n_qubits, 20)\n",
      "    circuit = QuantumCircuit(n_qubits, n_qubits)\n",
      "    for n in range(n_qubits-1):\n",
      "        circuit.h(n)\n",
      "        circuit.cx(n, n+1)\n",
      "    circuit.measure(list(range(n_qubits)), list(range(n_qubits)))\n",
      "\n",
      "    return circuit\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2}\n",
      "Best Accuracy Score: 0.6906432748538012\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rf__n_estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/h_pmxr992m143v567my_k4lm0000gn/T/ipykernel_34489/228792863.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting 5-fold analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mbest_params_5folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score_5folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecisionTreeWithGridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflakyZip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonFlakyZip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting 3-fold analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rp/h_pmxr992m143v567my_k4lm0000gn/T/ipykernel_34489/228792863.py\u001b[0m in \u001b[0;36mdecisionTreeWithGridSearch\u001b[0;34m(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mpreparationTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecTime\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPointsList\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Estimating preparation time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{param['rf__n_estimators']},{param['rf__max_depth']},{param['rf__criterion']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Random Forest analysis completed for {n_splits}-folds. Results saved to: {outFile}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rf__n_estimators'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,GridSearchCV\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "###############################################################################\n",
    "# read data from file\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        \n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "'''\n",
    "def computeResults(testLabels, predictLabels):\n",
    "    try:\n",
    "        precision = precision_score(testLabels, predictLabels)\n",
    "        recall = recall_score(testLabels, predictLabels)\n",
    "        accuracy = accuracy_score(testLabels, predictLabels)\n",
    "        f1 = f1_score(testLabels, predictLabels)\n",
    "        \n",
    "        print(f\"Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}, F1 Score: {f1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {e}\")\n",
    "        precision = recall = accuracy = f1 = \"-\"\n",
    "    return precision, recall, accuracy, f1\n",
    "'''\n",
    "###############################################################################\n",
    "# FLAST\n",
    "\n",
    "def vectorization(dataPoints, dim=0, eps=0.3):\n",
    "    countVec = CountVectorizer(stop_words=None)  # Disable stop words filtering\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "'''\n",
    "def classificationDecisionTree(trainData, trainLabels, testData, params):\n",
    "    # training\n",
    "    t0 = time.perf_counter()\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion=params.get(\"criterion\", \"gini\"),\n",
    "        splitter=params.get(\"splitter\", \"best\"),\n",
    "        max_depth=params.get(\"max_depth\"),\n",
    "        min_samples_split=params.get(\"min_samples_split\", 2),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
    "    )\n",
    "    clf.fit(trainData, trainLabels)\n",
    "    t1 = time.perf_counter()\n",
    "    trainTime = t1 - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    predictLabels = clf.predict(testData)\n",
    "    t1 = time.perf_counter()\n",
    "    testTime = t1 - t0\n",
    "\n",
    "    return trainTime, testTime, predictLabels\n",
    "'''\n",
    "def decisionTreeWithGridSearch(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps):\n",
    "    v0 = time.perf_counter()\n",
    "    \n",
    "                               \n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    if len(dataPoints) > 0:\n",
    "        print(f\"Sample document: {dataPoints[0]}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = vectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataPointsList = np.array([Z[i].toarray() for i in range(Z.shape[0])])\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('rf', DecisionTreeClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "            \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "            \"rf__max_depth\": [10, 30, 50, 100, 300, 500],\n",
    "            \"rf__min_samples_split\": [2, 5],\n",
    "            \"rf__min_samples_leaf\": [1, 2]\n",
    "        }\n",
    "    \n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='accuracy', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(dataPointsList.reshape(len(dataPointsList), -1), dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"params-rf-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_estimators,max_depth,criterion,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            preparationTime = vecTime / len(dataPointsList)  # Estimating preparation time\n",
    "            fo.write(f\"{param['rf__n_estimators']},{param['rf__max_depth']},{param['rf__criterion']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Random Forest analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup\n",
    "    flakyZip = \"cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"reduced_nonflaky_files.zip\"\n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results_DecisionTree\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform Random Forest analysis for 5 folds and 3 folds\n",
    "    dim = 100  # Example value for JL dimensionality reduction\n",
    "    eps = 0.3  # JL epsilon\n",
    "\n",
    "    print(\"Starting 5-fold analysis...\")\n",
    "    best_params_5folds, best_score_5folds = decisionTreeWithGridSearch(outDir, flakyZip, nonFlakyZip, extractDir, 5, dim, eps)\n",
    "\n",
    "    print(\"Starting 3-fold analysis...\")\n",
    "    best_params_3folds, best_score_3folds = decisionTreeWithGridSearch(outDir, flakyZip, nonFlakyZip, extractDir, 3, dim, eps)\n",
    "\n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best Accuracy Score: {best_score_5folds}\")\n",
    "\n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best Accuracy Score: {best_score_3folds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40150e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cc3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
