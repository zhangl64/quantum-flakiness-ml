{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986b85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_comments(code):\n",
    "    \"\"\"Get rid of comments.\"\"\"\n",
    "    pattern = r\"\"\"\n",
    "        ('(?:\\\\.|[^\\\\'])*'          # 단일 따옴표 문자열\n",
    "        | \"(?:\\\\.|[^\\\\\"])*\"         # 이중 따옴표 문자열\n",
    "        | '''[\\s\\S]*?'''            # 삼중 따옴표 문자열 (여러 줄 문자열)\n",
    "        | \\\"\\\"\\\"[\\s\\S]*?\\\"\\\"\\\"      # 삼중 따옴표 문자열\n",
    "        | \\#.*?$                    # 한 줄 주석\n",
    "        )\"\"\"\n",
    "    regex = re.compile(pattern, re.VERBOSE | re.MULTILINE)\n",
    "    def replacer(match):\n",
    "        s = match.group(0)\n",
    "        if s.startswith('#'):\n",
    "            return ''  # 주석은 제거\n",
    "        else:\n",
    "            return s  # 문자열은 그대로 유지\n",
    "    code_no_comments = regex.sub(replacer, code)\n",
    "    return code_no_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbbaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b766160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment_code(code):\n",
    "    # adding comments\n",
    "    augmented_codes = []\n",
    "    comments = [\"# TODO: Refactor this\", \"# This is a placeholder\", \"# Added for augmentation\"]\n",
    "    for comment in comments:\n",
    "        augmented_code = f\"{comment}\\n{code}\"\n",
    "        augmented_codes.append(augmented_code)\n",
    "    \n",
    "\n",
    "    code_lines = code.split('\\n')\n",
    "    if len(code_lines) > 1:\n",
    "        # adding spaces in random spot\n",
    "        idx = random.randint(0, len(code_lines)-1)\n",
    "        code_lines.insert(idx, '')\n",
    "        augmented_code = '\\n'.join(code_lines)\n",
    "        augmented_codes.append(augmented_code)\n",
    "    \n",
    "    return augmented_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dacecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataPoints(path, augment=False):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                        if augment:\n",
    "                            # apply data augmentation\n",
    "                            augmented_codes = augment_code(dp)\n",
    "                            dataPointsList.extend(augmented_codes)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921f27d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis...\n",
      "Number of flaky documents: 245\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 292\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best F1 Score: 0.9744077208406164\n",
      "KNN analysis completed for 5-folds. Results saved to: -params-knn-5-folds.csv\n",
      "Starting 5-fold analysis with unbalanced data...\n",
      "Number of flaky documents: 245\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 499\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 Score: 0.9684705882352942\n",
      "KNN analysis completed for 5-folds. Results saved to: imbalance-params-knn-5-folds.csv\n",
      "Starting 3-fold analysis...\n",
      "Number of flaky documents: 245\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 292\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best F1 Score: 0.9703187462609795\n",
      "KNN analysis completed for 3-folds. Results saved to: -params-knn-3-folds.csv\n",
      "Starting 3-fold analysis with unbalanced data...\n",
      "Number of flaky documents: 245\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 499\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 20, 'weights': 'distance'}\n",
      "Best F1 Score: 0.9523086821566785\n",
      "KNN analysis completed for 3-folds. Results saved to: imbalance-params-knn-3-folds.csv\n",
      "Best results for 5-fold:\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best f1 Score: 0.9744077208406164\n",
      "Best results for imbalanced 5-fold:\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best f1 Score: 0.9684705882352942\n",
      "Best results for imbalanced 3-fold:\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best f1 Score: 0.9703187462609795\n",
      "Best results for 3-fold:\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 20, 'weights': 'distance'}\n",
      "Best f1 Score: 0.9523086821566785\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def augment_code(code):\n",
    "    \"\"\"코드에 변형을 주어 데이터 증강을 수행합니다.\"\"\"\n",
    "    augmented_codes = []\n",
    "\n",
    "    # 예시 1: 주석 추가\n",
    "    comments = [\"# TODO: Refactor this\", \"# Added for augmentation\", \"# Generated comment\"]\n",
    "    for comment in comments:\n",
    "        augmented_code = f\"{comment}\\n{code}\"\n",
    "        augmented_codes.append(augmented_code)\n",
    "\n",
    "    # 예시 2: 공백 줄 추가\n",
    "    code_lines = code.split('\\n')\n",
    "    if len(code_lines) > 1:\n",
    "        idx = random.randint(0, len(code_lines) - 1)\n",
    "        code_lines.insert(idx, '')  # 빈 줄 추가\n",
    "        augmented_code = '\\n'.join(code_lines)\n",
    "        augmented_codes.append(augmented_code)\n",
    "\n",
    "    # 예시 3: 변수명 변경 (간단한 예)\n",
    "    if 'temp' in code:\n",
    "        augmented_code = code.replace('temp', 'tmp')\n",
    "        augmented_codes.append(augmented_code)\n",
    "\n",
    "    return augmented_codes\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"지정된 디렉토리에 zip 파일을 해제합니다.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path, augment=False):\n",
    "    \"\"\"주어진 디렉토리 내의 모든 .py 파일의 내용을 수집합니다.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                        if augment:\n",
    "                            # 데이터 증강 적용\n",
    "                            augmented_codes = augment_code(dp)\n",
    "                            dataPointsList.extend(augmented_codes)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "\n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "\n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"데이터 포인트를 벡터화합니다.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# KNN 모델과 GridSearchCV를 사용한 분석 함수\n",
    "\n",
    "def flastKNNWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # 압축 파일 해제\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    if os.path.exists(flakyDir):\n",
    "        shutil.rmtree(flakyDir)\n",
    "    if os.path.exists(nonFlakyDir):\n",
    "        shutil.rmtree(nonFlakyDir)\n",
    "    \n",
    "\n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    # 데이터 로딩 및 증강 적용\n",
    "    dataPointsFlaky = getDataPoints(flakyDir, augment=True)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir, augment=False)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # 벡터화\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # KNN 모델 정의\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # 하이퍼파라미터 그리드 정의\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'metric': ['cosine', 'euclidean'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "    }\n",
    "\n",
    "    # 스코어링 메트릭 정의\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score)\n",
    "    }\n",
    "\n",
    "    # 교차 검증 설정\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # GridSearchCV 실행\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # 최적 파라미터 및 점수 출력\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # 결과 저장\n",
    "    outFile = f\"{combination_label}-params-knn-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_neighbors,metric,weights,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)\n",
    "            fo.write(f\"{param['n_neighbors']},{param['metric']},{param['weights']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"KNN analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    flakyZip = \"compressedDataset/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    nonFlakyUnbalance = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "    \n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/resut_FlastKNN_augment\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis for 5 folds and 3 folds\n",
    "    print(\"Starting 5-fold analysis...\")\n",
    "    best_params_5folds, best_score_5folds = flastKNNWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir,5, \"\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    print (\"Starting 5-fold analysis with unbalanced data...\")\n",
    "    best_params_5folds_imbalance,best_score_5folds_imbalance = flastKNNWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 5,\"imbalance\")\n",
    "  \n",
    "    \n",
    "    print(\"Starting 3-fold analysis...\")\n",
    "    best_params_3folds, best_score_3folds = flastKNNWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, 3,\"\")\n",
    "\n",
    "    \n",
    "    print (\"Starting 3-fold analysis with unbalanced data...\")\n",
    "    best_params_3folds_imbalance,best_score_3folds_imbalance = flastKNNWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 3,\"imbalance\")\n",
    "  \n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds}\")\n",
    "\n",
    "    \n",
    "    print(\"Best results for imbalanced 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds_imbalance}\")\n",
    "    \n",
    "    print(\"Best results for imbalanced 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds}\")\n",
    "    \n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds_imbalance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ee3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a737c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'max_features': 'log2', 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "Best Threshold: 0.2\n",
      "Best F1 Score with Threshold: 0.6771653543307087\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: equal-params-dt-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 504 candidates, totalling 1512 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best Threshold: 0.1\n",
      "Best F1 Score with Threshold: 0.6666666666666666\n",
      "Decision Tree analysis completed for 3-folds. Results saved to: equal-params-dt-3-folds.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'max_features': 'log2', 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "Best F1 Score: 0.6771653543307087\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best F1 Score: 0.6666666666666666\n",
      "Starting Decision Tree analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 50, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "Best Threshold: 0.30000000000000004\n",
      "Best F1 Score with Threshold: 0.45045045045045046\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: larger-params-dt-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 3 folds for each of 504 candidates, totalling 1512 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best Threshold: 0.5\n",
      "Best F1 Score with Threshold: 0.425531914893617\n",
      "Decision Tree analysis completed for 3-folds. Results saved to: larger-params-dt-3-folds.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 50, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "Best F1 Score: 0.45045045045045046\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best F1 Score: 0.425531914893617\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree with GridSearchCV\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define Decision Tree model\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "        'max_depth': [None, 10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 5, 10],  # Minimum number of samples required to be at a leaf node\n",
    "        'max_features': [None, 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    }\n",
    "\n",
    "    # Custom scoring function (F1 score) for GridSearchCV\n",
    "    scoring = make_scorer(f1_score, zero_division=1)\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(dt_model, param_grid, cv=skf, scoring=scoring, refit=True, verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_dt_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Finding threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    best_threshold = 0.5  \n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred_proba = cross_val_predict(best_dt_model, Z, dataLabelsList, cv=skf, method='predict_proba')\n",
    "        y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "        f1 = f1_score(dataLabelsList, y_pred, zero_division=1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score with Threshold: {best_f1}\")\n",
    "\n",
    "    # Calculate other metrics using the best threshold\n",
    "    y_pred_proba = cross_val_predict(best_dt_model, Z, dataLabelsList, cv=skf, method='predict_proba')\n",
    "    y_pred = (y_pred_proba[:, 1] >= best_threshold).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(dataLabelsList, y_pred)\n",
    "    precision = precision_score(dataLabelsList, y_pred, zero_division=1)\n",
    "    recall = recall_score(dataLabelsList, y_pred, zero_division=1)\n",
    "    mcc = matthews_corrcoef(dataLabelsList, y_pred)\n",
    "    preparationTime = vecTime / len(dataPoints)\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-dt-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Write the header\n",
    "        fo.write(\"criterion,max_depth,min_samples_split,min_samples_leaf,max_features,threshold,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        # Write the data row\n",
    "        fo.write(f\"{best_params['criterion']},{best_params['max_depth']},{best_params['min_samples_split']},\"\n",
    "                 f\"{best_params['min_samples_leaf']},{best_params['max_features']},{best_threshold},\"\n",
    "                 f\"{accuracy},{precision},{recall},{best_f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastDTWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastDTWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Decision Tree analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastDTWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastDTWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9cf2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best Threshold: 0.5\n",
      "Best F1 Score with Threshold: 0.7741935483870968\n",
      "XGBoost analysis completed for 5-folds. Results saved to: equal-params-xgb-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best Threshold: 0.30000000000000004\n",
      "Best F1 Score with Threshold: 0.7524752475247525\n",
      "XGBoost analysis completed for 3-folds. Results saved to: equal-params-xgb-3-folds.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best F1 Score: 0.7741935483870968\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best F1 Score: 0.7524752475247525\n",
      "Starting XGBoost analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best Parameters: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best Threshold: 0.1\n",
      "Best F1 Score with Threshold: 0.6868686868686869\n",
      "XGBoost analysis completed for 5-folds. Results saved to: larger-params-xgb-5-folds.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best Threshold: 0.5\n",
      "Best F1 Score with Threshold: 0.5588235294117647\n",
      "XGBoost analysis completed for 3-folds. Results saved to: larger-params-xgb-3-folds.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best F1 Score: 0.6868686868686869\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best F1 Score: 0.5588235294117647\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# XGBoost with GridSearchCV and Threshold Adjustment\n",
    "def flastXGBWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define XGBoost model without 'use_label_encoder'\n",
    "    xgb_model = xgb.XGBClassifier(eval_metric=\"logloss\")\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1, 0.3, 0.5],  # Learning rate\n",
    "        'max_depth': [3, 5, 7, 10],  # Tree depth\n",
    "        'n_estimators': [50, 100, 200, 300],  # Number of boosting rounds\n",
    "    }\n",
    "\n",
    "    # Custom scoring function (F1 score) for GridSearchCV\n",
    "    scoring = make_scorer(f1_score, zero_division=1)\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=skf, scoring=scoring, refit=True, verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Find the optimal threshold using cross-validation\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    best_threshold = 0.5  # Default threshold\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Get cross-validated predicted probabilities\n",
    "    y_pred_proba = cross_val_predict(best_xgb_model, Z, dataLabelsList, cv=skf, method='predict_proba')\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "        f1 = f1_score(dataLabelsList, y_pred, zero_division=1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score with Threshold: {best_f1}\")\n",
    "\n",
    "    # Calculate other metrics using the best threshold\n",
    "    y_pred = (y_pred_proba[:, 1] >= best_threshold).astype(int)\n",
    "    accuracy = accuracy_score(dataLabelsList, y_pred)\n",
    "    precision = precision_score(dataLabelsList, y_pred, zero_division=1)\n",
    "    recall = recall_score(dataLabelsList, y_pred, zero_division=1)\n",
    "    mcc = matthews_corrcoef(dataLabelsList, y_pred)\n",
    "    preparationTime = vecTime / len(dataPoints)\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-xgb-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Write the header\n",
    "        fo.write(\"learning_rate,max_depth,n_estimators,threshold,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        # Write the data row\n",
    "        fo.write(f\"{best_params['learning_rate']},{best_params['max_depth']},{best_params['n_estimators']},\"\n",
    "                 f\"{best_threshold},{accuracy},{precision},{recall},{best_f1},{mcc},{preparationTime}\\n\")\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform XGBoost analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting XGBoost analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastXGBWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastXGBWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform XGBoost analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting XGBoost analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastXGBWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastXGBWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdde04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ab974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
