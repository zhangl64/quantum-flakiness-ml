{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42537f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents (balance combination): 45\n",
      "Number of non-flaky documents (balance combination): 45\n",
      "Total number of documents (balance combination): 90\n",
      "Number of flaky documents (imbalance combination): 45\n",
      "Number of non-flaky documents (imbalance combination): 243\n",
      "Total number of documents (imbalance combination): 288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import json\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "\n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "\n",
    "    return dataPointsList\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Parameters setup\n",
    "flakyZip = 'Dataset/flaky_files.zip'\n",
    "nonFlakyZip = 'Dataset/reduced_nonflaky_files.zip'\n",
    "imbalanceNonFlakyZip = 'Dataset/nonflaky_files.zip'\n",
    "\n",
    "\n",
    "#save best hyperparameter\n",
    "best_hyperparameter = 'results/best_hyperparameter'\n",
    "os.makedirs(best_hyperparameter,exist_ok=True)\n",
    "\n",
    "# Create directories\n",
    "outDirbalance = \"results/balance_flaky_nonflaky/\"\n",
    "outDirimbalance = \"results/imbalance_nonflaky/\"\n",
    "os.makedirs(outDirbalance, exist_ok=True)\n",
    "os.makedirs(outDirimbalance, exist_ok=True)\n",
    "\n",
    "extractDirbalance = \"extracted/balance_flaky_nonflaky/\"\n",
    "extractDirimbalance = \"extracted/imbalance_nonflaky/\"\n",
    "os.makedirs(extractDirbalance, exist_ok=True)\n",
    "os.makedirs(extractDirimbalance, exist_ok=True)\n",
    "\n",
    "# Extract and read data once for balance combination\n",
    "flakyDirbalance = os.path.join(extractDirbalance, 'flaky')\n",
    "nonFlakyDirbalance = os.path.join(extractDirbalance, 'nonFlaky')\n",
    "os.makedirs(flakyDirbalance, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirbalance, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirbalance)\n",
    "extract_zip(nonFlakyZip, nonFlakyDirbalance)\n",
    "\n",
    "dataPointsFlakybalance = getDataPoints(flakyDirbalance)\n",
    "dataPointsNonFlakybalance = getDataPoints(nonFlakyDirbalance)\n",
    "dataPointsbalance = dataPointsFlakybalance + dataPointsNonFlakybalance\n",
    "\n",
    "# Print the number of datasets for balance combination\n",
    "print(f\"Number of flaky documents (balance combination): {len(dataPointsFlakybalance)}\")\n",
    "print(f\"Number of non-flaky documents (balance combination): {len(dataPointsNonFlakybalance)}\")\n",
    "print(f\"Total number of documents (balance combination): {len(dataPointsbalance)}\")\n",
    "\n",
    "dataLabelsListbalance = np.array([1]*len(dataPointsFlakybalance) + [0]*len(dataPointsNonFlakybalance))\n",
    "\n",
    "# Extract and read data once for imbalance non-flaky combination\n",
    "flakyDirimbalance = os.path.join(extractDirimbalance, 'flaky')\n",
    "nonFlakyDirimbalance = os.path.join(extractDirimbalance, 'nonFlaky')\n",
    "os.makedirs(flakyDirimbalance, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirimbalance, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirimbalance)\n",
    "extract_zip(imbalanceNonFlakyZip, nonFlakyDirimbalance)\n",
    "\n",
    "dataPointsFlakyimbalance = getDataPoints(flakyDirimbalance)\n",
    "dataPointsNonFlakyimbalance = getDataPoints(nonFlakyDirimbalance)\n",
    "dataPointsimbalance = dataPointsFlakyimbalance + dataPointsNonFlakyimbalance\n",
    "\n",
    "# Print the number of datasets for imbalance combination\n",
    "print(f\"Number of flaky documents (imbalance combination): {len(dataPointsFlakyimbalance)}\")\n",
    "print(f\"Number of non-flaky documents (imbalance combination): {len(dataPointsNonFlakyimbalance)}\")\n",
    "print(f\"Total number of documents (imbalance combination): {len(dataPointsimbalance)}\")\n",
    "\n",
    "dataLabelsListimbalance = np.array([1]*len(dataPointsFlakyimbalance) + [0]*len(dataPointsNonFlakyimbalance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304d035",
   "metadata": {},
   "source": [
    "## KNN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "540d06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting KNN analysis for flaky vs smaller non-flaky files (balance combination)...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 7, 'knn__weights': 'distance', 'pca__n_components': 50}\n",
      "Best F1 Score: 0.6900653594771242 (Std Dev: 0.10649094843383755)\n",
      "KNN analysis completed for 5-folds. Results saved to: knn-results-vanilla.csv\n",
      "\n",
      "Best results for KNN 5-fold on balance combination:\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 7, 'knn__weights': 'distance', 'pca__n_components': 50}\n",
      "Best F1 Score: 0.6900653594771242 (Std Dev: 0.10649094843383755)\n",
      "\n",
      "Starting KNN analysis for flaky vs imbalance non-flaky files (imbalance combination)...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best hyperparameters saved to: results/best_hyperparameter\\best_params_knn.json\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'pca__n_components': 150}\n",
      "Best F1 Score: 0.4971028971028971 (Std Dev: 0.11015063115457374)\n",
      "KNN analysis completed for 5-folds. Results saved to: knn-results-vanilla.csv\n",
      "\n",
      "Best results for KNN 5-fold on imbalance combination:\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'pca__n_components': 150}\n",
      "Best F1 Score: 0.4971028971028971 (Std Dev: 0.11015063115457374)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "def runKNN(dataPoints, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define the pipeline with CountVectorizer, PCA, and KNN\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "        ('pca', PCA(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    if combination_label == \"balance\":\n",
    "        param_grid = {\n",
    "            'pca__n_components': [50, 60, 65],  # Number of components for PCA\n",
    "            'knn__n_neighbors': [3, 5, 7, 9],\n",
    "            'knn__metric': ['cosine', 'euclidean'],\n",
    "            'knn__weights': ['uniform', 'distance'],\n",
    "        }\n",
    "    else:\n",
    "        param_grid = {\n",
    "            'pca__n_components': [150, 180, 200, 220],  # Number of components for PCA\n",
    "            'knn__n_neighbors': [3, 5, 7, 9],\n",
    "            'knn__metric': ['cosine', 'euclidean'],\n",
    "            'knn__weights': ['uniform', 'distance'],\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "    # Custom scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer,  # MCC score custom function\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with the pipeline\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit='f1',  # Optimize the model based on F1 score\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV on data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "    # Get the index of the best parameter combination\n",
    "    best_index = grid_search.best_index_\n",
    "    # Get the standard deviation of F1 score for the best parameter combination\n",
    "    std_f1_best = grid_search.cv_results_['std_test_f1'][best_index]\n",
    "\n",
    "    if combination_label == \"imbalance\":\n",
    "        best_params_file = os.path.join(best_hyperparameter,  f\"best_params_knn.json\")\n",
    "        with open(best_params_file, 'w') as f:\n",
    "            json.dump(best_params, f)\n",
    "        print(f\"Best hyperparameters saved to: {best_params_file}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score} (Std Dev: {std_f1_best})\")\n",
    "\n",
    "   \n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"knn-results-vanilla.csv\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Updated CSV header to include standard deviation columns\n",
    "        fo.write(\"pca_n_components,n_neighbors,metric,weights,accuracy,std_accuracy,precision,std_precision,recall,std_recall,f1,std_f1,mcc,std_mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            std_accuracy = grid_search.cv_results_['std_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            std_precision = grid_search.cv_results_['std_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            std_recall = grid_search.cv_results_['std_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            std_f1 = grid_search.cv_results_['std_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            std_mcc = grid_search.cv_results_['std_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)\n",
    "            # Write the parameters, mean scores, and standard deviations to the CSV\n",
    "            fo.write(f\"{param['pca__n_components']},{param['knn__n_neighbors']},{param['knn__metric']},{param['knn__weights']},\"\n",
    "                     f\"{accuracy},{std_accuracy},{precision},{std_precision},{recall},{std_recall},{f1},{std_f1},{mcc},{std_mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"KNN analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    # Return std_f1_best along with other results\n",
    "    return best_params, best_score, std_f1_best\n",
    "\n",
    "# Run KNN on balance combination\n",
    "print(\"\\nStarting KNN analysis for flaky vs smaller non-flaky files (balance combination)...\")\n",
    "best_params_5folds_knn_balance, best_score_5folds_knn_balance, std_f1_best_balance = runKNN(\n",
    "    dataPointsbalance, dataLabelsListbalance, outDirbalance, 5, \"balance\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for KNN 5-fold on balance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_knn_balance}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_knn_balance} (Std Dev: {std_f1_best_balance})\")\n",
    "\n",
    "# Run KNN on imbalance non-flaky combination\n",
    "print(\"\\nStarting KNN analysis for flaky vs imbalance non-flaky files (imbalance combination)...\")\n",
    "best_params_5folds_knn_imbalance, best_score_5folds_knn_imbalance, std_f1_best_imbalance = runKNN(\n",
    "    dataPointsimbalance, dataLabelsListimbalance, outDirimbalance, 5, \"imbalance\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for KNN 5-fold on imbalance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_knn_imbalance}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_knn_imbalance} (Std Dev: {std_f1_best_imbalance})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1a814",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4436c1",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba4000e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SVM analysis for flaky vs smaller non-flaky files (balance combination)...\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Parameters: {'pca__n_components': 65, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Best F1 Score: 0.8239766081871345 (Std Dev: 0.10094645473774816)\n",
      "SVM analysis completed for 5-folds. Results saved to: svm-results-vanilla.csv\n",
      "\n",
      "Best results for SVM 5-fold on balance combination:\n",
      "Best Parameters: {'pca__n_components': 65, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Best F1 Score: 0.8239766081871345 (Std Dev: 0.10094645473774816)\n",
      "\n",
      "Starting SVM analysis for flaky vs imbalance non-flaky files (imbalance combination)...\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Best hyperparameters saved to: results/best_hyperparameter\\best_params_svm.json\n",
      "Best Parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score: 0.6914602683178535 (Std Dev: 0.12792567952776687)\n",
      "SVM analysis completed for 5-folds. Results saved to: svm-results-vanilla.csv\n",
      "\n",
      "Best results for SVM 5-fold on imbalance combination:\n",
      "Best Parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score: 0.6914602683178535 (Std Dev: 0.12792567952776687)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "def runSVM(dataPoints, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define the pipeline with CountVectorizer, PCA, and SVM\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "        ('pca', PCA(random_state=42)),\n",
    "        ('svm', SVC(probability=True, random_state=42))  # Enable probability estimates for threshold tuning\n",
    "    ])\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    if combination_label == \"balance\":\n",
    "        param_grid = {\n",
    "            'pca__n_components': [50, 60, 65],  # Number of principal components\n",
    "            'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "            'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types\n",
    "        }\n",
    "    else:\n",
    "        param_grid = {\n",
    "            'pca__n_components': [150, 180, 200, 220],\n",
    "            'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "            'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types\n",
    "        }\n",
    "\n",
    "    # Custom scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer,  # MCC score custom function\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with the pipeline\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit='f1',\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV on data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Get the index of the best parameter combination\n",
    "    best_index = grid_search.best_index_\n",
    "    # Get the standard deviation of F1 score for the best parameter combination\n",
    "    std_f1_best = grid_search.cv_results_['std_test_f1'][best_index]\n",
    "\n",
    "    if combination_label == \"imbalance\":\n",
    "        best_params_file = os.path.join(best_hyperparameter,  f\"best_params_svm.json\")\n",
    "        with open(best_params_file, 'w') as f:\n",
    "            json.dump(best_params, f)\n",
    "        print(f\"Best hyperparameters saved to: {best_params_file}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score} (Std Dev: {std_f1_best})\")\n",
    "  \n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"svm-results-vanilla.csv\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Updated CSV header to include standard deviation columns\n",
    "        fo.write(\"pca_n_components,C,kernel,accuracy,std_accuracy,precision,std_precision,recall,std_recall,f1,std_f1,mcc,std_mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            std_accuracy = grid_search.cv_results_['std_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            std_precision = grid_search.cv_results_['std_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            std_recall = grid_search.cv_results_['std_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            std_f1 = grid_search.cv_results_['std_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            std_mcc = grid_search.cv_results_['std_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)\n",
    "            # Write the parameters, mean scores, and standard deviations to the CSV\n",
    "            fo.write(f\"{param['pca__n_components']},{param['svm__C']},{param['svm__kernel']},\"\n",
    "                     f\"{accuracy},{std_accuracy},{precision},{std_precision},{recall},{std_recall},\"\n",
    "                     f\"{f1},{std_f1},{mcc},{std_mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"SVM analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    # Return std_f1_best along with other results\n",
    "    return best_params, best_score, std_f1_best\n",
    "# Run SVM on balance combination\n",
    "print(\"\\nStarting SVM analysis for flaky vs smaller non-flaky files (balance combination)...\")\n",
    "best_params_5folds_svm_balance, best_score_5folds_svm_balance, std_f1_best_balance = runSVM(\n",
    "    dataPointsbalance, dataLabelsListbalance, outDirbalance, 5, \"balance\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for SVM 5-fold on balance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_svm_balance}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_svm_balance} (Std Dev: {std_f1_best_balance})\")\n",
    "\n",
    "# Run SVM on imbalance non-flaky combination\n",
    "print(\"\\nStarting SVM analysis for flaky vs imbalance non-flaky files (imbalance combination)...\")\n",
    "best_params_5folds_svm_imbalance, best_score_5folds_svm_imbalance, std_f1_best_imbalance = runSVM(\n",
    "    dataPointsimbalance, dataLabelsListimbalance, outDirimbalance, 5, \"imbalance\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for SVM 5-fold on imbalance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_svm_imbalance}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_svm_imbalance} (Std Dev: {std_f1_best_imbalance})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8185e97",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac1b2a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGBoost analysis for flaky vs imbalance non-flaky files (balance combination)...\n",
      "Starting runXGB with combination_label: balance\n",
      "Parameter grid for balance: {'pca__n_components': [50, 60, 65], 'xgb__n_estimators': [100, 150, 200], 'xgb__max_depth': [3, 5, 7, 10], 'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5]}\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'pca__n_components': 50, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Best F1 Score: 0.7701589865541946 (Std Dev: 0.07448215535694924)\n",
      "Best Parameters: {'pca__n_components': 50, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Best F1 Score: 0.7701589865541946 (Std Dev: 0.07448215535694924)\n",
      "XGBoost analysis completed for 5-folds. Results saved to: xgb-results-vanilla.csv\n",
      "\n",
      "Best results for XGBoost 5-fold on balance combination:\n",
      "Best Parameters: {'pca__n_components': 50, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Best F1 Score: 0.7701589865541946 (Std Dev: 0.07448215535694924)\n",
      "\n",
      "Starting XGBoost analysis for flaky vs imbalance non-flaky files (imbalance combination)...\n",
      "Starting runXGB with combination_label: imbalance\n",
      "Parameter grid for imbalance: {'pca__n_components': [150, 180, 200, 220], 'xgb__n_estimators': [100, 150, 200], 'xgb__max_depth': [3, 5, 7, 10], 'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5]}\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'pca__n_components': 150, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 150}\n",
      "Best F1 Score: 0.5822510822510822 (Std Dev: 0.13133374400163644)\n",
      "Best hyperparameters saved to: results/best_hyperparameter\\imbalance_best_params_xgb.json\n",
      "Best Parameters: {'pca__n_components': 150, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 150}\n",
      "Best F1 Score: 0.5822510822510822 (Std Dev: 0.13133374400163644)\n",
      "XGBoost analysis completed for 5-folds. Results saved to: xgb-results-vanilla.csv\n",
      "\n",
      "Best results for XGBoost 5-fold on imbalance combination:\n",
      "Best Parameters: {'pca__n_components': 150, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 150}\n",
      "Best F1 Score: 0.5822510822510822 (Std Dev: 0.13133374400163644)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import warnings  # Import the warnings module\n",
    "\n",
    "# Suppress warnings from XGBoost\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "def runXGB(dataPoints, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    print(f\"Starting runXGB with combination_label: {combination_label}\")\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define the pipeline with CountVectorizer, PCA, and XGBoost\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "        ('pca', PCA(random_state=42)),\n",
    "        ('xgb', XGBClassifier(\n",
    "            eval_metric='logloss',  \n",
    "            use_label_encoder=False, \n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    if combination_label == \"balance\":\n",
    "        param_grid = {\n",
    "            'pca__n_components': [50, 60, 65],\n",
    "            'xgb__n_estimators': [100, 150, 200],\n",
    "            'xgb__max_depth': [3, 5, 7, 10],\n",
    "            'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "        }\n",
    "    else:\n",
    "        param_grid = {\n",
    "            'pca__n_components': [150, 180, 200, 220],\n",
    "            'xgb__n_estimators': [100, 150, 200],\n",
    "            'xgb__max_depth': [3, 5, 7, 10],\n",
    "            'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "        }\n",
    "\n",
    "    print(f\"Parameter grid for {combination_label}: {param_grid}\")\n",
    "\n",
    "    # Custom scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer,\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with the pipeline\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit='f1',\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV on data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Get the index of the best parameter combination\n",
    "    best_index = grid_search.best_index_\n",
    "    # Get the standard deviation of F1 score for the best parameter combination\n",
    "    std_f1_best = grid_search.cv_results_['std_test_f1'][best_index]\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score} (Std Dev: {std_f1_best})\")\n",
    "    if combination_label == \"imbalance\":\n",
    "        best_params_file = os.path.join(best_hyperparameter,  f\"{combination_label}_best_params_xgb.json\")\n",
    "        with open(best_params_file, 'w') as f:\n",
    "            json.dump(best_params, f)\n",
    "        print(f\"Best hyperparameters saved to: {best_params_file}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score} (Std Dev: {std_f1_best})\")\n",
    "    # Save the results\n",
    "    outFile = f\"xgb-results-vanilla.csv\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Update CSV header to include standard deviation columns\n",
    "        fo.write(\"pca_n_components,n_estimators,max_depth,learning_rate,accuracy,std_accuracy,precision,std_precision,recall,std_recall,f1,std_f1,mcc,std_mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            std_accuracy = grid_search.cv_results_['std_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            std_precision = grid_search.cv_results_['std_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            std_recall = grid_search.cv_results_['std_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            std_f1 = grid_search.cv_results_['std_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            std_mcc = grid_search.cv_results_['std_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)\n",
    "            # Write the parameters, mean scores, and standard deviations to the CSV\n",
    "            fo.write(f\"{param['pca__n_components']},{param['xgb__n_estimators']},{param['xgb__max_depth']},{param['xgb__learning_rate']},\"\n",
    "                     f\"{accuracy},{std_accuracy},{precision},{std_precision},{recall},{std_recall},{f1},{std_f1},{mcc},{std_mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    # Return std_f1_best along with other results\n",
    "    return best_params, best_score, std_f1_best\n",
    "\n",
    "print(\"\\nStarting XGBoost analysis for flaky vs balance non-flaky files (balance combination)...\")\n",
    "best_params_5folds_xgb_balance, best_score_5folds_xgb_balance, std_f1_best_balance = runXGB(\n",
    "    dataPointsbalance, dataLabelsListbalance, outDirbalance, 5, \"balance\"\n",
    ")\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost 5-fold on balance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_xgb_balance}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_xgb_balance} (Std Dev: {std_f1_best_balance})\")\n",
    "\n",
    "# Run XGBoost on imbalance combination\n",
    "print(\"\\nStarting XGBoost analysis for flaky vs imbalance non-flaky files (imbalance combination)...\")\n",
    "best_params_5folds_xgb_imbalance, best_score_5folds_xgb_imbalance, std_f1_best_imbalance = runXGB(\n",
    "    dataPointsimbalance, dataLabelsListimbalance, outDirimbalance, 5, \"imbalance\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost 5-fold on imbalance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_xgb_imbalance}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_xgb_imbalance} (Std Dev: {std_f1_best_imbalance})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f0847",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4750a0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2635afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Random Forest analysis for flaky vs smaller non-flaky files (balance combination)...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Parameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 200}\n",
      "Best F1 Score from cross-validation: 0.8908359133126936\n",
      "Best hyperparameters saved to: results/best_hyperparameter\\best_params_rf.json\n",
      "Best Parameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 200}\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.8786363636363637 (Std Dev: 0.07076325329243224)\n",
      "Final Recall: 0.9111111111111111 (Std Dev: 0.08314794192830981)\n",
      "Final Accuracy: 0.8888888888888887 (Std Dev: 0.0496903994999953)\n",
      "Final F1 Score: 0.8908359133126936 (Std Dev: 0.050435935677112964)\n",
      "Final MCC: 0.7856438407434342 (Std Dev: 0.1001822478018566)\n",
      "Random Forest analysis completed for 5-folds. Results saved to: rf-results-vanilla.csv\n",
      "\n",
      "Best results for Random Forest 5-fold on balance combination:\n",
      "Best Parameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 200}\n",
      "Best F1 Score: 0.8908359133126936 (Std Dev: 0.050435935677112964)\n",
      "Final MCC: 0.7856438407434342 (Std Dev: 0.1001822478018566)\n",
      "\n",
      "Starting Random Forest analysis for flaky vs imbalance non-flaky files (imbalance combination)...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Parameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 50}\n",
      "Best F1 Score from cross-validation: 0.8661764705882353\n",
      "Best hyperparameters saved to: results/best_hyperparameter\\best_params_rf.json\n",
      "Best Parameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 50}\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.9464285714285715 (Std Dev: 0.0658538889806635)\n",
      "Final Recall: 0.7999999999999999 (Std Dev: 0.08314794192830981)\n",
      "Final Accuracy: 0.9617664851784635 (Std Dev: 0.020420985102973584)\n",
      "Final F1 Score: 0.8661764705882353 (Std Dev: 0.07299808027053445)\n",
      "Final MCC: 0.8487345249535231 (Std Dev: 0.08319818066872256)\n",
      "Random Forest analysis completed for 5-folds. Results saved to: rf-results-vanilla.csv\n",
      "\n",
      "Best results for Random Forest 5-fold on imbalance combination:\n",
      "Best Parameters: {'rf__criterion': 'entropy', 'rf__max_depth': 10, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 50}\n",
      "Best F1 Score: 0.8661764705882353 (Std Dev: 0.07299808027053445)\n",
      "Final MCC: 0.8487345249535231 (Std Dev: 0.08319818066872256)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "###############################################################################\n",
    "# Random Forest\n",
    "\n",
    "def runRandomForest(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with CountVectorizer and Random Forest\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Vectorizer\n",
    "        ('rf', RandomForestClassifier(random_state=42))    # Random Forest classifier\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'rf__n_estimators': [50, 100, 200],                 # Number of trees in the forest\n",
    "        'rf__max_depth': [10, 20, 30],                      # Maximum depth of the tree\n",
    "        'rf__min_samples_split': [5, 10],                   # Minimum number of samples required to split a node\n",
    "        'rf__min_samples_leaf': [2, 5],                     # Minimum number of samples required at a leaf node\n",
    "        'rf__criterion': ['gini', 'entropy'],               # Function to measure the quality of a split\n",
    "        #'rf__class_weight': ['balanced']                    # Class weights\n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "    best_params_file = os.path.join(best_hyperparameter,  f\"best_params_rf.json\")\n",
    "    with open(best_params_file, 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    print(f\"Best hyperparameters saved to: {best_params_file}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'n_estimators': results['params'][idx].get('rf__n_estimators'),\n",
    "            'max_depth': results['params'][idx].get('rf__max_depth'),\n",
    "            'min_samples_split': results['params'][idx].get('rf__min_samples_split'),\n",
    "            'min_samples_leaf': results['params'][idx].get('rf__min_samples_leaf'),\n",
    "            'criterion': results['params'][idx].get('rf__criterion'),\n",
    "            'class_weight': results['params'][idx].get('rf__class_weight'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],\n",
    "            'std_accuracy': results.get('std_test_accuracy', [None])[idx],\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'std_precision': results.get('std_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'std_recall': results.get('std_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'std_f1': results.get('std_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],\n",
    "            'std_mcc': results.get('std_test_mcc', [None])[idx],\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    best_index = grid_search.best_index_\n",
    "    final_f1 = results['mean_test_f1'][best_index]\n",
    "    std_final_f1 = results['std_test_f1'][best_index]\n",
    "    final_precision = results['mean_test_precision'][best_index]\n",
    "    std_final_precision = results['std_test_precision'][best_index]\n",
    "    final_recall = results['mean_test_recall'][best_index]\n",
    "    std_final_recall = results['std_test_recall'][best_index]\n",
    "    final_accuracy = results['mean_test_accuracy'][best_index]\n",
    "    std_final_accuracy = results['std_test_accuracy'][best_index]\n",
    "    final_mcc = results['mean_test_mcc'][best_index]\n",
    "    std_final_mcc = results['std_test_mcc'][best_index]\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision} (Std Dev: {std_final_precision})\")\n",
    "    print(f\"Final Recall: {final_recall} (Std Dev: {std_final_recall})\")\n",
    "    print(f\"Final Accuracy: {final_accuracy} (Std Dev: {std_final_accuracy})\")\n",
    "    print(f\"Final F1 Score: {final_f1} (Std Dev: {std_final_f1})\")\n",
    "    print(f\"Final MCC: {final_mcc} (Std Dev: {std_final_mcc})\")\n",
    "  # Save the results\n",
    "    outFile = f\"rf-results-vanilla.csv\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Update CSV header to include standard deviation columns\n",
    "        fo.write(\"pca_n_components,n_estimators,max_depth,learning_rate,accuracy,std_accuracy,precision,std_precision,recall,std_recall,f1,std_f1,mcc,std_mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            std_accuracy = grid_search.cv_results_['std_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            std_precision = grid_search.cv_results_['std_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            std_recall = grid_search.cv_results_['std_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            std_f1 = grid_search.cv_results_['std_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            std_mcc = grid_search.cv_results_['std_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)\n",
    "            # Write the parameters, mean scores, and standard deviations to the CSV\n",
    "            fo.write(f\"{param[ 'rf__n_estimators']},{param[ 'rf__max_depth']},{param['rf__min_samples_split']},{param['rf__min_samples_leaf'],{param['rf__criterion']}},\"\n",
    "                     f\"{accuracy},{std_accuracy},{precision},{std_precision},{recall},{std_recall},{f1},{std_f1},{mcc},{std_mcc},{preparationTime}\\n\")\n",
    "\n",
    "\n",
    "    print(f\"Random Forest analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "\n",
    "    # Return the best parameters and metrics with their standard deviations\n",
    "    return best_params, final_f1, std_final_f1, final_mcc, std_final_mcc\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Random Forest on balance combination\n",
    "print(\"\\nStarting Random Forest analysis for flaky vs smaller non-flaky files (balance combination)...\")\n",
    "best_params_5folds_balance, best_score_5folds_balance, std_f1_balance, balance_mcc_5folds, std_mcc_balance = runRandomForest(\n",
    "    dataPointsbalance, dataLabelsListbalance, outDirbalance, 5\n",
    ")\n",
    "\n",
    "print(\"\\nBest results for Random Forest 5-fold on balance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_balance}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_balance} (Std Dev: {std_f1_balance})\")\n",
    "print(f\"Final MCC: {balance_mcc_5folds} (Std Dev: {std_mcc_balance})\")\n",
    "\n",
    "# Run Random Forest on imbalance combination\n",
    "print(\"\\nStarting Random Forest analysis for flaky vs imbalance non-flaky files (imbalance combination)...\")\n",
    "best_params_5folds_imbalance, best_f1_5folds, std_f1_imbalance, final_mcc_5folds, std_mcc_imbalance = runRandomForest(\n",
    "    dataPointsimbalance, dataLabelsListimbalance, outDirimbalance, 5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for Random Forest 5-fold on imbalance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds} (Std Dev: {std_f1_imbalance})\")\n",
    "print(f\"Final MCC: {final_mcc_5folds} (Std Dev: {std_mcc_imbalance})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2db92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4cd022",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd039429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Decision Tree analysis for flaky vs smaller non-flaky files (balance combination)...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best hyperparameters saved to: results/best_hyperparameter\\best_params_dt.json\n",
      "Best Parameters: {'dt__criterion': 'entropy', 'dt__max_depth': 10, 'dt__max_features': None, 'dt__min_samples_leaf': 5, 'dt__min_samples_split': 5}\n",
      "Best Parameters: {'dt__criterion': 'entropy', 'dt__max_depth': 10, 'dt__max_features': None, 'dt__min_samples_leaf': 5, 'dt__min_samples_split': 5}\n",
      "Best F1 Score from cross-validation: 0.8829640947288006 (Std Dev: 0.10307657727394873)\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.9384615384615385 (Std Dev: 0.12307692307692308)\n",
      "Final Recall: 0.8666666666666668 (Std Dev: 0.1632993161855452)\n",
      "Final Accuracy: 0.888888888888889 (Std Dev: 0.09296222517045283)\n",
      "Final F1 Score: 0.8829640947288006 (Std Dev: 0.10307657727394873)\n",
      "Final MCC: 0.8058403455783832 (Std Dev: 0.15642087867118531)\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: dt-results-vanilla.csv\n",
      "\n",
      "Best results for Decision Tree 5-fold on balance combination:\n",
      "Best Parameters: {'dt__criterion': 'entropy', 'dt__max_depth': 10, 'dt__max_features': None, 'dt__min_samples_leaf': 5, 'dt__min_samples_split': 5}\n",
      "Best F1 Score: 0.8829640947288006 (Std Dev: 0.10307657727394873)\n",
      "Best MCC Score: 0.8058403455783832 (Std Dev: 0.15642087867118531)\n",
      "\n",
      "Starting Decision Tree analysis for flaky vs imbalance non-flaky files (imbalance combination)...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best hyperparameters saved to: results/best_hyperparameter\\best_params_dt.json\n",
      "Best Parameters: {'dt__criterion': 'entropy', 'dt__max_depth': 10, 'dt__max_features': None, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 10}\n",
      "Best Parameters: {'dt__criterion': 'entropy', 'dt__max_depth': 10, 'dt__max_features': None, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 10}\n",
      "Best F1 Score from cross-validation: 0.8765775401069519 (Std Dev: 0.07936549544226573)\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.9134615384615385 (Std Dev: 0.12071042174211706)\n",
      "Final Recall: 0.8666666666666668 (Std Dev: 0.12957670877434002)\n",
      "Final Accuracy: 0.9620084694494857 (Std Dev: 0.025290395415347663)\n",
      "Final F1 Score: 0.8765775401069519 (Std Dev: 0.07936549544226573)\n",
      "Final MCC: 0.8636674239403248 (Std Dev: 0.08665187639527051)\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: dt-results-vanilla.csv\n",
      "\n",
      "Best results for Decision Tree 5-fold on imbalance combination:\n",
      "Best Parameters: {'dt__criterion': 'entropy', 'dt__max_depth': 10, 'dt__max_features': None, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 10}\n",
      "Best F1 Score: 0.8765775401069519 (Std Dev: 0.07936549544226573)\n",
      "Best MCC Score: 0.8636674239403248 (Std Dev: 0.08665187639527051)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree\n",
    "\n",
    "def runDecisionTree(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer\n",
    "    }\n",
    "\n",
    "    # Define a pipeline and Decision Tree\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Vectorizer\n",
    "        ('dt', DecisionTreeClassifier(random_state=42))    # Decision Tree classifier\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'dt__max_depth': [10, 20, 30],                     # Maximum depth of the tree\n",
    "        'dt__min_samples_split': [5, 10],                  # Minimum number of samples required to split a node\n",
    "        'dt__min_samples_leaf': [2, 5],                    # Minimum number of samples required at a leaf node\n",
    "        'dt__criterion': ['gini', 'entropy'],              # Function to measure the quality of a split\n",
    "        'dt__max_features': [None, 'sqrt', 'log2'],        # Controls how many features to consider for splits\n",
    "        #    'dt__class_weight': ['balanced']                   # Class weights\n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring,\n",
    "        refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    # Get the index of the best parameter combination\n",
    "    best_index = grid_search.best_index_\n",
    "\n",
    "    # Extract the cross-validation results\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Extract standard deviations of the best scores\n",
    "    std_f1_best = results['std_test_f1'][best_index]\n",
    "    std_mcc_best = results['std_test_mcc'][best_index]\n",
    "    best_params_file = os.path.join(best_hyperparameter,  f\"best_params_dt.json\")\n",
    "    with open(best_params_file, 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    print(f\"Best hyperparameters saved to: {best_params_file}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv} (Std Dev: {std_f1_best})\")\n",
    "\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'max_depth': results['params'][idx].get('dt__max_depth'),\n",
    "            'min_samples_split': results['params'][idx].get('dt__min_samples_split'),\n",
    "            'min_samples_leaf': results['params'][idx].get('dt__min_samples_leaf'),\n",
    "            'criterion': results['params'][idx].get('dt__criterion'),\n",
    "            'max_features': results['params'][idx].get('dt__max_features'),\n",
    "            'class_weight': results['params'][idx].get('dt__class_weight'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],\n",
    "            'std_accuracy': results.get('std_test_accuracy', [None])[idx],\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'std_precision': results.get('std_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'std_recall': results.get('std_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'std_f1': results.get('std_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],\n",
    "            'std_mcc': results.get('std_test_mcc', [None])[idx],\n",
    "        })\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][best_index]\n",
    "    std_final_f1 = results['std_test_f1'][best_index]\n",
    "    final_precision = results['mean_test_precision'][best_index]\n",
    "    std_final_precision = results['std_test_precision'][best_index]\n",
    "    final_recall = results['mean_test_recall'][best_index]\n",
    "    std_final_recall = results['std_test_recall'][best_index]\n",
    "    final_accuracy = results['mean_test_accuracy'][best_index]\n",
    "    std_final_accuracy = results['std_test_accuracy'][best_index]\n",
    "    final_mcc = results['mean_test_mcc'][best_index]\n",
    "    std_final_mcc = results['std_test_mcc'][best_index]\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision} (Std Dev: {std_final_precision})\")\n",
    "    print(f\"Final Recall: {final_recall} (Std Dev: {std_final_recall})\")\n",
    "    print(f\"Final Accuracy: {final_accuracy} (Std Dev: {std_final_accuracy})\")\n",
    "    print(f\"Final F1 Score: {final_f1} (Std Dev: {std_final_f1})\")\n",
    "    print(f\"Final MCC: {final_mcc} (Std Dev: {std_final_mcc})\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile = f\"dt-results-vanilla.csv\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Update CSV header to include standard deviation columns\n",
    "        fo.write(\"pca_n_components,n_estimators,max_depth,learning_rate,accuracy,std_accuracy,precision,std_precision,recall,std_recall,f1,std_f1,mcc,std_mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            std_accuracy = grid_search.cv_results_['std_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            std_precision = grid_search.cv_results_['std_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            std_recall = grid_search.cv_results_['std_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            std_f1 = grid_search.cv_results_['std_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            std_mcc = grid_search.cv_results_['std_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)\n",
    "            # Write the parameters, mean scores, and standard deviations to the CSV\n",
    "            fo.write(f\"{param[ 'dt__max_depth']},{param[  'dt__min_samples_split']},{param['dt__min_samples_leaf']},{param['dt__criterion'],{param['dt__max_features']}},\"\n",
    "                     f\"{accuracy},{std_accuracy},{precision},{std_precision},{recall},{std_recall},{f1},{std_f1},{mcc},{std_mcc},{preparationTime}\\n\")\n",
    "     \n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "\n",
    "    # Return the best parameters and metrics with their standard deviations\n",
    "    return best_params, final_f1, std_final_f1, final_mcc, std_final_mcc\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Decision Tree on balance combination\n",
    "print(\"\\nStarting Decision Tree analysis for flaky vs smaller non-flaky files (balance combination)...\")\n",
    "best_params_5folds_balance, best_f1_score_balance, std_f1_balance, best_mcc_balance, std_mcc_balance = runDecisionTree(\n",
    "    dataPointsbalance, dataLabelsListbalance, outDirbalance, 5\n",
    ")\n",
    "\n",
    "print(\"\\nBest results for Decision Tree 5-fold on balance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_balance}\")\n",
    "print(f\"Best F1 Score: {best_f1_score_balance} (Std Dev: {std_f1_balance})\")\n",
    "print(f\"Best MCC Score: {best_mcc_balance} (Std Dev: {std_mcc_balance})\")\n",
    "\n",
    "# Run Decision Tree on imbalance non-flaky combination\n",
    "print(\"\\nStarting Decision Tree analysis for flaky vs imbalance non-flaky files (imbalance combination)...\")\n",
    "best_params_5folds_imbalance, best_f1_score_imbalance, std_f1_imbalance, best_mcc_imbalance, std_mcc_imbalance = runDecisionTree(\n",
    "    dataPointsimbalance, dataLabelsListimbalance, outDirimbalance, 5\n",
    ")\n",
    "\n",
    "print(\"\\nBest results for Decision Tree 5-fold on imbalance combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "print(f\"Best F1 Score: {best_f1_score_imbalance} (Std Dev: {std_f1_imbalance})\")\n",
    "print(f\"Best MCC Score: {best_mcc_imbalance} (Std Dev: {std_mcc_imbalance})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2106766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
