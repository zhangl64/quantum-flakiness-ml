{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xY94WkB0xrt_",
    "outputId": "6cf94895-a052-4455-c17b-e5b95c58922a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky files: 45\n",
      "Number of non-flaky files: 243\n",
      "Total number of data points: 288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "# Function to extract zip files\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "# Function to read .py files from the given directory\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file skipped: {file_path}\")\n",
    "    return dataPointsList\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "# Paths to your zip files (update these paths as necessary)\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"Dataset/nonflaky_files.zip\"\n",
    "\n",
    "# Create directories for extraction\n",
    "extractDir = \"extracted_files\"\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "\n",
    "# Ensure directories exist for flaky and non-flaky files\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "# Extract the zip files to their respective directories\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "# Collect data points (Python files) from both flaky and non-flaky directories\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "\n",
    "# Combine the data points from both classes (flaky and non-flaky)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "# Create labels: 1 for flaky files and 0 for non-flaky files\n",
    "dataLabelsList = np.array([1] * len(dataPointsFlaky) + [0] * len(dataPointsNonFlaky))\n",
    "\n",
    "# Output some basic information\n",
    "print(f\"Number of flaky files: {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky files: {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of data points: {len(dataPoints)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McHkV0aFx4sL"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfYjJdp_x7le",
    "outputId": "691fe17e-312c-4b7e-e602-69e455dedc0a"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Summarize and find best parameter combinations\n",
    "def summarize_and_find_best(df_results):\n",
    "    summary_df = df_results.groupby(['rf__n_estimators', 'rf__max_depth', 'rf__min_samples_split', 'rf__min_samples_leaf', 'rf__criterion', 'threshold']).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'f1': 'mean',\n",
    "        'mcc': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Finding the best parameter set based on the highest F1 score\n",
    "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
    "\n",
    "    return summary_df, best_row\n",
    "\n",
    "# Combined SMOTE and Threshold-based Random Forest \n",
    "def runRFWithSMOTEAndThreshold(dataPoints, y, outDir, n_splits, param_grid, thresholds):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Define pipeline with Vectorizer, SMOTE, and Random Forest\n",
    "        pipeline = ImbPipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words=None)),  # Vectorizer to convert text data to numerical\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=param_dict['rf__n_estimators'],\n",
    "                max_depth=param_dict['rf__max_depth'],\n",
    "                min_samples_split=param_dict['rf__min_samples_split'],\n",
    "                min_samples_leaf=param_dict['rf__min_samples_leaf'],\n",
    "                criterion=param_dict['rf__criterion'],\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(dataPoints, y)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "\n",
    "                # Store results\n",
    "                metrics_per_combination.append({\n",
    "                    'rf__n_estimators': param_dict['rf__n_estimators'],\n",
    "                    'rf__max_depth': param_dict['rf__max_depth'],\n",
    "                    'rf__min_samples_split': param_dict['rf__min_samples_split'],\n",
    "                    'rf__min_samples_leaf': param_dict['rf__min_samples_leaf'],\n",
    "                    'rf__criterion': param_dict['rf__criterion'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    # Convert metrics to DataFrame and save results\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    outFile = os.path.join(outDir, \"rf-smote-threshold-results.csv\")\n",
    "    df_results.to_csv(outFile, index=False)\n",
    "\n",
    "    # Summarize results and return the best parameter set\n",
    "    summary_df, best_params = summarize_and_find_best(df_results)\n",
    "\n",
    "    # Save summarized results to a new CSV file\n",
    "    summary_outFile = os.path.join(outDir, \"rf-smote-threshold-summary.csv\")\n",
    "    summary_df.to_csv(summary_outFile, index=False)\n",
    "    print(f\"Summary of results saved to {summary_outFile}\")\n",
    "\n",
    "    return df_results, best_params\n",
    "\n",
    "# Hyperparameters and thresholds\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],                   # Number of trees in the forest\n",
    "    'rf__max_depth': [10, 20, 30],                        # Maximum depth of the tree\n",
    "    'rf__min_samples_split': [5, 10],                     # Minimum number of samples required to split a node\n",
    "    'rf__min_samples_leaf': [2, 5],                       # Minimum number of samples required at a leaf node\n",
    "    'rf__criterion': ['gini', 'entropy']                  # Function to measure the quality of a split\n",
    "}\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "\n",
    "outDir = \"results/\"\n",
    "df_results, best_params = runRFWithSMOTEAndThreshold(dataPoints, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
    "\n",
    "print(\"\\nRandom Forest with SMOTE and Threshold analysis completed.\")\n",
    "print(\"Best parameter set based on F1-score:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6_eoOI9ycjk"
   },
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoFWUzGmyecK",
    "outputId": "0112bb1f-4db0-482b-e8f6-2d68a0092411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of results saved to results/dt-smote-threshold-summary.csv\n",
      "\n",
      "Decision Tree with SMOTE and Threshold analysis completed.\n",
      "Best parameter set based on F1-score:\n",
      "dt__max_depth                  10\n",
      "dt__min_samples_split           5\n",
      "dt__min_samples_leaf            2\n",
      "dt__criterion             entropy\n",
      "dt__max_features             sqrt\n",
      "threshold                     0.7\n",
      "accuracy                 0.885662\n",
      "precision                0.668298\n",
      "recall                   0.688889\n",
      "f1                       0.663687\n",
      "mcc                      0.605756\n",
      "Name: 15, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Summarize and find best parameter combinations\n",
    "def summarize_and_find_best(df_results):\n",
    "    summary_df = df_results.groupby(['dt__max_depth', 'dt__min_samples_split', 'dt__min_samples_leaf', 'dt__criterion', 'dt__max_features', 'threshold']).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'f1': 'mean',\n",
    "        'mcc': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Finding the best parameter set based on the highest F1 score\n",
    "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
    "\n",
    "    return summary_df, best_row\n",
    "\n",
    "# Combined SMOTE and Threshold-based Decision Tree \n",
    "def runDTWithSMOTEAndThreshold(dataPoints, y, outDir, n_splits, param_grid, thresholds):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Define pipeline with Vectorizer, SMOTE, and Decision Tree\n",
    "        pipeline = ImbPipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(\n",
    "                max_depth=param_dict['dt__max_depth'],\n",
    "                min_samples_split=param_dict['dt__min_samples_split'],\n",
    "                min_samples_leaf=param_dict['dt__min_samples_leaf'],\n",
    "                criterion=param_dict['dt__criterion'],\n",
    "                max_features=param_dict['dt__max_features'],  # max_features parameter added\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(dataPoints, y)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Decision Tree probability for positive class\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "\n",
    "                # Store results\n",
    "                metrics_per_combination.append({\n",
    "                    'dt__max_depth': param_dict['dt__max_depth'],\n",
    "                    'dt__min_samples_split': param_dict['dt__min_samples_split'],\n",
    "                    'dt__min_samples_leaf': param_dict['dt__min_samples_leaf'],\n",
    "                    'dt__criterion': param_dict['dt__criterion'],\n",
    "                    'dt__max_features': param_dict['dt__max_features'],  # Store max_features in results\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    # Convert metrics to DataFrame and save results\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    outFile = os.path.join(outDir, \"dt-smote-threshold-results.csv\")\n",
    "    df_results.to_csv(outFile, index=False)\n",
    "\n",
    "    # Summarize results and return the best parameter set\n",
    "    summary_df, best_params = summarize_and_find_best(df_results)\n",
    "\n",
    "    # Save summarized results to a new CSV file\n",
    "    summary_outFile = os.path.join(outDir, \"dt-smote-threshold-summary.csv\")\n",
    "    summary_df.to_csv(summary_outFile, index=False)\n",
    "    print(f\"Summary of results saved to {summary_outFile}\")\n",
    "\n",
    "    return df_results, best_params\n",
    "\n",
    "# Hyperparameters and thresholds\n",
    "param_grid = {\n",
    "    'dt__max_depth': [10, 20, 30],                  # Maximum depth of the decision tree\n",
    "    'dt__min_samples_split': [5, 10],               # Minimum number of samples required to split a node\n",
    "    'dt__min_samples_leaf': [2, 5],                 # Minimum number of samples required at a leaf node\n",
    "    'dt__criterion': ['gini', 'entropy'],           # Function to measure the quality of a split\n",
    "    'dt__max_features': [None, 'sqrt', 'log2'],     # Controls how many features to consider for splits\n",
    "}\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "\n",
    "outDir = \"results/\"\n",
    "df_results, best_params = runDTWithSMOTEAndThreshold(dataPoints, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
    "\n",
    "print(\"\\nDecision Tree with SMOTE and Threshold analysis completed.\")\n",
    "print(\"Best parameter set based on F1-score:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Obxhsyp84Ib8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of results saved to results/dt-smote-threshold-summary.csv\n",
      "\n",
      "Decision Tree with SMOTE and Threshold analysis completed.\n",
      "Best parameter set based on F1-score:\n",
      "dt__max_depth                  10\n",
      "dt__min_samples_split           5\n",
      "dt__min_samples_leaf            2\n",
      "dt__criterion             entropy\n",
      "dt__max_features             sqrt\n",
      "threshold                     0.7\n",
      "accuracy                 0.885662\n",
      "precision                0.668298\n",
      "recall                   0.688889\n",
      "f1                       0.663687\n",
      "mcc                      0.605756\n",
      "Name: 15, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Summarize and find best parameter combinations\n",
    "def summarize_and_find_best(df_results):\n",
    "    summary_df = df_results.groupby(['dt__max_depth', 'dt__min_samples_split', 'dt__min_samples_leaf', 'dt__criterion', 'dt__max_features', 'threshold']).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'f1': 'mean',\n",
    "        'mcc': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Finding the best parameter set based on the highest F1 score\n",
    "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
    "\n",
    "    return summary_df, best_row\n",
    "\n",
    "# Combined SMOTE and Threshold-based Decision Tree with Vectorizer\n",
    "def runDTWithSMOTEAndThreshold(dataPoints, y, outDir, n_splits, param_grid, thresholds):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Define pipeline with Vectorizer, SMOTE, and Decision Tree\n",
    "        pipeline = ImbPipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(\n",
    "                max_depth=param_dict['dt__max_depth'],\n",
    "                min_samples_split=param_dict['dt__min_samples_split'],\n",
    "                min_samples_leaf=param_dict['dt__min_samples_leaf'],\n",
    "                criterion=param_dict['dt__criterion'],\n",
    "                max_features=param_dict['dt__max_features'],\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(dataPoints, y)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Decision Tree probability for positive class\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "\n",
    "                # Store results\n",
    "                metrics_per_combination.append({\n",
    "                    'dt__max_depth': param_dict['dt__max_depth'],\n",
    "                    'dt__min_samples_split': param_dict['dt__min_samples_split'],\n",
    "                    'dt__min_samples_leaf': param_dict['dt__min_samples_leaf'],\n",
    "                    'dt__criterion': param_dict['dt__criterion'],\n",
    "                    'dt__max_features': param_dict['dt__max_features'],  # Store max_features in results\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    # Convert metrics to DataFrame and save results\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    outFile = os.path.join(outDir, \"dt-smote-threshold-results.csv\")\n",
    "    df_results.to_csv(outFile, index=False)\n",
    "\n",
    "    # Summarize results and return the best parameter set\n",
    "    summary_df, best_params = summarize_and_find_best(df_results)\n",
    "\n",
    "    # Save summarized results to a new CSV file\n",
    "    summary_outFile = os.path.join(outDir, \"dt-smote-threshold-summary.csv\")\n",
    "    summary_df.to_csv(summary_outFile, index=False)\n",
    "    print(f\"Summary of results saved to {summary_outFile}\")\n",
    "\n",
    "    return df_results, best_params\n",
    "\n",
    "# Hyperparameters and thresholds\n",
    "param_grid = {\n",
    "    'dt__max_depth': [10, 20, 30],                  # Maximum depth of the decision tree\n",
    "    'dt__min_samples_split': [5, 10],               # Minimum number of samples required to split a node\n",
    "    'dt__min_samples_leaf': [2, 5],                 # Minimum number of samples required at a leaf node\n",
    "    'dt__criterion': ['gini', 'entropy'],           # Function to measure the quality of a split\n",
    "    'dt__max_features': [None, 'sqrt', 'log2'],     # Controls how many features to consider for splits\n",
    "}\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "\n",
    "outDir = \"results/\"\n",
    "df_results, best_params = runDTWithSMOTEAndThreshold(dataPoints, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
    "\n",
    "print(\"\\nDecision Tree with SMOTE and Threshold analysis completed.\")\n",
    "print(\"Best parameter set based on F1-score:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just add Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing parameter combination 1/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 2/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 3/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 4/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 5/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 6/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 7/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 8/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 9/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 10/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 11/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 12/72: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 13/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 14/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 15/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 16/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 17/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 18/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 19/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 20/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 21/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 22/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 23/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 24/72: {'dt__max_depth': 10, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 25/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 26/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 27/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 28/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 29/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 30/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 31/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 32/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 33/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 34/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 35/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 36/72: {'dt__max_depth': 20, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 37/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 38/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 39/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 40/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 41/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 42/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 43/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 44/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 45/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 46/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 47/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 48/72: {'dt__max_depth': 20, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 49/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 50/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 51/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 52/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 53/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 54/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 55/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 56/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 57/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 58/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 59/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 60/72: {'dt__max_depth': 30, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 61/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 62/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 63/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 64/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 65/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 66/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 2, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 67/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Processing parameter combination 68/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 69/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'gini', 'dt__max_features': 'log2'}\n",
      "Processing parameter combination 70/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': None}\n",
      "Processing parameter combination 71/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'sqrt'}\n",
      "Processing parameter combination 72/72: {'dt__max_depth': 30, 'dt__min_samples_split': 10, 'dt__min_samples_leaf': 5, 'dt__criterion': 'entropy', 'dt__max_features': 'log2'}\n",
      "Detailed results saved to results/dt-smote-threshold-results.csv\n",
      "Summary of results saved to results/dt-smote-threshold-summary.csv\n",
      "\n",
      "Best parameter set at threshold=0.5 based on F1-score:\n",
      "dt__max_depth                 10\n",
      "dt__min_samples_split          5\n",
      "dt__min_samples_leaf           2\n",
      "dt__criterion            entropy\n",
      "dt__max_features            log2\n",
      "accuracy                    0.45\n",
      "precision                   0.45\n",
      "recall                       1.0\n",
      "f1                       0.62069\n",
      "mcc                          0.0\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Decision Tree with SMOTE and Threshold analysis completed.\n",
      "Best parameter set at threshold=0.5 based on F1-score:\n",
      "dt__max_depth                 10\n",
      "dt__min_samples_split          5\n",
      "dt__min_samples_leaf           2\n",
      "dt__criterion            entropy\n",
      "dt__max_features            log2\n",
      "accuracy                    0.45\n",
      "precision                   0.45\n",
      "recall                       1.0\n",
      "f1                       0.62069\n",
      "mcc                          0.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import product\n",
    "\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Summarize and find best parameter combinations\n",
    "def summarize_and_find_best(df_results, threshold=0.5):\n",
    "    # Filter results for the specified threshold\n",
    "    df_results = df_results[df_results['threshold'] == threshold]\n",
    "\n",
    "    # Group by hyperparameters (excluding threshold)\n",
    "    summary_df = df_results.groupby([\n",
    "        'dt__max_depth',\n",
    "        'dt__min_samples_split',\n",
    "        'dt__min_samples_leaf',\n",
    "        'dt__criterion',\n",
    "        'dt__max_features'\n",
    "    ]).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'f1': 'mean',\n",
    "        'mcc': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Finding the best parameter set based on the highest F1 score\n",
    "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
    "\n",
    "    return summary_df, best_row\n",
    "\n",
    "# Combined SMOTE and Threshold-based Decision Tree with Vectorizer\n",
    "def runDTWithSMOTEAndThreshold(dataPoints, y, outDir, n_splits, param_grid, thresholds):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Convert dataPoints and y to numpy arrays\n",
    "    dataPoints = np.array(dataPoints)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Generate all combinations of parameters\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = [param_grid[key] for key in param_keys]\n",
    "    param_combinations = list(product(*param_values))\n",
    "\n",
    "    total_combinations = len(param_combinations)\n",
    "    combination_counter = 0\n",
    "\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        combination_counter += 1\n",
    "        print(f\"Processing parameter combination {combination_counter}/{total_combinations}: {param_dict}\")\n",
    "\n",
    "        # Define pipeline with Vectorizer, SMOTE, and Decision Tree\n",
    "        pipeline = ImbPipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(\n",
    "                max_depth=param_dict['dt__max_depth'],\n",
    "                min_samples_split=param_dict['dt__min_samples_split'],\n",
    "                min_samples_leaf=param_dict['dt__min_samples_leaf'],\n",
    "                criterion=param_dict['dt__criterion'],\n",
    "                max_features=param_dict['dt__max_features'],\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(dataPoints, y)):\n",
    "            # Split the data correctly\n",
    "            X_train = dataPoints[train_index]\n",
    "            X_test = dataPoints[test_index]\n",
    "            y_train = y[train_index]\n",
    "            y_test = y[test_index]\n",
    "\n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Decision Tree probability for positive class\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "\n",
    "                # Store results\n",
    "                metrics_per_combination.append({\n",
    "                    'dt__max_depth': param_dict['dt__max_depth'],\n",
    "                    'dt__min_samples_split': param_dict['dt__min_samples_split'],\n",
    "                    'dt__min_samples_leaf': param_dict['dt__min_samples_leaf'],\n",
    "                    'dt__criterion': param_dict['dt__criterion'],\n",
    "                    'dt__max_features': param_dict['dt__max_features'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    # Convert metrics to DataFrame and save results\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    outFile = os.path.join(outDir, \"dt-smote-threshold-results.csv\")\n",
    "    df_results.to_csv(outFile, index=False)\n",
    "\n",
    "    print(f\"Detailed results saved to {outFile}\")\n",
    "\n",
    "    # Summarize results and return the best parameter set at threshold=0.5\n",
    "    summary_df, best_params = summarize_and_find_best(df_results, threshold=0.5)\n",
    "\n",
    "    # Save summarized results to a new CSV file\n",
    "    summary_outFile = os.path.join(outDir, \"dt-smote-threshold-summary.csv\")\n",
    "    summary_df.to_csv(summary_outFile, index=False)\n",
    "    print(f\"Summary of results saved to {summary_outFile}\")\n",
    "\n",
    "    print(\"\\nBest parameter set at threshold=0.5 based on F1-score:\")\n",
    "    print(best_params)\n",
    "\n",
    "    return df_results, best_params\n",
    "\n",
    "# Hyperparameters and thresholds\n",
    "param_grid = {\n",
    "    'dt__max_depth': [10, 20, 30],\n",
    "    'dt__min_samples_split': [5, 10],\n",
    "    'dt__min_samples_leaf': [2, 5],\n",
    "    'dt__criterion': ['gini', 'entropy'],\n",
    "    'dt__max_features': [None, 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "# Replace with your actual data\n",
    "# dataPoints = [...]\n",
    "# dataLabelsList = [...]\n",
    "\n",
    "# For demonstration, let's create some sample data\n",
    "dataPoints = [\"Sample text data {}\".format(i) for i in range(100)]\n",
    "dataLabelsList = np.random.choice([0, 1], size=100)\n",
    "\n",
    "outDir = \"results/\"\n",
    "df_results, best_params = runDTWithSMOTEAndThreshold(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5,\n",
    "    param_grid=param_grid, thresholds=thresholds\n",
    ")\n",
    "\n",
    "print(\"\\nDecision Tree with SMOTE and Threshold analysis completed.\")\n",
    "print(\"Best parameter set at threshold=0.5 based on F1-score:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
