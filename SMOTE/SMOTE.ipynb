{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84acd571",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2d4796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
    "\n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "\n",
    "    return dataPointsList\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Parameters setup\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"Dataset/nonflaky_files.zip\"\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "extractDir = \"smote-extracted\"\n",
    "os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "# Extract the zip files\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "if len(dataPoints) == 0:\n",
    "    raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "# Create labels: 1 for flaky, 0 for non-flaky\n",
    "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf94de6",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f791afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "\n",
      "Starting KNN analysis with SMOTE and PCA for 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Best Parameters with SMOTE and PCA: {'knn__metric': 'cosine', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'pca__n_components': 180}\n",
      "Best F1 Score from cross-validation: 0.6293884220354808\n",
      "Per-fold metrics saved to: smote-results\\knn-smote-pca-fold-results-5-folds.csv\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.6670995670995671\n",
      "Final Recall: 0.6\n",
      "Final Accuracy: 0.8871038251366121\n",
      "Final F1 Score: 0.6293884220354808\n",
      "Final MCC: 0.5656807602939421\n",
      "KNN analysis completed for 5-folds with SMOTE and PCA. Results saved to: smote-results\\knn-results-5-folds.csv\n",
      "\n",
      "Best results for KNN with SMOTE and PCA 5-fold cross-validation:\n",
      "Best Parameters: {'knn__metric': 'cosine', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'pca__n_components': 180}\n",
      "Best F1 Score: 0.6293884220354808\n",
      "Best MCC: 0.5656807602939421\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score,\n",
    "    matthews_corrcoef, make_scorer\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD  # Suitable for sparse data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def runKNNWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    # Define scoring metrics with MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': make_scorer(matthews_corrcoef)  # Directly using matthews_corrcoef\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Define a pipeline with CountVectorizer, TruncatedSVD, SMOTE, and KNN\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "        ('pca', PCA()),\n",
    "        ('smote', SMOTE(random_state=42)),  # SMOTE for oversampling\n",
    "        ('knn', KNeighborsClassifier()),  # KNN classifier\n",
    "        \n",
    "\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'pca__n_components': [150,180,200,200],  # Variance ratios\n",
    "        'knn__n_neighbors': [3, 5, 7, 9, 11,],  # Number of neighbors for KNN\n",
    "        'knn__weights': ['uniform', 'distance'],  # Uniform or distance-based weighting\n",
    "        'knn__metric': ['euclidean', 'cosine']  # Distance metrics\n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring,\n",
    "        refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE and PCA: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'svd__n_components': results['params'][idx].get('svd__n_components'),\n",
    "            'n_neighbors': results['params'][idx].get('knn__n_neighbors'),\n",
    "            'weights': results['params'][idx].get('knn__weights'),\n",
    "            'metric': results['params'][idx].get('knn__metric'),\n",
    "            'accuracy': results['mean_test_accuracy'][idx],\n",
    "            'precision': results['mean_test_precision'][idx],\n",
    "            'recall': results['mean_test_recall'][idx],\n",
    "            'f1': results['mean_test_f1'][idx],\n",
    "            'mcc': results['mean_test_mcc'][idx],  # Include MCC in fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"knn-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    best_index = grid_search.best_index_\n",
    "    final_f1 = results['mean_test_f1'][best_index]\n",
    "    final_precision = results['mean_test_precision'][best_index]\n",
    "    final_recall = results['mean_test_recall'][best_index]\n",
    "    final_accuracy = results['mean_test_accuracy'][best_index]\n",
    "    final_mcc = results['mean_test_mcc'][best_index]  # Include final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the final results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"knn-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC\n",
    "\n",
    "    print(f\"KNN analysis completed for {n_splits}-folds with SMOTE and PCA. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC as well\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "extractDir = \"smote-extracted\"\n",
    "os.makedirs(extractDir, exist_ok=True)  # Corrected 'Tarue' to 'True'\n",
    "\n",
    "# Extract the zip files\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "if len(dataPoints) == 0:\n",
    "    raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "# Create labels: 1 for flaky, 0 for non-flaky\n",
    "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run KNN with SMOTE and PCA using 5-fold cross-validation\n",
    "print(\"\\nStarting KNN analysis with SMOTE and PCA for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runKNNWithSMOTE(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nBest results for KNN with SMOTE and PCA 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Best MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c826fa",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c857480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SVM analysis with SMOTE for 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters with SMOTE: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score from cross-validation: 0.6837175895999426\n",
      "Per-fold metrics saved to: smote-results\\svm-smote-fold-results-5-folds.csv\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.8313131313131313\n",
      "Final Recall: 0.6244444444444444\n",
      "Final Accuracy: 0.9137158469945355\n",
      "Final F1 Score: 0.6837175895999426\n",
      "SVM analysis completed for 5-folds with SMOTE. Results saved to: smote-results\\svm-results-5-folds.csv\n",
      "\n",
      "Best results for SVM with SMOTE 5-fold cross-validation:\n",
      "Best Parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score: 0.6837175895999426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "def runSVMWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1',\n",
    "        'mcc': make_scorer(matthews_corrcoef)  \n",
    "\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Define a pipeline with SMOTE and SVM \n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
    "        ('pca', PCA()),\n",
    "        ('smote', SMOTE(random_state=42)),    # SMOTE for oversampling\n",
    "        ('svm', SVC(probability=True))        # SVM classifier\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "    'pca__n_components': [150, 180, 200, 220],           # PCA components\n",
    "    'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],            # Regularization parameter C for SVM\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types for SVM\n",
    "        }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'C': results['params'][idx].get('svm__C'),\n",
    "            'kernel': results['params'][idx].get('svm__kernel'),\n",
    "            'accuracy': results['mean_test_accuracy'][idx],\n",
    "            'precision': results['mean_test_precision'][idx],\n",
    "            'recall': results['mean_test_recall'][idx],\n",
    "            'f1': results['mean_test_f1'][idx],\n",
    "            'mcc': results['mean_test_mcc'][idx],\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile = os.path.join(outDir, f\"svm-smote-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile = os.path.join(outDir, f\"svm-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1\\n\")\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1}\\n\")\n",
    "\n",
    "    print(f\"SVM analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
    "\n",
    "    return best_params, final_f1\n",
    "\n",
    "\n",
    "\n",
    "# Run SVM with SMOTE using 5-fold cross-validation\n",
    "print(\"\\nStarting SVM analysis with SMOTE for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds = runSVMWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "\n",
    "print(\"\\nBest results for SVM with SMOTE 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fd5f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032a4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6bbf78",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb569e16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGBoost analysis with SMOTE, PCA, and CountVectorizer for 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 126\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Run XGBoost with SMOTE, PCA, and CountVectorizer using 5-fold cross-validation\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting XGBoost analysis with SMOTE, PCA, and CountVectorizer for 5-fold cross-validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m best_params_5folds, best_f1_5folds, final_mcc_5folds \u001b[38;5;241m=\u001b[39m \u001b[43mrunXGBWithSMOTE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataPoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataLabelsList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutDir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest results for XGBoost with SMOTE, PCA, and CountVectorizer 5-fold cross-validation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 59\u001b[0m, in \u001b[0;36mrunXGBWithSMOTE\u001b[1;34m(dataPoints, dataLabelsList, outDir, n_splits)\u001b[0m\n\u001b[0;32m     54\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     55\u001b[0m     pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39mskf, scoring\u001b[38;5;241m=\u001b[39mscoring, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV to the data\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataPoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataLabelsList\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Retrieve the best parameters and score from cross-validation\u001b[39;00m\n\u001b[0;32m     62\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:915\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    913\u001b[0m     score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n\u001b[1;32m--> 915\u001b[0m         train_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_params_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    920\u001b[0m     total_time \u001b[38;5;241m=\u001b[39m score_time \u001b[38;5;241m+\u001b[39m fit_time\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:971\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[0;32m    969\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[0;32m    975\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:139\u001b[0m, in \u001b[0;36m_MultimetricScorer.__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _BaseScorer):\n\u001b[1;32m--> 139\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcached_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m         score \u001b[38;5;241m=\u001b[39m scorer(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mscore)\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:371\u001b[0m, in \u001b[0;36m_Scorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_regressor(estimator) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pos_label()\n\u001b[0;32m    370\u001b[0m response_method \u001b[38;5;241m=\u001b[39m _check_response_method(estimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_method)\n\u001b[1;32m--> 371\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmethod_caller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:89\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_method \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[response_method]\n\u001b[1;32m---> 89\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_response_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     cache[response_method] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_response.py:211\u001b[0m, in \u001b[0;36m_get_response_values\u001b[1;34m(estimator, X, response_method, pos_label, return_response_method_used)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    209\u001b[0m         pos_label \u001b[38;5;241m=\u001b[39m classes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 211\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_log_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    214\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m _process_predict_proba(\n\u001b[0;32m    215\u001b[0m         y_pred\u001b[38;5;241m=\u001b[39my_pred,\n\u001b[0;32m    216\u001b[0m         target_type\u001b[38;5;241m=\u001b[39mtarget_type,\n\u001b[0;32m    217\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m    218\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m    219\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\pipeline.py:459\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 459\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1417\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1417\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1419\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    113\u001b[0m             doc \u001b[38;5;241m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m             doc \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\haha9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:242\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         )\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_word_ngrams\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "###############################################################################\n",
    "# XGBoost with SMOTE, PCA, and CountVectorizer\n",
    "\n",
    "def runXGBWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer  # Add MCC to scoring\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with CountVectorizer, PCA, SMOTE, and XGBoost\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  \n",
    "        ('pca', PCA()),                                    \n",
    "        ('smote', SMOTE(random_state=42)),                 \n",
    "        ('xgb', XGBClassifier(eval_metric='logloss', random_state=42))  \n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'pca__n_components': [150,180, 200, 220],             \n",
    "        'xgb__n_estimators': [100, 150, 200],         \n",
    "        'xgb__max_depth': [3, 5, 7, 10],                  \n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],      \n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE, PCA, and CountVectorizer: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combination\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'n_components': results['params'][idx].get('pca__n_components'),\n",
    "            'n_estimators': results['params'][idx].get('xgb__n_estimators'),\n",
    "            'max_depth': results['params'][idx].get('xgb__max_depth'),\n",
    "            'learning_rate': results['params'][idx].get('xgb__learning_rate'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],  # Add MCC to fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"xgb-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]  # Extract final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"xgb-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in the header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC in the data\n",
    "\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds with SMOTE, PCA, and CountVectorizer. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run XGBoost with SMOTE, PCA, and CountVectorizer using 5-fold cross-validation\n",
    "print(\"\\nStarting XGBoost analysis with SMOTE, PCA, and CountVectorizer for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runXGBWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost with SMOTE, PCA, and CountVectorizer 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Final MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb851b8",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd465761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Random Forest analysis with SMOTE and PCA for 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Parameters with SMOTE and PCA: {'rf__criterion': 'gini', 'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100}\n",
      "Best F1 Score from cross-validation: 0.7881461675579323\n",
      "Per-fold metrics saved to: smote-results\\rf-smote-pca-fold-results-5-folds.csv\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.8214285714285715\n",
      "Final Recall: 0.7666666666666666\n",
      "Final Accuracy: 0.9369945355191257\n",
      "Final F1 Score: 0.7881461675579323\n",
      "Final MCC: 0.7552915142498005\n",
      "Random Forest analysis completed for 5-folds with SMOTE and PCA. Results saved to: smote-results\\rf-results-5-folds.csv\n",
      "\n",
      "Best results for Random Forest with SMOTE and PCA 5-fold cross-validation:\n",
      "Best Parameters: {'rf__criterion': 'gini', 'rf__max_depth': 20, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100}\n",
      "Best F1 Score: 0.7881461675579323\n",
      "Final MCC: 0.7552915142498005\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "###############################################################################\n",
    "# Random Forest with SMOTE and PCA\n",
    "\n",
    "def runRFWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer  # Add MCC to scoring\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with PCA, SMOTE, and Random Forest\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)), \n",
    "        ('smote', SMOTE(random_state=42)),                \n",
    "        ('rf', RandomForestClassifier(random_state=42))  \n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'rf__n_estimators': [50,100,200],  \n",
    "        'rf__criterion': ['gini', 'entropy'],   \n",
    "        'rf__max_depth': [10, 20,30],  \n",
    "        'rf__min_samples_split': [5, 10],       \n",
    "        'rf__min_samples_leaf': [2, 5],        \n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE and PCA: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'n_components': results['params'][idx].get('pca__n_components'),\n",
    "            'n_estimators': results['params'][idx].get('rf__n_estimators'),\n",
    "            'max_depth': results['params'][idx].get('rf__max_depth'),\n",
    "            'min_samples_split': results['params'][idx].get('rf__min_samples_split'),\n",
    "            'min_samples_leaf': results['params'][idx].get('rf__min_samples_leaf'),\n",
    "            'criterion': results['params'][idx].get('rf__criterion'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],  # Using dynamic access to avoid KeyError\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],  # Add MCC to fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"rf-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]  # Extract final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"rf-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC in data\n",
    "\n",
    "    print(f\"Random Forest analysis completed for {n_splits}-folds with SMOTE and PCA. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Random Forest with SMOTE and PCA using 5-fold cross-validation\n",
    "print(\"\\nStarting Random Forest analysis with SMOTE and PCA for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runRFWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for Random Forest with SMOTE and PCA 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Final MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef3f6e",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6504957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4a6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Decision Tree analysis with SMOTE and PCA for 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters with SMOTE and PCA: {'dt__criterion': 'gini', 'dt__max_depth': 10, 'dt__max_features': 'sqrt', 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 5}\n",
      "Best F1 Score from cross-validation: 0.6685964912280702\n",
      "Per-fold metrics saved to: smote-results\\dt-smote-pca-fold-results-5-folds.csv\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.6663441134029371\n",
      "Final Recall: 0.7022222222222222\n",
      "Final Accuracy: 0.893825136612022\n",
      "Final F1 Score: 0.6685964912280702\n",
      "Final MCC: 0.6174703479703749\n",
      "Decision Tree analysis completed for 5-folds with SMOTE and PCA. Results saved to: smote-results\\dt-results-5-folds.csv\n",
      "\n",
      "Best results for Decision Tree with SMOTE and PCA 5-fold cross-validation:\n",
      "Best Parameters: {'dt__criterion': 'gini', 'dt__max_depth': 10, 'dt__max_features': 'sqrt', 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 5}\n",
      "Best F1 Score: 0.6685964912280702\n",
      "Final MCC: 0.6174703479703749\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def runDTWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer  # Add MCC to scoring\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with PCA, SMOTE, and Decision Tree\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
    "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
    "        ('dt', DecisionTreeClassifier(random_state=42))    # Decision Tree classifier\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'dt__criterion': ['gini', 'entropy'], \n",
    "        'dt__max_depth': [10, 20,30],  \n",
    "        'dt__min_samples_split': [5, 10], \n",
    "        'dt__min_samples_leaf': [ 2, 5],  \n",
    "        'dt__max_features': [ 'sqrt', 'log2'],  \n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring,\n",
    "        refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE and PCA: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'max_depth': results['params'][idx].get('dt__max_depth'),\n",
    "            'min_samples_split': results['params'][idx].get('dt__min_samples_split'),\n",
    "            'min_samples_leaf': results['params'][idx].get('dt__min_samples_leaf'),\n",
    "            'criterion': results['params'][idx].get('dt__criterion'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],  # Using dynamic access to avoid KeyError\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],  # Add MCC to fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"dt-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]  # Extract final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"dt-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC in data\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {n_splits}-folds with SMOTE and PCA. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Decision Tree with SMOTE and PCA using 5-fold cross-validation\n",
    "print(\"\\nStarting Decision Tree analysis with SMOTE and PCA for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runDTWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for Decision Tree with SMOTE and PCA 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Final MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b949ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file for XGBoost (5-fold-fold) does not exist: smote-results\\xgb-pca-smote-fold-results-5-folds.csv\n",
      "CSV file for Random Forest (5-fold-fold) does not exist: smote-results\\rf-pca-smote-fold-results-5-folds.csv\n",
      "CSV file for Decision Tree (5-fold-fold) does not exist: smote-results\\dt-pca-smote-fold-results-5-folds.csv\n",
      "Best results saved to: best_results_smote.csv\n",
      "\n",
      "Best Results from All SMOTE Models\n",
      "Model        Fold  Best Accuracy  Best Precision  Best Recall  Best F1 Score  Best MCC Score                                                                                                                                                                                                              Best Parameters\n",
      "  KNN 5-fold-fold       0.887104        0.667100     0.600000       0.629388        0.565681 {'svd__n_components': nan, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine', 'accuracy': 0.8871038251366121, 'precision': 0.6670995670995671, 'recall': 0.6, 'f1': 0.6293884220354808, 'mcc': 0.5656807602939421}\n",
      "  SVM 5-fold-fold       0.913716        0.831313     0.624444       0.683718        0.661069                                          {'C': 0.01, 'kernel': 'linear', 'accuracy': 0.9137158469945356, 'precision': 0.8313131313131313, 'recall': 0.6244444444444444, 'f1': 0.6837175895999426, 'mcc': 0.6610692566090756}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the best results from the CSV files of each model\n",
    "def extract_best_results(model_name, fold, csv_file):\n",
    "    \"\"\"\n",
    "    Extracts the best result from the CSV file for a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model (e.g., \"XGBoost\", \"Random Forest\", \"Decision Tree\")\n",
    "    - fold: Number of folds (e.g., 5 or 3)\n",
    "    - csv_file: The path to the CSV file containing the model's results\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the best results for the model and fold.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"CSV file for {model_name} ({fold}-fold) does not exist: {csv_file}\")\n",
    "        return None\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Get the row with the best F1 score\n",
    "    best_row = df.loc[df['f1'].idxmax()]\n",
    "\n",
    "    # Collect the best results into a dictionary\n",
    "    best_results = {\n",
    "        'Model': model_name,\n",
    "        'Fold': f\"{fold}-fold\",\n",
    "        'Best Accuracy': best_row['accuracy'],\n",
    "        'Best Precision': best_row['precision'],\n",
    "        'Best Recall': best_row['recall'],\n",
    "        'Best F1 Score': best_row['f1'],\n",
    "        'Best MCC Score': best_row['mcc'],\n",
    "\n",
    "        'Best Parameters': best_row.to_dict()  # Including all parameters\n",
    "    }\n",
    "\n",
    "    return best_results\n",
    "\n",
    "# Function to gather and print/save the best results from all SMOTE models\n",
    "def gather_best_results(models_results_dir, output_file):\n",
    "    \"\"\"\n",
    "    Gathers the best results from all SMOTE models and writes them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - models_results_dir: Directory where the model result CSV files are stored for the SMOTE models.\n",
    "    - output_file: Path to the output CSV file to store the best results.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'KNN': {'5-fold': 'knn-smote-pca-fold-results-5-folds.csv'},\n",
    "        'SVM': {'5-fold': 'svm-smote-fold-results-5-folds.csv'},\n",
    "        'XGBoost': {'5-fold': 'xgb-smote-pca-fold-results-5-foldss.csv'},\n",
    "        'Random Forest': {'5-fold': 'rf-smote-pca-fold-results-5-folds.csv'},\n",
    "        'Decision Tree': {'5-fold': 'dt-smote-pca-fold-results-5-folds.csv',}\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the best results from each model and fold\n",
    "    best_results = []\n",
    "\n",
    "    for model_name, folds in models.items():\n",
    "        for fold, csv_file in folds.items():\n",
    "            full_csv_path = os.path.join(models_results_dir, csv_file)\n",
    "            best_result = extract_best_results(model_name, fold, full_csv_path)\n",
    "            if best_result:\n",
    "                best_results.append(best_result)\n",
    "\n",
    "    # Convert the list of best results into a DataFrame\n",
    "    best_results_df = pd.DataFrame(best_results)\n",
    "\n",
    "    # Save the best results to the output CSV file\n",
    "    best_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Best results saved to: {output_file}\")\n",
    "\n",
    "    # Print the best results as a table\n",
    "    print(f\"\\nBest Results from All SMOTE Models\")\n",
    "    print(best_results_df.to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories where the model result CSV files are stored\n",
    "    results_dir = 'smote-results'\n",
    "\n",
    "    # Path to the output CSV file where best results will be stored\n",
    "    output_file = \"best_results_smote.csv\"\n",
    "\n",
    "    # Gather and save the best results\n",
    "    gather_best_results(results_dir, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf999d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
