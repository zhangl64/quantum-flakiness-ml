{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ea354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482b7176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Per-fold KNN analysis completed. Results saved to: knn_thresholds-thresholds-knn-results-per-fold.csv\n",
      "Averaged results saved to: knn_thresholds-thresholds-knn-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   n_neighbors  weights  metric  fold  threshold  accuracy  precision  \\\n",
      "0            3  uniform  cosine     1        0.1  0.750000   0.333333   \n",
      "1            3  uniform  cosine     1        0.2  0.750000   0.333333   \n",
      "2            3  uniform  cosine     1        0.3  0.750000   0.333333   \n",
      "3            3  uniform  cosine     1        0.4  0.833333   0.400000   \n",
      "4            3  uniform  cosine     1        0.5  0.833333   0.400000   \n",
      "\n",
      "     recall        f1       mcc  \n",
      "0  0.666667  0.444444  0.336123  \n",
      "1  0.666667  0.444444  0.336123  \n",
      "2  0.666667  0.444444  0.336123  \n",
      "3  0.222222  0.285714  0.211100  \n",
      "4  0.222222  0.285714  0.211100  \n",
      "Averaged Results:\n",
      "   n_neighbors   weights  metric  threshold  accuracy_mean  accuracy_std  \\\n",
      "0            3  distance  cosine        0.1       0.846271      0.059096   \n",
      "1            3  distance  cosine        0.2       0.862994      0.060301   \n",
      "2            3  distance  cosine        0.3       0.886328      0.068031   \n",
      "3            3  distance  cosine        0.4       0.872938      0.036390   \n",
      "4            3  distance  cosine        0.5       0.872938      0.036390   \n",
      "\n",
      "   precision_mean  precision_std  recall_mean  recall_std   f1_mean    f1_std  \\\n",
      "0        0.507714       0.114429     0.800000    0.092962  0.617918  0.109757   \n",
      "1        0.549366       0.142259     0.800000    0.092962  0.646823  0.124473   \n",
      "2        0.635535       0.204465     0.755556    0.121716  0.680819  0.157883   \n",
      "3        0.703333       0.252323     0.333333    0.136083  0.437416  0.148168   \n",
      "4        0.736667       0.282941     0.311111    0.092962  0.427160  0.128847   \n",
      "\n",
      "   mcc_mean   mcc_std  \n",
      "0  0.552524  0.132953  \n",
      "1  0.585290  0.148913  \n",
      "2  0.623942  0.187884  \n",
      "3  0.415840  0.167484  \n",
      "4  0.415501  0.166922  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        from sklearn.random_projection import SparseRandomProjection\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Modified KNN with Threshold Adjustment\n",
    "\n",
    "def flastKNNWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize KNeighborsClassifier with current hyperparameter combination\n",
    "        knn_model = KNeighborsClassifier(\n",
    "            n_neighbors=param_dict['n_neighbors'],\n",
    "            weights=param_dict['weights'],\n",
    "            metric=param_dict['metric'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            knn_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(knn_model, \"predict_proba\"):\n",
    "                y_pred_proba = knn_model.predict_proba(X_test)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = knn_model.kneighbors(X_test)\n",
    "                weights = knn_model._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / knn_model.n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'metric': param_dict['metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-knn-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold KNN analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['n_neighbors', 'weights', 'metric', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-knn-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['cosine', 'euclidean'],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/knn_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/knn_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis with threshold adjustments\n",
    "    print(\"Starting KNN analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastKNNWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"knn_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f8ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NB analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'sigmoid'}\n",
      "Per-fold SVM analysis completed. Results saved to: NB_thresholds-thresholds-svm-results-per-fold.csv\n",
      "Averaged results saved to: NB_thresholds-thresholds-svm-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "      C  kernel  fold  threshold  accuracy  precision    recall        f1  \\\n",
      "0  0.01  linear     1        0.1  0.450000   0.200000  0.888889  0.326531   \n",
      "1  0.01  linear     1        0.2  0.866667   0.545455  0.666667  0.600000   \n",
      "2  0.01  linear     1        0.3  0.916667   0.833333  0.555556  0.666667   \n",
      "3  0.01  linear     1        0.4  0.900000   0.800000  0.444444  0.571429   \n",
      "4  0.01  linear     1        0.5  0.900000   0.800000  0.444444  0.571429   \n",
      "\n",
      "        mcc  \n",
      "0  0.198030  \n",
      "1  0.524735  \n",
      "2  0.637905  \n",
      "3  0.548860  \n",
      "4  0.548860  \n",
      "Averaged Results:\n",
      "      C  kernel  threshold  accuracy_mean  accuracy_std  precision_mean  \\\n",
      "0  0.01  linear        0.1       0.508079      0.077624        0.208601   \n",
      "1  0.01  linear        0.2       0.856045      0.039196        0.531948   \n",
      "2  0.01  linear        0.3       0.879379      0.045829        0.616667   \n",
      "3  0.01  linear        0.4       0.886158      0.032798        0.724286   \n",
      "4  0.01  linear        0.5       0.886158      0.022806        0.793333   \n",
      "\n",
      "   precision_std  recall_mean  recall_std   f1_mean    f1_std  mcc_mean  \\\n",
      "0       0.023329     0.800000    0.121716  0.329439  0.031658  0.189248   \n",
      "1       0.129199     0.600000    0.126686  0.557782  0.104294  0.478034   \n",
      "2       0.242670     0.488889    0.243432  0.531020  0.226207  0.477414   \n",
      "3       0.197536     0.377778    0.168508  0.485814  0.195405  0.465127   \n",
      "4       0.216538     0.333333    0.136083  0.457316  0.156799  0.460887   \n",
      "\n",
      "    mcc_std  \n",
      "0  0.062319  \n",
      "1  0.129329  \n",
      "2  0.245236  \n",
      "3  0.195646  \n",
      "4  0.158944  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        from sklearn.random_projection import SparseRandomProjection\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Modified SVM with Threshold Adjustment\n",
    "\n",
    "def flastNBWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "     # Initialize MultinomialNB with current hyperparameter combination\n",
    "        nb_model = MultinomialNB(\n",
    "            alpha=param_dict['alpha']\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            svm_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = svm_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'alpha': param_dict['alpha'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-nb-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold SVM analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['C', 'kernel', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-NB-result-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/NB_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/NB_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis with threshold adjustments\n",
    "    print(\"Starting NB analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastSVMWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"NB_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21e916",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d70da5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NB analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 0.01, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 10.0, 'kernel': 'sigmoid'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'linear'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'poly'}\n",
      "Training with parameters: {'C': 100.0, 'kernel': 'sigmoid'}\n",
      "Per-fold SVM analysis completed. Results saved to: NB_thresholds-thresholds-svm-results-per-fold.csv\n",
      "Averaged results saved to: NB_thresholds-thresholds-svm-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "      C  kernel  fold  threshold  accuracy  precision    recall        f1  \\\n",
      "0  0.01  linear     1        0.1  0.166667   0.152542  1.000000  0.264706   \n",
      "1  0.01  linear     1        0.2  0.866667   0.666667  0.222222  0.333333   \n",
      "2  0.01  linear     1        0.3  0.866667   1.000000  0.111111  0.200000   \n",
      "3  0.01  linear     1        0.4  0.866667   1.000000  0.111111  0.200000   \n",
      "4  0.01  linear     1        0.5  0.850000   1.000000  0.000000  0.000000   \n",
      "\n",
      "        mcc  \n",
      "0  0.054690  \n",
      "1  0.331954  \n",
      "2  0.309912  \n",
      "3  0.309912  \n",
      "4  0.000000  \n",
      "Averaged Results:\n",
      "      C  kernel  threshold  accuracy_mean  accuracy_std  precision_mean  \\\n",
      "0  0.01  linear        0.1       0.351243      0.156093        0.185429   \n",
      "1  0.01  linear        0.2       0.876102      0.033442        0.625458   \n",
      "2  0.01  linear        0.3       0.869492      0.036500        0.800000   \n",
      "3  0.01  linear        0.4       0.852712      0.025885        0.800000   \n",
      "4  0.01  linear        0.5       0.849435      0.012807        0.800000   \n",
      "\n",
      "   precision_std  recall_mean  recall_std   f1_mean    f1_std  mcc_mean  \\\n",
      "0       0.035228     0.933333    0.099381  0.307808  0.049359  0.154695   \n",
      "1       0.120180     0.466667    0.298142  0.494964  0.203159  0.458024   \n",
      "2       0.273861     0.200000    0.213726  0.280759  0.266368  0.294057   \n",
      "3       0.447214     0.066667    0.099381  0.112727  0.164844  0.134497   \n",
      "4       0.447214     0.022222    0.049690  0.040000  0.089443  0.050841   \n",
      "\n",
      "    mcc_std  \n",
      "0  0.111082  \n",
      "1  0.196873  \n",
      "2  0.264528  \n",
      "3  0.227660  \n",
      "4  0.146820  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        from sklearn.random_projection import SparseRandomProjection\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Modified SVM with Threshold Adjustment\n",
    "\n",
    "def flastNBWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "     # Initialize MultinomialNB with current hyperparameter combination\n",
    "        nb_model = MultinomialNB(\n",
    "            alpha=param_dict['alpha']\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            svm_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = svm_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'alpha': param_dict['alpha'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-nb-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold SVM analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['C', 'kernel', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-NB-result-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/NB_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/NB_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis with threshold adjustments\n",
    "    print(\"Starting NB analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastSVMWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"NB_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b8a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'cosine'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab21781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177d9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf20f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6714b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d24dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86424000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf4ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd0a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6b8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d3de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da58612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe719e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28286158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a169e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7c893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b4093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        from sklearn.random_projection import SparseRandomProjection\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Modified KNN with Threshold Adjustment\n",
    "\n",
    "def flastKNNWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize KNeighborsClassifier with current hyperparameter combination\n",
    "        knn_model = KNeighborsClassifier(\n",
    "            n_neighbors=param_dict['n_neighbors'],\n",
    "            weights=param_dict['weights'],\n",
    "            metric=param_dict['metric'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            knn_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(knn_model, \"predict_proba\"):\n",
    "                y_pred_proba = knn_model.predict_proba(X_test)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = knn_model.kneighbors(X_test)\n",
    "                weights = knn_model._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / knn_model.n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'metric': param_dict['metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-knn-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold KNN analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['n_neighbors', 'weights', 'metric', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0] \n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-knn-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['cosine', 'euclidean'],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/knn_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/knn_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis with threshold adjustments\n",
    "    print(\"Starting KNN analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastKNNWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, dim=100, eps=0.3,\n",
    "        combination_label=\"knn_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
