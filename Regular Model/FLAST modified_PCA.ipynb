{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a42537f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents (equal combination): 45\n",
      "Number of non-flaky documents (equal combination): 44\n",
      "Total number of documents (equal combination): 89\n",
      "************SAHPE od DATA: (89, 7570)\n",
      "Number of flaky documents (larger combination): 45\n",
      "Number of non-flaky documents (larger combination): 243\n",
      "Total number of documents (larger combination): 288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score,matthews_corrcoef, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Parameters setup\n",
    "flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "# Create directories\n",
    "outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "outDirLarger = \"results/larger_nonflaky/\"\n",
    "os.makedirs(outDirEqual, exist_ok=True)\n",
    "os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "os.makedirs(extractDirEqual, exist_ok=True)\n",
    "os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "# Extract and read data once for equal combination\n",
    "flakyDirEqual = os.path.join(extractDirEqual, 'flaky')\n",
    "nonFlakyDirEqual = os.path.join(extractDirEqual, 'nonFlaky')\n",
    "os.makedirs(flakyDirEqual, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirEqual, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirEqual)\n",
    "extract_zip(nonFlakyZip, nonFlakyDirEqual)\n",
    "\n",
    "dataPointsFlakyEqual = getDataPoints(flakyDirEqual)\n",
    "dataPointsNonFlakyEqual = getDataPoints(nonFlakyDirEqual)\n",
    "dataPointsEqual = dataPointsFlakyEqual + dataPointsNonFlakyEqual\n",
    "\n",
    "# Print the number of datasets for equal combination\n",
    "print(f\"Number of flaky documents (equal combination): {len(dataPointsFlakyEqual)}\")\n",
    "print(f\"Number of non-flaky documents (equal combination): {len(dataPointsNonFlakyEqual)}\")\n",
    "print(f\"Total number of documents (equal combination): {len(dataPointsEqual)}\")\n",
    "\n",
    "dataLabelsListEqual = np.array([1]*len(dataPointsFlakyEqual) + [0]*len(dataPointsNonFlakyEqual))\n",
    "\n",
    "# Vectorize data once\n",
    "Z_equal = flastVectorization(dataPointsEqual)\n",
    "\n",
    "print(\"************SAHPE od DATA:\", Z_equal.shape)\n",
    "### After the split, PCA should happen\n",
    "### MCC -> make sure way score weighted\n",
    "\n",
    "# Extract and read data once for larger non-flaky combination\n",
    "flakyDirLarger = os.path.join(extractDirLarger, 'flaky')\n",
    "nonFlakyDirLarger = os.path.join(extractDirLarger, 'nonFlaky')\n",
    "os.makedirs(flakyDirLarger, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirLarger, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirLarger)\n",
    "extract_zip(largerNonFlakyZip, nonFlakyDirLarger)\n",
    "\n",
    "dataPointsFlakyLarger = getDataPoints(flakyDirLarger)\n",
    "dataPointsNonFlakyLarger = getDataPoints(nonFlakyDirLarger)\n",
    "dataPointsLarger = dataPointsFlakyLarger + dataPointsNonFlakyLarger\n",
    "\n",
    "# Print the number of datasets for larger combination\n",
    "print(f\"Number of flaky documents (larger combination): {len(dataPointsFlakyLarger)}\")\n",
    "print(f\"Number of non-flaky documents (larger combination): {len(dataPointsNonFlakyLarger)}\")\n",
    "print(f\"Total number of documents (larger combination): {len(dataPointsLarger)}\")\n",
    "\n",
    "dataLabelsListLarger = np.array([1]*len(dataPointsFlakyLarger) + [0]*len(dataPointsNonFlakyLarger))\n",
    "\n",
    "Z_larger = flastVectorization(dataPointsLarger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304d035",
   "metadata": {},
   "source": [
    "## KNN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "540d06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting KNN analysis for flaky vs smaller non-flaky files (equal combination)...\n",
      "Data length 89\n",
      "[49, 56, 64]\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 7, 'knn__weights': 'distance', 'pca__n_components': 65}\n",
      "Best F1 Score: 0.6666666666666666\n",
      "KNN analysis completed for 5-folds. Results saved to: equal-params-knn-5-folds.csv\n",
      "\n",
      "Starting KNN analysis for flaky vs larger non-flaky files (larger combination)...\n",
      "Data length 288\n",
      "[161, 184, 207]\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'pca__n_components': 180}\n",
      "Best F1 Score: 0.49360639360639363\n",
      "KNN analysis completed for 5-folds. Results saved to: larger-params-knn-5-folds.csv\n",
      "\n",
      "Best results for KNN 5-fold on larger combination:\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'pca__n_components': 180}\n",
      "Best F1 Score: 0.49360639360639363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "### training performance -> \n",
    "### return performance for trainning?\n",
    "\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "def runKNN(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "    \n",
    "    # Define the pipeline with PCA and KNN\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA()),  \n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    dataset_length = Z.shape[0]\n",
    "    \n",
    "    print ('Data length',dataset_length)\n",
    "    print([math.floor(i*0.08*dataset_length) for i in range(7,10)])\n",
    "    \n",
    "    \n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    if combination_label ==\"equal\":\n",
    "         param_grid = {\n",
    "        \n",
    "            'pca__n_components': [50,60,65],  # Variance ratios\n",
    "            'knn__n_neighbors': [3, 5, 7, 9],\n",
    "            'knn__metric': ['cosine', 'euclidean'],\n",
    "            'knn__weights': ['uniform', 'distance'],\n",
    "            }\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        param_grid = {\n",
    "        \n",
    "            'pca__n_components': [180,200,220],  # Variance ratios\n",
    "            'knn__n_neighbors': [3, 5, 7, 9],\n",
    "            'knn__metric': ['cosine', 'euclidean'],\n",
    "            'knn__weights': ['uniform', 'distance'],\n",
    "            }\n",
    "    \n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  \n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer, #MCC score custom function\n",
    "    }\n",
    "    print(type(Z))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform GridSearchCV with the pipeline\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid, \n",
    "        cv=skf, \n",
    "        scoring=scoring, \n",
    "        refit='f1', \n",
    "        verbose=1, \n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit the GridSearchCV on data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "    \n",
    "    \n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "    \n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-knn-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"pca_n_components,n_neighbors,metric,weights,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)  \n",
    "            fo.write(f\"{param['pca__n_components']},{param['knn__n_neighbors']},{param['knn__metric']},{param['knn__weights']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "    \n",
    "    print(f\"KNN analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# Run KNN on equal combination\n",
    "print(\"\\nStarting KNN analysis for flaky vs smaller non-flaky files (equal combination)...\")\n",
    "best_params_5folds_knn_equal, best_score_5folds_knn_equal = runKNN(Z_equal, dataLabelsListEqual, outDirEqual, 5, \"equal\")\n",
    "\n",
    "# Display results\n",
    "#print(\"\\nBest results for KNN 5-fold on equal combination:\")\n",
    "#print(f\"Best Parameters: {best_params_5folds_knn_equal}\")\n",
    "#print(f\"Best F1 Score: {best_score_5folds_knn_equal}\")\n",
    "\n",
    "\n",
    "\n",
    "# Run KNN on larger non-flaky combination\n",
    "print(\"\\nStarting KNN analysis for flaky vs larger non-flaky files (larger combination)...\")\n",
    "best_params_5folds_knn_larger, best_score_5folds_knn_larger = runKNN(Z_larger, dataLabelsListLarger, outDirLarger, 5, \"larger\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for KNN 5-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_knn_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_knn_larger}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1a814",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410e6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4436c1",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba4000e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SVM analysis for flaky vs smaller non-flaky files (equal combination)...\n",
      "Data length 89\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Parameters: {'pca__n_components': 50, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score: 0.8274148606811146\n",
      "SVM analysis completed for 5-folds. Results saved to: equal-params-svm-5-folds.csv\n",
      "\n",
      "Starting SVM analysis for flaky vs larger non-flaky files (larger combination)...\n",
      "Data length 288\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score: 0.6914602683178535\n",
      "SVM analysis completed for 5-folds. Results saved to: larger-params-svm-5-folds.csv\n",
      "\n",
      "Best results for SVM 5-fold on larger combination:\n",
      "Best Parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score: 0.6914602683178535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "def runSVM(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "    \n",
    "    # Define the pipeline with PCA and SVM\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA()),\n",
    "        ('svm', SVC())\n",
    "    ])\n",
    "    \n",
    "    dataset_length = Z.shape[0]\n",
    "    print('Data length', dataset_length)\n",
    "    \n",
    "    if combination_label ==\"equal\":\n",
    "         param_grid = {\n",
    "        \n",
    "            'pca__n_components': [50,60,65],  # Variance ratios\n",
    "            'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "            'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types}\n",
    "         }\n",
    "             \n",
    "    else:\n",
    "        # Parameter grid for hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'pca__n_components': [180,200,220],\n",
    "            'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "            'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types\n",
    "        }\n",
    "    \n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer,  # MCC score custom function\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform GridSearchCV with the pipeline\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit='f1',\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit the GridSearchCV on data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "    \n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "    \n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-svm-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"pca_n_components,C,kernel,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)\n",
    "            fo.write(f\"{param['pca__n_components']},{param['svm__C']},{param['svm__kernel']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "    \n",
    "    print(f\"SVM analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# Run SVM on equal combination\n",
    "print(\"\\nStarting SVM analysis for flaky vs smaller non-flaky files (equal combination)...\")\n",
    "best_params_5folds_svm_equal, best_score_5folds_svm_equal = runSVM(Z_equal, dataLabelsListEqual, outDirEqual, 5, \"equal\")\n",
    "\n",
    "# Display results\n",
    "#print(\"\\nBest results for SVM 5-fold on equal combination:\")\n",
    "#print(f\"Best Parameters: {best_params_5folds_svm_equal}\")\n",
    "#print(f\"Best F1 Score: {best_score_5folds_svm_equal}\")\n",
    "\n",
    "\n",
    "# Run SVM on larger non-flaky combination\n",
    "print(\"\\nStarting SVM analysis for flaky vs larger non-flaky files (larger combination)...\")\n",
    "best_params_5folds_svm_larger, best_score_5folds_svm_larger = runSVM(Z_larger, dataLabelsListLarger, outDirLarger, 5, \"larger\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for SVM 5-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_svm_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_svm_larger}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2dd8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554dd8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8185e97",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1b2a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGBoost analysis for flaky vs larger non-flaky files (larger combination)...\n",
      "Data length 288\n",
      "[161, 184, 207]\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Best F1 Score: 0.45282051282051283\n",
      "XGBoost analysis completed for 5-folds. Results saved to: larger-params-xgb-5-folds.csv\n",
      "Data length 89\n",
      "[49, 56, 64]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [89, 288]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting XGBoost analysis for flaky vs larger non-flaky files (larger combination)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m best_params_5folds_xgb_larger, best_score_5folds_xgb_larger \u001b[38;5;241m=\u001b[39m runXGB(Z_larger, dataLabelsListLarger, outDirLarger, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarger\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m best_params_5folds_xgb_equal, best_score_5folds_xgb_equal \u001b[38;5;241m=\u001b[39m runXGB(Z_equal, dataLabelsListLarger, outDirLarger, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest results for XGBoost 5-fold on larger combination:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m, in \u001b[0;36mrunXGB\u001b[1;34m(Z, dataLabelsList, outDir, n_splits, combination_label)\u001b[0m\n\u001b[0;32m     49\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     50\u001b[0m     pipeline,\n\u001b[0;32m     51\u001b[0m     param_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV on data\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(Z, dataLabelsList)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Get the best parameters and the best score for F1\u001b[39;00m\n\u001b[0;32m     63\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:923\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    920\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\n\u001b[0;32m    921\u001b[0m scorers, refit_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scorers()\n\u001b[1;32m--> 923\u001b[0m X, y \u001b[38;5;241m=\u001b[39m indexable(X, y)\n\u001b[0;32m    924\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    926\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [89, 288]"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "def runXGB(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define the pipeline with PCA and XGBoost\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA()),\n",
    "        ('xgb', XGBClassifier(eval_metric=\"logloss\"))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Parameter grid for hyperparameter tuning \n",
    "    param_grid = {\n",
    "           # 'pca__n_components': [180,200,220],\n",
    "            'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],  # Learning rate\n",
    "            'xgb__max_depth': [3, 5, 7, 10],              # Tree depth\n",
    "            'xgb__n_estimators': [50, 100, 200],     # Number of boosting rounds\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    # Scoring metrics including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer,  # MCC score custom function\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with the pipeline\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit='f1',\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV on data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-xgb-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"pca_n_components,learning_rate,max_depth,n_estimators,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)\n",
    "            fo.write(f\"{param['xgb__learning_rate']},{param['xgb__max_depth']},{param['xgb__n_estimators']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "\n",
    "# Run XGBoost on larger non-flaky combination\n",
    "print(\"\\nStarting XGBoost analysis for flaky vs larger non-flaky files (larger combination)...\")\n",
    "best_params_5folds_xgb_larger, best_score_5folds_xgb_larger = runXGB(Z_larger, dataLabelsListLarger, outDirLarger, 5, \"larger\")\n",
    "best_params_5folds_xgb_equal, best_score_5folds_xgb_equal = runXGB(Z_equal, dataLabelsListLarger, outDirLarger, 5, \"equal\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost 5-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_xgb_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_xgb_larger}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4750a0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b205525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5d2db92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeed817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e2ede46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e62a9b",
   "metadata": {},
   "source": [
    "## get best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85548e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file for KNN (equal, 3-fold) does not exist: results/equal_flaky_nonflaky/equal-params-knn-3-folds.csv\n",
      "CSV file for KNN (larger, 3-fold) does not exist: results/larger_nonflaky/larger-params-knn-3-folds.csv\n",
      "CSV file for SVM (equal, 3-fold) does not exist: results/equal_flaky_nonflaky/equal-params-svm-3-folds.csv\n",
      "CSV file for SVM (larger, 3-fold) does not exist: results/larger_nonflaky/larger-params-svm-3-folds.csv\n",
      "CSV file for Naive Bayes (equal, 5-fold) does not exist: results/equal_flaky_nonflaky/equal-params-nb-5-folds.csv\n",
      "CSV file for Naive Bayes (larger, 5-fold) does not exist: results/larger_nonflaky/larger-params-nb-5-folds.csv\n",
      "CSV file for Naive Bayes (equal, 3-fold) does not exist: results/equal_flaky_nonflaky/equal-params-nb-3-folds.csv\n",
      "CSV file for Naive Bayes (larger, 3-fold) does not exist: results/larger_nonflaky/larger-params-nb-3-folds.csv\n",
      "CSV file for XGBoost (equal, 5-fold) does not exist: results/equal_flaky_nonflaky/equal-params-xgb-5-folds.csv\n",
      "CSV file for XGBoost (equal, 3-fold) does not exist: results/equal_flaky_nonflaky/equal-params-xgb-3-folds.csv\n",
      "CSV file for XGBoost (larger, 3-fold) does not exist: results/larger_nonflaky/larger-params-xgb-3-folds.csv\n",
      "CSV file for Random Forest (equal, 3-fold) does not exist: results/equal_flaky_nonflaky/equal-params-rf-3-folds.csv\n",
      "CSV file for Random Forest (larger, 3-fold) does not exist: results/larger_nonflaky/larger-params-rf-3-folds.csv\n",
      "CSV file for Decision Tree (equal, 3-fold) does not exist: results/equal_flaky_nonflaky/equal-params-dt-3-folds.csv\n",
      "CSV file for Decision Tree (larger, 3-fold) does not exist: results/larger_nonflaky/larger-params-dt-3-folds.csv\n",
      "Combined best results saved to: combined_best_results.csv\n",
      "\n",
      "Combined Best Results from All Models:\n",
      "                  Model   Fold  Accuracy  Precision   Recall       F1      MCC                                                                                                    Parameters\n",
      "    equal Decision Tree 5-fold  0.686275   0.656517 0.844444 0.727424 0.403994 {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "              equal KNN 5-fold  0.729412   0.876190 0.555556 0.666667 0.503770                      {'pca_n_components': 65, 'n_neighbors': 7, 'metric': 'euclidean', 'weights': 'distance'}\n",
      "    equal Random Forest 5-fold  0.820915   0.880635 0.755556 0.806199 0.657279 {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'criterion': 'entropy'}\n",
      "              equal SVM 5-fold  0.842484   0.897857 0.777778 0.827415 0.699573                                                       {'pca_n_components': 50, 'C': 0.01, 'kernel': 'linear'}\n",
      "not equal Decision Tree 5-fold  0.871567   0.601667 0.533333 0.557606 0.489453 {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "          not equal KNN 5-fold  0.888929   0.870000 0.355556 0.493606 0.505094                     {'pca_n_components': 180, 'n_neighbors': 3, 'metric': 'euclidean', 'weights': 'distance'}\n",
      "not equal Random Forest 5-fold  0.888687   1.000000 0.288889 0.418881 0.451088  {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "          not equal SVM 5-fold  0.920145   0.844524 0.622222 0.691460 0.672036                                                      {'pca_n_components': 220, 'C': 0.01, 'kernel': 'linear'}\n",
      "      not equal XGBoost 5-fold  0.933333   0.311111 0.452821 0.490596 0.311803       {'pca_n_components': 0.5, 'learning_rate': 5.0, 'max_depth': 100.0, 'n_estimators': 0.8888082274652149}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the best results from the CSV files of each model\n",
    "def extract_best_results(model_name, combination, fold, csv_file):\n",
    "    \"\"\"\n",
    "    Extracts the best result from the CSV file for a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model (e.g., \"KNN\", \"SVM\")\n",
    "    - combination: The combination of flaky and non-flaky files (e.g., \"equal\", \"larger\")\n",
    "    - fold: Number of folds (e.g., \"5-fold\" or \"3-fold\")\n",
    "    - csv_file: The path to the CSV file containing the model's results\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the best results for the model, combination, and fold.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold}) does not exist: {csv_file}\")\n",
    "        return None\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold}) is empty: {csv_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Get the row with the best F1 score\n",
    "    best_row = df.loc[df['f1'].idxmax()]\n",
    "    \n",
    "    # Extract metrics\n",
    "    accuracy = best_row['accuracy']\n",
    "    precision = best_row['precision']\n",
    "    recall = best_row['recall']\n",
    "    f1 = best_row['f1']\n",
    "    mcc = best_row.get('mcc', None)  # Get MCC if available\n",
    "    \n",
    "    # Collect parameters (exclude known metric columns)\n",
    "    metric_columns = ['accuracy', 'precision', 'recall', 'f1', 'mcc', 'preparationTime']\n",
    "    parameter_columns = [col for col in df.columns if col not in metric_columns]\n",
    "    parameters = {col: best_row[col] for col in parameter_columns}\n",
    "    \n",
    "    # Create a combined model name (e.g., 'equal KNN' or 'not equal KNN')\n",
    "    combination_label = 'not equal' if combination == 'larger' else 'equal'\n",
    "    combined_model_name = f\"{combination_label} {model_name}\"\n",
    "    \n",
    "    # Collect the best results into a dictionary\n",
    "    best_results = {\n",
    "        'Model': combined_model_name,\n",
    "        'Fold': fold,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'MCC': mcc,\n",
    "        'Parameters': parameters\n",
    "    }\n",
    "    \n",
    "    return best_results\n",
    "\n",
    "# Function to gather and print/save the best results from all models and combinations\n",
    "def gather_best_results(models_results_dirs, output_file):\n",
    "    \"\"\"\n",
    "    Gathers the best results from all models for both combinations and writes them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - models_results_dirs: Dictionary mapping combination names to their result directories.\n",
    "    - output_file: Path to the output CSV file to store the best results.\n",
    "    \"\"\"\n",
    "    # List of models and their corresponding result files for both 5-fold and 3-fold\n",
    "    models = {\n",
    "        'KNN': {'5-fold': 'params-knn-5-folds.csv', '3-fold': 'params-knn-3-folds.csv'},\n",
    "        'SVM': {'5-fold': 'params-svm-5-folds.csv', '3-fold': 'params-svm-3-folds.csv'},\n",
    "        'Naive Bayes': {'5-fold': 'params-nb-5-folds.csv', '3-fold': 'params-nb-3-folds.csv'},\n",
    "        'XGBoost': {'5-fold': 'params-xgb-5-folds.csv', '3-fold': 'params-xgb-3-folds.csv'},\n",
    "        'Random Forest': {'5-fold': 'params-rf-5-folds.csv', '3-fold': 'params-rf-3-folds.csv'},\n",
    "        'Decision Tree': {'5-fold': 'params-dt-5-folds.csv', '3-fold': 'params-dt-3-folds.csv'}\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the best results from each model, combination, and fold\n",
    "    best_results = []\n",
    "\n",
    "    # Iterate over each model, fold, and combination\n",
    "    for model_name, folds in models.items():\n",
    "        for fold_label, csv_file in folds.items():\n",
    "            for combination, results_dir in models_results_dirs.items():\n",
    "                # Adjust the filename to include the combination prefix (e.g., equal-params-xgb-5-folds.csv)\n",
    "                full_csv_file = f\"{combination}-{csv_file}\"\n",
    "                full_csv_path = os.path.join(results_dir, full_csv_file)\n",
    "                best_result = extract_best_results(model_name, combination, fold_label, full_csv_path)\n",
    "                if best_result:\n",
    "                    best_results.append(best_result)\n",
    "\n",
    "    if not best_results:\n",
    "        print(f\"No best results found.\")\n",
    "        return\n",
    "\n",
    "    # Convert the list of best results into a DataFrame\n",
    "    best_results_df = pd.DataFrame(best_results)\n",
    "    \n",
    "    # Reorder columns for clarity\n",
    "    columns = ['Model', 'Fold', 'Accuracy', 'Precision', 'Recall', 'F1', 'MCC', 'Parameters']\n",
    "    best_results_df = best_results_df[columns]\n",
    "    \n",
    "    # Add sorting helper columns\n",
    "    # Extract model name (e.g., 'KNN', 'SVM')\n",
    "    best_results_df['Model_Name'] = best_results_df['Model'].apply(lambda x: x.split(' ', 1)[1])\n",
    "    # Extract combination order (0 for 'equal', 1 for 'not equal')\n",
    "    best_results_df['Combination_Order'] = best_results_df['Model'].apply(lambda x: 0 if 'equal' in x else 1)\n",
    "    # Extract fold number (e.g., 5 or 3)\n",
    "    best_results_df['Fold_Number'] = best_results_df['Fold'].apply(lambda x: int(x.split('-')[0]))\n",
    "    \n",
    "    # Sort the DataFrame\n",
    "    best_results_df = best_results_df.sort_values(by=['Model_Name', 'Fold_Number', 'Combination_Order'])\n",
    "    \n",
    "    # Drop helper columns\n",
    "    best_results_df = best_results_df.drop(columns=['Model_Name', 'Combination_Order', 'Fold_Number'])\n",
    "    \n",
    "    # Save the best results to the output CSV file\n",
    "    best_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined best results saved to: {output_file}\")\n",
    "    \n",
    "    # Print the best results as a table\n",
    "    print(\"\\nCombined Best Results from All Models:\")\n",
    "    print(best_results_df.to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories where the model result CSV files are stored for each combination\n",
    "    models_results_dirs = {\n",
    "        'equal': 'results/equal_flaky_nonflaky/',\n",
    "        'larger': 'results/larger_nonflaky/'  # 'larger' will be labeled as 'not equal' in output\n",
    "    }\n",
    "\n",
    "    # Path to the output CSV file where best results will be stored\n",
    "    output_file = \"combined_best_results.csv\"\n",
    "\n",
    "    # Gather and save the combined best results\n",
    "    gather_best_results(models_results_dirs, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc893fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
