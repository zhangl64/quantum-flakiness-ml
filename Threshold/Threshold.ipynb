{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e03351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Decision Tree analysis completed for 5 folds. Results saved to: equal-params-dt-5-folds-Threshold.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Decision Tree analysis completed for 3 folds. Results saved to: equal-params-dt-3-folds-Threshold.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {0.1: {'f1': 0.6540756302521008, 'accuracy': 0.6596491228070175, 'precision': 0.6605844155844156, 'recall': 0.6555555555555556, 'mcc': 0.3219734115997638}, 0.2: {'f1': 0.6540756302521008, 'accuracy': 0.6596491228070175, 'precision': 0.6605844155844156, 'recall': 0.6555555555555556, 'mcc': 0.3219734115997638}, 0.30000000000000004: {'f1': 0.6540756302521008, 'accuracy': 0.6596491228070175, 'precision': 0.6605844155844156, 'recall': 0.6555555555555556, 'mcc': 0.3219734115997638}, 0.4: {'f1': 0.6540756302521008, 'accuracy': 0.6596491228070175, 'precision': 0.6605844155844156, 'recall': 0.6555555555555556, 'mcc': 0.3219734115997638}, 0.5: {'f1': 0.6540756302521008, 'accuracy': 0.6596491228070175, 'precision': 0.6605844155844156, 'recall': 0.6555555555555556, 'mcc': 0.3219734115997638}, 0.6: {'f1': 0.5543977591036414, 'accuracy': 0.6058479532163743, 'precision': 0.6117748917748917, 'recall': 0.5288888888888889, 'mcc': 0.2182164952665861}, 0.7000000000000001: {'f1': 0.5724428718855963, 'accuracy': 0.6269005847953215, 'precision': 0.6481385281385281, 'recall': 0.5288888888888889, 'mcc': 0.2611291898990602}, 0.8: {'f1': 0.5724428718855963, 'accuracy': 0.6269005847953215, 'precision': 0.6481385281385281, 'recall': 0.5288888888888889, 'mcc': 0.2611291898990602}, 0.9: {'f1': 0.5724428718855963, 'accuracy': 0.6269005847953215, 'precision': 0.6481385281385281, 'recall': 0.5288888888888889, 'mcc': 0.2611291898990602}}\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {0.1: {'f1': 0.5512188960464822, 'accuracy': 0.5745967741935484, 'precision': 0.5974358974358974, 'recall': 0.5347222222222222, 'mcc': 0.15932048308127475}, 0.2: {'f1': 0.5512188960464822, 'accuracy': 0.5745967741935484, 'precision': 0.5974358974358974, 'recall': 0.5347222222222222, 'mcc': 0.15932048308127475}, 0.30000000000000004: {'f1': 0.510989010989011, 'accuracy': 0.5638440860215054, 'precision': 0.6166666666666667, 'recall': 0.47222222222222215, 'mcc': 0.15145405649877242}, 0.4: {'f1': 0.510989010989011, 'accuracy': 0.5638440860215054, 'precision': 0.6166666666666667, 'recall': 0.47222222222222215, 'mcc': 0.15145405649877242}, 0.5: {'f1': 0.510989010989011, 'accuracy': 0.5638440860215054, 'precision': 0.6166666666666667, 'recall': 0.47222222222222215, 'mcc': 0.15145405649877242}, 0.6: {'f1': 0.4925558312655087, 'accuracy': 0.5638440860215054, 'precision': 0.6166666666666667, 'recall': 0.42777777777777776, 'mcc': 0.14805869628024204}, 0.7000000000000001: {'f1': 0.4925558312655087, 'accuracy': 0.5638440860215054, 'precision': 0.6166666666666667, 'recall': 0.42777777777777776, 'mcc': 0.14805869628024204}, 0.8: {'f1': 0.4925558312655087, 'accuracy': 0.5638440860215054, 'precision': 0.6166666666666667, 'recall': 0.42777777777777776, 'mcc': 0.14805869628024204}, 0.9: {'f1': 0.4925558312655087, 'accuracy': 0.5638440860215054, 'precision': 0.6166666666666667, 'recall': 0.42777777777777776, 'mcc': 0.14805869628024204}}\n",
      "Starting Decision Tree analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Decision Tree analysis completed for 5 folds. Results saved to: larger-params-dt-5-folds-Threshold.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Decision Tree analysis completed for 3 folds. Results saved to: larger-params-dt-3-folds-Threshold.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {0.1: {'f1': 0.41692810457516344, 'accuracy': 0.823879781420765, 'precision': 0.44642857142857145, 'recall': 0.4022222222222222, 'mcc': 0.3185982402980728}, 0.2: {'f1': 0.41692810457516344, 'accuracy': 0.823879781420765, 'precision': 0.44642857142857145, 'recall': 0.4022222222222222, 'mcc': 0.3185982402980728}, 0.30000000000000004: {'f1': 0.40632204396910276, 'accuracy': 0.823879781420765, 'precision': 0.4416666666666666, 'recall': 0.3822222222222222, 'mcc': 0.3069270097000493}, 0.4: {'f1': 0.40632204396910276, 'accuracy': 0.823879781420765, 'precision': 0.4416666666666666, 'recall': 0.3822222222222222, 'mcc': 0.3069270097000493}, 0.5: {'f1': 0.40632204396910276, 'accuracy': 0.823879781420765, 'precision': 0.4416666666666666, 'recall': 0.3822222222222222, 'mcc': 0.3069270097000493}, 0.6: {'f1': 0.41743315508021395, 'accuracy': 0.8304918032786885, 'precision': 0.47023809523809523, 'recall': 0.3822222222222222, 'mcc': 0.3245425812522293}, 0.7000000000000001: {'f1': 0.4298141074611663, 'accuracy': 0.8371584699453554, 'precision': 0.5092857142857142, 'recall': 0.3822222222222222, 'mcc': 0.3458712508087344}, 0.8: {'f1': 0.4056382832853421, 'accuracy': 0.8338251366120218, 'precision': 0.4892857142857142, 'recall': 0.36, 'mcc': 0.322268874442558}, 0.9: {'f1': 0.4056382832853421, 'accuracy': 0.8338251366120218, 'precision': 0.4892857142857142, 'recall': 0.36, 'mcc': 0.322268874442558}}\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {0.1: {'f1': 0.3717303510406959, 'accuracy': 0.8007920792079207, 'precision': 0.3650793650793651, 'recall': 0.38472222222222224, 'mcc': 0.2558674996260069}, 0.2: {'f1': 0.3717303510406959, 'accuracy': 0.8007920792079207, 'precision': 0.3650793650793651, 'recall': 0.38472222222222224, 'mcc': 0.2558674996260069}, 0.30000000000000004: {'f1': 0.3609195402298851, 'accuracy': 0.8007920792079207, 'precision': 0.3609022556390977, 'recall': 0.36388888888888893, 'mcc': 0.24406944388069549}, 0.4: {'f1': 0.3609195402298851, 'accuracy': 0.8007920792079207, 'precision': 0.3609022556390977, 'recall': 0.36388888888888893, 'mcc': 0.24406944388069549}, 0.5: {'f1': 0.3609195402298851, 'accuracy': 0.8007920792079207, 'precision': 0.3609022556390977, 'recall': 0.36388888888888893, 'mcc': 0.24406944388069549}, 0.6: {'f1': 0.32756132756132755, 'accuracy': 0.8205610561056105, 'precision': 0.40482654600301665, 'recall': 0.3, 'mcc': 0.24214780449336662}, 0.7000000000000001: {'f1': 0.32756132756132755, 'accuracy': 0.8205610561056105, 'precision': 0.40482654600301665, 'recall': 0.3, 'mcc': 0.24214780449336662}, 0.8: {'f1': 0.31290931290931295, 'accuracy': 0.8205610561056105, 'precision': 0.4024955436720143, 'recall': 0.27777777777777773, 'mcc': 0.2296755145022055}, 0.9: {'f1': 0.31290931290931295, 'accuracy': 0.8205610561056105, 'precision': 0.4024955436720143, 'recall': 0.27777777777777773, 'mcc': 0.2296755145022055}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree with Manual Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, params):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define Decision Tree model with given parameters\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        criterion=params.get('criterion', 'entropy'),\n",
    "        max_depth=params.get('max_depth', None),\n",
    "        min_samples_split=params.get('min_samples_split', 2),\n",
    "        min_samples_leaf=params.get('min_samples_leaf', 1),\n",
    "        max_features=params.get('max_features', None),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize metrics storage for each threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_threshold = {threshold: {'f1': 0, 'accuracy': 0, 'precision': 0, 'recall': 0, 'mcc': 0}\n",
    "                             for threshold in thresholds}\n",
    "    successFold = 0\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "        X_train, X_test = Z[train_index], Z[test_index]\n",
    "        y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "        if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "            print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "            continue\n",
    "\n",
    "        # Train the model\n",
    "        dt_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities on test set\n",
    "        y_pred_proba = dt_model.predict_proba(X_test)\n",
    "\n",
    "        # Calculate metrics for each threshold\n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "            # Calculate metrics for this threshold\n",
    "            f1 = f1_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "            recall = recall_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "            # Accumulate metrics for the current threshold\n",
    "            metrics_per_threshold[threshold]['f1'] += f1\n",
    "            metrics_per_threshold[threshold]['accuracy'] += accuracy\n",
    "            metrics_per_threshold[threshold]['precision'] += precision\n",
    "            metrics_per_threshold[threshold]['recall'] += recall\n",
    "            metrics_per_threshold[threshold]['mcc'] += mcc\n",
    "\n",
    "        successFold += 1\n",
    "\n",
    "    if successFold == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return params, None\n",
    "\n",
    "    # Average metrics over all folds\n",
    "    for threshold in thresholds:\n",
    "        metrics_per_threshold[threshold]['f1'] /= successFold\n",
    "        metrics_per_threshold[threshold]['accuracy'] /= successFold\n",
    "        metrics_per_threshold[threshold]['precision'] /= successFold\n",
    "        metrics_per_threshold[threshold]['recall'] /= successFold\n",
    "        metrics_per_threshold[threshold]['mcc'] /= successFold\n",
    "\n",
    "    # Save the results for each threshold\n",
    "    outFile = f\"{combination_label}-params-dt-{n_splits}-folds-Threshold.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Write the header\n",
    "        fo.write(\"threshold,accuracy,precision,recall,f1,mcc\\n\")\n",
    "        # Write the data for each threshold\n",
    "        for threshold in thresholds:\n",
    "            fo.write(f\"{threshold},{metrics_per_threshold[threshold]['accuracy']},{metrics_per_threshold[threshold]['precision']},\"\n",
    "                     f\"{metrics_per_threshold[threshold]['recall']},{metrics_per_threshold[threshold]['f1']},{metrics_per_threshold[threshold]['mcc']}\\n\")\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {successFold} folds. Results saved to: {outFile}\")\n",
    "    return params, metrics_per_threshold\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    params = {\n",
    "        \"criterion\": \"entropy\",\n",
    "        \"max_depth\": 300,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"max_features\": 'log2'\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", params=params)\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\", params=params)\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Decision Tree analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", params=params)\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\", params=params)\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6a1c0",
   "metadata": {},
   "source": [
    "Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c255f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Decision Tree analysis completed for 5 folds. Results saved to: equal-params-dt-5-folds-Threshold-allKfold.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Decision Tree analysis completed for 3 folds. Results saved to: equal-params-dt-3-folds-Threshold-allKfold.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {1: {0.1: {'f1': 0.6363636363636364, 'accuracy': 0.5789473684210527, 'precision': 0.5833333333333334, 'recall': 0.7, 'mcc': 0.14951420452417674}, 0.2: {'f1': 0.6363636363636364, 'accuracy': 0.5789473684210527, 'precision': 0.5833333333333334, 'recall': 0.7, 'mcc': 0.14951420452417674}, 0.30000000000000004: {'f1': 0.6363636363636364, 'accuracy': 0.5789473684210527, 'precision': 0.5833333333333334, 'recall': 0.7, 'mcc': 0.14951420452417674}, 0.4: {'f1': 0.7, 'accuracy': 0.6842105263157895, 'precision': 0.7, 'recall': 0.7, 'mcc': 0.36666666666666664}, 0.5: {'f1': 0.7, 'accuracy': 0.6842105263157895, 'precision': 0.7, 'recall': 0.7, 'mcc': 0.36666666666666664}, 0.6: {'f1': 0.7, 'accuracy': 0.6842105263157895, 'precision': 0.7, 'recall': 0.7, 'mcc': 0.36666666666666664}, 0.7000000000000001: {'f1': 0.7, 'accuracy': 0.6842105263157895, 'precision': 0.7, 'recall': 0.7, 'mcc': 0.36666666666666664}, 0.8: {'f1': 0.7, 'accuracy': 0.6842105263157895, 'precision': 0.7, 'recall': 0.7, 'mcc': 0.36666666666666664}, 0.9: {'f1': 0.7, 'accuracy': 0.6842105263157895, 'precision': 0.7, 'recall': 0.7, 'mcc': 0.36666666666666664}}, 2: {0.1: {'f1': 0.64, 'accuracy': 0.5263157894736842, 'precision': 0.5333333333333333, 'recall': 0.8, 'mcc': 0.027216552697590865}, 0.2: {'f1': 0.64, 'accuracy': 0.5263157894736842, 'precision': 0.5333333333333333, 'recall': 0.8, 'mcc': 0.027216552697590865}, 0.30000000000000004: {'f1': 0.6666666666666666, 'accuracy': 0.5789473684210527, 'precision': 0.5714285714285714, 'recall': 0.8, 'mcc': 0.15118578920369088}, 0.4: {'f1': 0.6666666666666666, 'accuracy': 0.5789473684210527, 'precision': 0.5714285714285714, 'recall': 0.8, 'mcc': 0.15118578920369088}, 0.5: {'f1': 0.6666666666666666, 'accuracy': 0.5789473684210527, 'precision': 0.5714285714285714, 'recall': 0.8, 'mcc': 0.15118578920369088}, 0.6: {'f1': 0.6666666666666666, 'accuracy': 0.5789473684210527, 'precision': 0.5714285714285714, 'recall': 0.8, 'mcc': 0.15118578920369088}, 0.7000000000000001: {'f1': 0.6666666666666666, 'accuracy': 0.5789473684210527, 'precision': 0.5714285714285714, 'recall': 0.8, 'mcc': 0.15118578920369088}, 0.8: {'f1': 0.6666666666666666, 'accuracy': 0.5789473684210527, 'precision': 0.5714285714285714, 'recall': 0.8, 'mcc': 0.15118578920369088}, 0.9: {'f1': 0.6666666666666666, 'accuracy': 0.5789473684210527, 'precision': 0.5714285714285714, 'recall': 0.8, 'mcc': 0.15118578920369088}}, 3: {0.1: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.2: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.30000000000000004: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.4: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.5: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.6: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.7000000000000001: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.8: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}, 0.9: {'f1': 0.42857142857142855, 'accuracy': 0.5789473684210527, 'precision': 0.6, 'recall': 0.3333333333333333, 'mcc': 0.15118578920369088}}, 4: {0.1: {'f1': 0.9473684210526315, 'accuracy': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0, 'mcc': 0.9}, 0.2: {'f1': 0.9473684210526315, 'accuracy': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0, 'mcc': 0.9}, 0.30000000000000004: {'f1': 0.9473684210526315, 'accuracy': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0, 'mcc': 0.9}, 0.4: {'f1': 0.9473684210526315, 'accuracy': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0, 'mcc': 0.9}, 0.5: {'f1': 0.9473684210526315, 'accuracy': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0, 'mcc': 0.9}, 0.6: {'f1': 0.8235294117647058, 'accuracy': 0.8421052631578947, 'precision': 0.875, 'recall': 0.7777777777777778, 'mcc': 0.6854365268376295}, 0.7000000000000001: {'f1': 0.8235294117647058, 'accuracy': 0.8421052631578947, 'precision': 0.875, 'recall': 0.7777777777777778, 'mcc': 0.6854365268376295}, 0.8: {'f1': 0.8235294117647058, 'accuracy': 0.8421052631578947, 'precision': 0.875, 'recall': 0.7777777777777778, 'mcc': 0.6854365268376295}, 0.9: {'f1': 0.8235294117647058, 'accuracy': 0.8421052631578947, 'precision': 0.875, 'recall': 0.7777777777777778, 'mcc': 0.6854365268376295}}, 5: {0.1: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.2: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.30000000000000004: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.4: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.5: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.6: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.7000000000000001: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.8: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}, 0.9: {'f1': 0.631578947368421, 'accuracy': 0.6111111111111112, 'precision': 0.6, 'recall': 0.6666666666666666, 'mcc': 0.22360679774997896}}}\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {1: {0.1: {'f1': 0.6666666666666666, 'accuracy': 0.71875, 'precision': 0.8181818181818182, 'recall': 0.5625, 'mcc': 0.4605661864718383}, 0.2: {'f1': 0.6666666666666666, 'accuracy': 0.71875, 'precision': 0.8181818181818182, 'recall': 0.5625, 'mcc': 0.4605661864718383}, 0.30000000000000004: {'f1': 0.6666666666666666, 'accuracy': 0.71875, 'precision': 0.8181818181818182, 'recall': 0.5625, 'mcc': 0.4605661864718383}, 0.4: {'f1': 0.6666666666666666, 'accuracy': 0.71875, 'precision': 0.8181818181818182, 'recall': 0.5625, 'mcc': 0.4605661864718383}, 0.5: {'f1': 0.6666666666666666, 'accuracy': 0.71875, 'precision': 0.8181818181818182, 'recall': 0.5625, 'mcc': 0.4605661864718383}, 0.6: {'f1': 0.6666666666666666, 'accuracy': 0.71875, 'precision': 0.8181818181818182, 'recall': 0.5625, 'mcc': 0.4605661864718383}, 0.7000000000000001: {'f1': 0.6666666666666666, 'accuracy': 0.71875, 'precision': 0.8181818181818182, 'recall': 0.5625, 'mcc': 0.4605661864718383}, 0.8: {'f1': 0.36363636363636365, 'accuracy': 0.5625, 'precision': 0.6666666666666666, 'recall': 0.25, 'mcc': 0.16012815380508713}, 0.9: {'f1': 0.36363636363636365, 'accuracy': 0.5625, 'precision': 0.6666666666666666, 'recall': 0.25, 'mcc': 0.16012815380508713}}, 2: {0.1: {'f1': 0.6666666666666666, 'accuracy': 0.6451612903225806, 'precision': 0.6470588235294118, 'recall': 0.6875, 'mcc': 0.28870545602072445}, 0.2: {'f1': 0.6666666666666666, 'accuracy': 0.6451612903225806, 'precision': 0.6470588235294118, 'recall': 0.6875, 'mcc': 0.28870545602072445}, 0.30000000000000004: {'f1': 0.6666666666666666, 'accuracy': 0.6451612903225806, 'precision': 0.6470588235294118, 'recall': 0.6875, 'mcc': 0.28870545602072445}, 0.4: {'f1': 0.6666666666666666, 'accuracy': 0.6451612903225806, 'precision': 0.6470588235294118, 'recall': 0.6875, 'mcc': 0.28870545602072445}, 0.5: {'f1': 0.6666666666666666, 'accuracy': 0.6451612903225806, 'precision': 0.6470588235294118, 'recall': 0.6875, 'mcc': 0.28870545602072445}, 0.6: {'f1': 0.6666666666666666, 'accuracy': 0.6451612903225806, 'precision': 0.6470588235294118, 'recall': 0.6875, 'mcc': 0.28870545602072445}, 0.7000000000000001: {'f1': 0.6451612903225806, 'accuracy': 0.6451612903225806, 'precision': 0.6666666666666666, 'recall': 0.625, 'mcc': 0.2916666666666667}, 0.8: {'f1': 0.6451612903225806, 'accuracy': 0.6451612903225806, 'precision': 0.6666666666666666, 'recall': 0.625, 'mcc': 0.2916666666666667}, 0.9: {'f1': 0.6451612903225806, 'accuracy': 0.6451612903225806, 'precision': 0.6666666666666666, 'recall': 0.625, 'mcc': 0.2916666666666667}}, 3: {0.1: {'f1': 0.75, 'accuracy': 0.7419354838709677, 'precision': 0.7058823529411765, 'recall': 0.8, 'mcc': 0.48954403412209796}, 0.2: {'f1': 0.75, 'accuracy': 0.7419354838709677, 'precision': 0.7058823529411765, 'recall': 0.8, 'mcc': 0.48954403412209796}, 0.30000000000000004: {'f1': 0.75, 'accuracy': 0.7419354838709677, 'precision': 0.7058823529411765, 'recall': 0.8, 'mcc': 0.48954403412209796}, 0.4: {'f1': 0.75, 'accuracy': 0.7419354838709677, 'precision': 0.7058823529411765, 'recall': 0.8, 'mcc': 0.48954403412209796}, 0.5: {'f1': 0.75, 'accuracy': 0.7419354838709677, 'precision': 0.7058823529411765, 'recall': 0.8, 'mcc': 0.48954403412209796}, 0.6: {'f1': 0.75, 'accuracy': 0.7419354838709677, 'precision': 0.7058823529411765, 'recall': 0.8, 'mcc': 0.48954403412209796}, 0.7000000000000001: {'f1': 0.7741935483870968, 'accuracy': 0.7741935483870968, 'precision': 0.75, 'recall': 0.8, 'mcc': 0.55}, 0.8: {'f1': 0.7741935483870968, 'accuracy': 0.7741935483870968, 'precision': 0.75, 'recall': 0.8, 'mcc': 0.55}, 0.9: {'f1': 0.7741935483870968, 'accuracy': 0.7741935483870968, 'precision': 0.75, 'recall': 0.8, 'mcc': 0.55}}}\n",
      "Starting Decision Tree analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree analysis completed for 5 folds. Results saved to: larger-params-dt-5-folds-Threshold-allKfold.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Decision Tree analysis completed for 3 folds. Results saved to: larger-params-dt-3-folds-Threshold-allKfold.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {1: {0.1: {'f1': 0.5454545454545454, 'accuracy': 0.8360655737704918, 'precision': 0.5, 'recall': 0.6, 'mcc': 0.44922235061060267}, 0.2: {'f1': 0.5454545454545454, 'accuracy': 0.8360655737704918, 'precision': 0.5, 'recall': 0.6, 'mcc': 0.44922235061060267}, 0.30000000000000004: {'f1': 0.47619047619047616, 'accuracy': 0.819672131147541, 'precision': 0.45454545454545453, 'recall': 0.5, 'mcc': 0.3681867696240635}, 0.4: {'f1': 0.47619047619047616, 'accuracy': 0.819672131147541, 'precision': 0.45454545454545453, 'recall': 0.5, 'mcc': 0.3681867696240635}, 0.5: {'f1': 0.47619047619047616, 'accuracy': 0.819672131147541, 'precision': 0.45454545454545453, 'recall': 0.5, 'mcc': 0.3681867696240635}, 0.6: {'f1': 0.23529411764705882, 'accuracy': 0.7868852459016393, 'precision': 0.2857142857142857, 'recall': 0.2, 'mcc': 0.11843289779978046}, 0.7000000000000001: {'f1': 0.23529411764705882, 'accuracy': 0.7868852459016393, 'precision': 0.2857142857142857, 'recall': 0.2, 'mcc': 0.11843289779978046}, 0.8: {'f1': 0.23529411764705882, 'accuracy': 0.7868852459016393, 'precision': 0.2857142857142857, 'recall': 0.2, 'mcc': 0.11843289779978046}, 0.9: {'f1': 0.23529411764705882, 'accuracy': 0.7868852459016393, 'precision': 0.2857142857142857, 'recall': 0.2, 'mcc': 0.11843289779978046}}, 2: {0.1: {'f1': 0.5714285714285714, 'accuracy': 0.85, 'precision': 0.5454545454545454, 'recall': 0.6, 'mcc': 0.4815713303308872}, 0.2: {'f1': 0.5714285714285714, 'accuracy': 0.85, 'precision': 0.5454545454545454, 'recall': 0.6, 'mcc': 0.4815713303308872}, 0.30000000000000004: {'f1': 0.42105263157894735, 'accuracy': 0.8166666666666667, 'precision': 0.4444444444444444, 'recall': 0.4, 'mcc': 0.31311214554257477}, 0.4: {'f1': 0.42105263157894735, 'accuracy': 0.8166666666666667, 'precision': 0.4444444444444444, 'recall': 0.4, 'mcc': 0.31311214554257477}, 0.5: {'f1': 0.42105263157894735, 'accuracy': 0.8166666666666667, 'precision': 0.4444444444444444, 'recall': 0.4, 'mcc': 0.31311214554257477}, 0.6: {'f1': 0.42105263157894735, 'accuracy': 0.8166666666666667, 'precision': 0.4444444444444444, 'recall': 0.4, 'mcc': 0.31311214554257477}, 0.7000000000000001: {'f1': 0.2857142857142857, 'accuracy': 0.8333333333333334, 'precision': 0.5, 'recall': 0.2, 'mcc': 0.23904572186687872}, 0.8: {'f1': 0.2857142857142857, 'accuracy': 0.8333333333333334, 'precision': 0.5, 'recall': 0.2, 'mcc': 0.23904572186687872}, 0.9: {'f1': 0.2857142857142857, 'accuracy': 0.8333333333333334, 'precision': 0.5, 'recall': 0.2, 'mcc': 0.23904572186687872}}, 3: {0.1: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.2: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.30000000000000004: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.4: {'f1': 0.35294117647058826, 'accuracy': 0.8166666666666667, 'precision': 0.375, 'recall': 0.3333333333333333, 'mcc': 0.24715576637149037}, 0.5: {'f1': 0.35294117647058826, 'accuracy': 0.8166666666666667, 'precision': 0.375, 'recall': 0.3333333333333333, 'mcc': 0.24715576637149037}, 0.6: {'f1': 0.35294117647058826, 'accuracy': 0.8166666666666667, 'precision': 0.375, 'recall': 0.3333333333333333, 'mcc': 0.24715576637149037}, 0.7000000000000001: {'f1': 0.35294117647058826, 'accuracy': 0.8166666666666667, 'precision': 0.375, 'recall': 0.3333333333333333, 'mcc': 0.24715576637149037}, 0.8: {'f1': 0.25, 'accuracy': 0.8, 'precision': 0.2857142857142857, 'recall': 0.2222222222222222, 'mcc': 0.13812794737179585}, 0.9: {'f1': 0.25, 'accuracy': 0.8, 'precision': 0.2857142857142857, 'recall': 0.2222222222222222, 'mcc': 0.13812794737179585}}, 4: {0.1: {'f1': 0.375, 'accuracy': 0.8333333333333334, 'precision': 0.42857142857142855, 'recall': 0.3333333333333333, 'mcc': 0.2835257867105283}, 0.2: {'f1': 0.375, 'accuracy': 0.8333333333333334, 'precision': 0.42857142857142855, 'recall': 0.3333333333333333, 'mcc': 0.2835257867105283}, 0.30000000000000004: {'f1': 0.375, 'accuracy': 0.8333333333333334, 'precision': 0.42857142857142855, 'recall': 0.3333333333333333, 'mcc': 0.2835257867105283}, 0.4: {'f1': 0.4, 'accuracy': 0.85, 'precision': 0.5, 'recall': 0.3333333333333333, 'mcc': 0.32673201960653564}, 0.5: {'f1': 0.4, 'accuracy': 0.85, 'precision': 0.5, 'recall': 0.3333333333333333, 'mcc': 0.32673201960653564}, 0.6: {'f1': 0.4, 'accuracy': 0.85, 'precision': 0.5, 'recall': 0.3333333333333333, 'mcc': 0.32673201960653564}, 0.7000000000000001: {'f1': 0.46153846153846156, 'accuracy': 0.8833333333333333, 'precision': 0.75, 'recall': 0.3333333333333333, 'mcc': 0.44908871313907184}, 0.8: {'f1': 0.46153846153846156, 'accuracy': 0.8833333333333333, 'precision': 0.75, 'recall': 0.3333333333333333, 'mcc': 0.44908871313907184}, 0.9: {'f1': 0.46153846153846156, 'accuracy': 0.8833333333333333, 'precision': 0.75, 'recall': 0.3333333333333333, 'mcc': 0.44908871313907184}}, 5: {0.1: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.2: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.30000000000000004: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.4: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.5: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.6: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.7000000000000001: {'f1': 0.5, 'accuracy': 0.8333333333333334, 'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'mcc': 0.4041060310241455}, 0.8: {'f1': 0.5263157894736842, 'accuracy': 0.85, 'precision': 0.5, 'recall': 0.5555555555555556, 'mcc': 0.4383570037596047}, 0.9: {'f1': 0.5263157894736842, 'accuracy': 0.85, 'precision': 0.5, 'recall': 0.5555555555555556, 'mcc': 0.4383570037596047}}}\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: {1: {0.1: {'f1': 0.375, 'accuracy': 0.801980198019802, 'precision': 0.375, 'recall': 0.375, 'mcc': 0.25735294117647056}, 0.2: {'f1': 0.375, 'accuracy': 0.801980198019802, 'precision': 0.375, 'recall': 0.375, 'mcc': 0.25735294117647056}, 0.30000000000000004: {'f1': 0.375, 'accuracy': 0.801980198019802, 'precision': 0.375, 'recall': 0.375, 'mcc': 0.25735294117647056}, 0.4: {'f1': 0.3333333333333333, 'accuracy': 0.801980198019802, 'precision': 0.35714285714285715, 'recall': 0.3125, 'mcc': 0.2183299028739316}, 0.5: {'f1': 0.3333333333333333, 'accuracy': 0.801980198019802, 'precision': 0.35714285714285715, 'recall': 0.3125, 'mcc': 0.2183299028739316}, 0.6: {'f1': 0.32, 'accuracy': 0.8316831683168316, 'precision': 0.4444444444444444, 'recall': 0.25, 'mcc': 0.2450127728922795}, 0.7000000000000001: {'f1': 0.3333333333333333, 'accuracy': 0.8415841584158416, 'precision': 0.5, 'recall': 0.25, 'mcc': 0.27438044751954294}, 0.8: {'f1': 0.3333333333333333, 'accuracy': 0.8415841584158416, 'precision': 0.5, 'recall': 0.25, 'mcc': 0.27438044751954294}, 0.9: {'f1': 0.3333333333333333, 'accuracy': 0.8415841584158416, 'precision': 0.5, 'recall': 0.25, 'mcc': 0.27438044751954294}}, 2: {0.1: {'f1': 0.29411764705882354, 'accuracy': 0.76, 'precision': 0.2777777777777778, 'recall': 0.3125, 'mcc': 0.15051959733051185}, 0.2: {'f1': 0.29411764705882354, 'accuracy': 0.76, 'precision': 0.2777777777777778, 'recall': 0.3125, 'mcc': 0.15051959733051185}, 0.30000000000000004: {'f1': 0.30303030303030304, 'accuracy': 0.77, 'precision': 0.29411764705882354, 'recall': 0.3125, 'mcc': 0.1655662093760438}, 0.4: {'f1': 0.30303030303030304, 'accuracy': 0.77, 'precision': 0.29411764705882354, 'recall': 0.3125, 'mcc': 0.1655662093760438}, 0.5: {'f1': 0.30303030303030304, 'accuracy': 0.77, 'precision': 0.29411764705882354, 'recall': 0.3125, 'mcc': 0.1655662093760438}, 0.6: {'f1': 0.27586206896551724, 'accuracy': 0.79, 'precision': 0.3076923076923077, 'recall': 0.25, 'mcc': 0.15572928580876177}, 0.7000000000000001: {'f1': 0.27586206896551724, 'accuracy': 0.79, 'precision': 0.3076923076923077, 'recall': 0.25, 'mcc': 0.15572928580876177}, 0.8: {'f1': 0.2857142857142857, 'accuracy': 0.8, 'precision': 0.3333333333333333, 'recall': 0.25, 'mcc': 0.17459497553883238}, 0.9: {'f1': 0.2857142857142857, 'accuracy': 0.8, 'precision': 0.3333333333333333, 'recall': 0.25, 'mcc': 0.17459497553883238}}, 3: {0.1: {'f1': 0.3076923076923077, 'accuracy': 0.82, 'precision': 0.36363636363636365, 'recall': 0.26666666666666666, 'mcc': 0.21033978418556948}, 0.2: {'f1': 0.3076923076923077, 'accuracy': 0.82, 'precision': 0.36363636363636365, 'recall': 0.26666666666666666, 'mcc': 0.21033978418556948}, 0.30000000000000004: {'f1': 0.3076923076923077, 'accuracy': 0.82, 'precision': 0.36363636363636365, 'recall': 0.26666666666666666, 'mcc': 0.21033978418556948}, 0.4: {'f1': 0.3076923076923077, 'accuracy': 0.82, 'precision': 0.36363636363636365, 'recall': 0.26666666666666666, 'mcc': 0.21033978418556948}, 0.5: {'f1': 0.3076923076923077, 'accuracy': 0.82, 'precision': 0.36363636363636365, 'recall': 0.26666666666666666, 'mcc': 0.21033978418556948}, 0.6: {'f1': 0.25, 'accuracy': 0.82, 'precision': 0.3333333333333333, 'recall': 0.2, 'mcc': 0.1614681617175282}, 0.7000000000000001: {'f1': 0.25, 'accuracy': 0.82, 'precision': 0.3333333333333333, 'recall': 0.2, 'mcc': 0.1614681617175282}, 0.8: {'f1': 0.25, 'accuracy': 0.82, 'precision': 0.3333333333333333, 'recall': 0.2, 'mcc': 0.1614681617175282}, 0.9: {'f1': 0.25, 'accuracy': 0.82, 'precision': 0.3333333333333333, 'recall': 0.2, 'mcc': 0.1614681617175282}}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree with Manual Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, params):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define Decision Tree model with given parameters\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        criterion=params.get('criterion', 'entropy'),\n",
    "        max_depth=params.get('max_depth', None),\n",
    "        min_samples_split=params.get('min_samples_split', 2),\n",
    "        min_samples_leaf=params.get('min_samples_leaf', 1),\n",
    "        max_features=params.get('max_features', None),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "       # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_fold = {fold+1: {threshold: {'f1': 0, 'accuracy': 0, 'precision': 0, 'recall': 0, 'mcc': 0}\n",
    "                                 for threshold in thresholds}\n",
    "                        for fold in range(n_splits)}\n",
    "    successFold = 0\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "        X_train, X_test = Z[train_index], Z[test_index]\n",
    "        y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "        if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "            print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "            continue\n",
    "\n",
    "        # Train the model\n",
    "        dt_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities on test set\n",
    "        y_pred_proba = dt_model.predict_proba(X_test)\n",
    "\n",
    "        # Calculate metrics for each threshold for this fold\n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "            # Calculate metrics for this threshold\n",
    "            f1 = f1_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "            recall = recall_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "            # Store metrics for the current threshold for this fold\n",
    "            metrics_per_fold[fold+1][threshold]['f1'] = f1\n",
    "            metrics_per_fold[fold+1][threshold]['accuracy'] = accuracy\n",
    "            metrics_per_fold[fold+1][threshold]['precision'] = precision\n",
    "            metrics_per_fold[fold+1][threshold]['recall'] = recall\n",
    "            metrics_per_fold[fold+1][threshold]['mcc'] = mcc\n",
    "\n",
    "        successFold += 1\n",
    "\n",
    "    if successFold == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return params, None\n",
    "\n",
    "    # Save the results for each threshold and fold\n",
    "    outFile = f\"{combination_label}-params-dt-{n_splits}-folds-Threshold-allKfold.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Write the header\n",
    "        fo.write(\"fold,threshold,accuracy,precision,recall,f1,mcc\\n\")\n",
    "        # Write the data for each fold and each threshold\n",
    "        for fold in range(1, successFold + 1):\n",
    "            for threshold in thresholds:\n",
    "                fo.write(f\"{fold},{threshold},{metrics_per_fold[fold][threshold]['accuracy']},{metrics_per_fold[fold][threshold]['precision']},\"\n",
    "                         f\"{metrics_per_fold[fold][threshold]['recall']},{metrics_per_fold[fold][threshold]['f1']},{metrics_per_fold[fold][threshold]['mcc']}\\n\")\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {successFold} folds. Results saved to: {outFile}\")\n",
    "    return params, metrics_per_fold\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    params = {\n",
    "        \"criterion\": \"entropy\",\n",
    "        \"max_depth\": 300,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"max_features\" : 'log2'\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", params=params)\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\", params=params)\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Decision Tree analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", params=params)\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\", params=params)\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157d704",
   "metadata": {},
   "source": [
    "Decition Tree - all Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6318a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Decision Tree analysis completed. Results saved to: equal-params-dt-5-folds-Threshold-allKfold-wogridsearch.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "      criterion  max_depth  min_samples_split  min_samples_leaf max_features  \\\n",
      "0          gini        NaN                  2                 1         None   \n",
      "1          gini        NaN                  2                 1         None   \n",
      "2          gini        NaN                  2                 1         None   \n",
      "3          gini        NaN                  2                 1         None   \n",
      "4          gini        NaN                  2                 1         None   \n",
      "...         ...        ...                ...               ...          ...   \n",
      "22675   entropy      500.0                 10                10         log2   \n",
      "22676   entropy      500.0                 10                10         log2   \n",
      "22677   entropy      500.0                 10                10         log2   \n",
      "22678   entropy      500.0                 10                10         log2   \n",
      "22679   entropy      500.0                 10                10         log2   \n",
      "\n",
      "       fold  threshold  accuracy  precision    recall        f1       mcc  \n",
      "0         1        0.1  0.473684   0.500000  0.400000  0.444444 -0.044947  \n",
      "1         1        0.2  0.473684   0.500000  0.400000  0.444444 -0.044947  \n",
      "2         1        0.3  0.473684   0.500000  0.400000  0.444444 -0.044947  \n",
      "3         1        0.4  0.473684   0.500000  0.400000  0.444444 -0.044947  \n",
      "4         1        0.5  0.473684   0.500000  0.400000  0.444444 -0.044947  \n",
      "...     ...        ...       ...        ...       ...       ...       ...  \n",
      "22675     5        0.5  0.555556   0.555556  0.555556  0.555556  0.111111  \n",
      "22676     5        0.6  0.555556   0.555556  0.555556  0.555556  0.111111  \n",
      "22677     5        0.7  0.388889   0.250000  0.111111  0.153846 -0.267261  \n",
      "22678     5        0.8  0.388889   0.250000  0.111111  0.153846 -0.267261  \n",
      "22679     5        0.9  0.388889   0.250000  0.111111  0.153846 -0.267261  \n",
      "\n",
      "[22680 rows x 12 columns]\n",
      "Starting Decision Tree analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt'}\n",
      "Training with parameters: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}\n",
      "Decision Tree analysis completed. Results saved to: larger-params-dt-5-folds-Threshold-allKfold-wogridsearch.csv\n",
      "Best results for 5-fold on larger combination:\n",
      "      criterion  max_depth  min_samples_split  min_samples_leaf max_features  \\\n",
      "0          gini        NaN                  2                 1         None   \n",
      "1          gini        NaN                  2                 1         None   \n",
      "2          gini        NaN                  2                 1         None   \n",
      "3          gini        NaN                  2                 1         None   \n",
      "4          gini        NaN                  2                 1         None   \n",
      "...         ...        ...                ...               ...          ...   \n",
      "22675   entropy      500.0                 10                10         log2   \n",
      "22676   entropy      500.0                 10                10         log2   \n",
      "22677   entropy      500.0                 10                10         log2   \n",
      "22678   entropy      500.0                 10                10         log2   \n",
      "22679   entropy      500.0                 10                10         log2   \n",
      "\n",
      "       fold  threshold  accuracy  precision    recall        f1       mcc  \n",
      "0         1        0.1  0.770492   0.300000  0.300000  0.300000  0.162745  \n",
      "1         1        0.2  0.770492   0.300000  0.300000  0.300000  0.162745  \n",
      "2         1        0.3  0.770492   0.300000  0.300000  0.300000  0.162745  \n",
      "3         1        0.4  0.770492   0.300000  0.300000  0.300000  0.162745  \n",
      "4         1        0.5  0.770492   0.300000  0.300000  0.300000  0.162745  \n",
      "...     ...        ...       ...        ...       ...       ...       ...  \n",
      "22675     5        0.5  0.816667   0.333333  0.222222  0.266667  0.171145  \n",
      "22676     5        0.6  0.816667   0.333333  0.222222  0.266667  0.171145  \n",
      "22677     5        0.7  0.866667   0.666667  0.222222  0.333333  0.331954  \n",
      "22678     5        0.8  0.850000   1.000000  0.000000  0.000000  0.000000  \n",
      "22679     5        0.9  0.850000   1.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[22680 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Manual Grid Search Decision Tree with Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize DecisionTreeClassifier with current hyperparameter combination\n",
    "        dt_model = DecisionTreeClassifier(\n",
    "            criterion=param_dict['criterion'],\n",
    "            max_depth=param_dict['max_depth'],\n",
    "            min_samples_split=param_dict['min_samples_split'],\n",
    "            min_samples_leaf=param_dict['min_samples_leaf'],\n",
    "            max_features=param_dict['max_features'],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            dt_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = dt_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'criterion': param_dict['criterion'],\n",
    "                    'max_depth': param_dict['max_depth'],\n",
    "                    'min_samples_split': param_dict['min_samples_split'],\n",
    "                    'min_samples_leaf': param_dict['min_samples_leaf'],\n",
    "                    'max_features': param_dict['max_features'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return param_dict, None\n",
    "\n",
    "    # Save the results for each combination, threshold, and fold\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    outFile = f\"{combination_label}-params-dt-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Decision Tree analysis completed. Results saved to: {outFile}\")\n",
    "    return param_dict, df_results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "        'max_depth': [None, 10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 5, 10],  # Minimum number of samples required to be at a leaf node\n",
    "        'max_features': [None, 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, df_results_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(df_results_5folds_1)\n",
    "\n",
    "    # Perform Decision Tree analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, df_results_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on larger combination:\")\n",
    "    print(df_results_5folds_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00921a6d",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124ba5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Random Forest analysis completed. Results saved to: equal-params-rf-5-folds-Threshold-allKfold-wogridsearch.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "       n_estimators criterion  max_depth  min_samples_split  min_samples_leaf  \\\n",
      "0                10      gini         10                  2                 1   \n",
      "1                10      gini         10                  2                 1   \n",
      "2                10      gini         10                  2                 1   \n",
      "3                10      gini         10                  2                 1   \n",
      "4                10      gini         10                  2                 1   \n",
      "...             ...       ...        ...                ...               ...   \n",
      "10795           500   entropy        500                  5                 2   \n",
      "10796           500   entropy        500                  5                 2   \n",
      "10797           500   entropy        500                  5                 2   \n",
      "10798           500   entropy        500                  5                 2   \n",
      "10799           500   entropy        500                  5                 2   \n",
      "\n",
      "       fold  threshold  accuracy  precision    recall        f1       mcc  \n",
      "0         1        0.1  0.473684   0.500000  0.900000  0.642857 -0.223607  \n",
      "1         1        0.2  0.473684   0.500000  0.800000  0.615385 -0.121716  \n",
      "2         1        0.3  0.526316   0.545455  0.600000  0.571429  0.044947  \n",
      "3         1        0.4  0.526316   0.545455  0.600000  0.571429  0.044947  \n",
      "4         1        0.5  0.578947   0.666667  0.400000  0.500000  0.190964  \n",
      "...     ...        ...       ...        ...       ...       ...       ...  \n",
      "10795     5        0.5  0.666667   0.636364  0.777778  0.700000  0.341882  \n",
      "10796     5        0.6  0.722222   0.750000  0.666667  0.705882  0.447214  \n",
      "10797     5        0.7  0.555556   0.666667  0.222222  0.333333  0.149071  \n",
      "10798     5        0.8  0.555556   1.000000  0.111111  0.200000  0.242536  \n",
      "10799     5        0.9  0.500000   1.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[10800 rows x 12 columns]\n",
      "Starting Random Forest analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 10, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 50, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 100, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'gini'}\n",
      "Training with parameters: {'n_estimators': 300, 'max_depth': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Training with parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Manual Grid Search Random Forest with Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize RandomForestClassifier with current hyperparameter combination\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=param_dict['n_estimators'],\n",
    "            criterion=param_dict['criterion'],\n",
    "            max_depth=param_dict['max_depth'],\n",
    "            min_samples_split=param_dict['min_samples_split'],\n",
    "            min_samples_leaf=param_dict['min_samples_leaf'],\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            rf_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'n_estimators': param_dict['n_estimators'],\n",
    "                    'criterion': param_dict['criterion'],\n",
    "                    'max_depth': param_dict['max_depth'],\n",
    "                    'min_samples_split': param_dict['min_samples_split'],\n",
    "                    'min_samples_leaf': param_dict['min_samples_leaf'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return param_dict, None\n",
    "\n",
    "    # Save the results for each combination, threshold, and fold\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    outFile = f\"{combination_label}-params-rf-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Random Forest analysis completed. Results saved to: {outFile}\")\n",
    "    return param_dict, df_results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 300, 500],  # Number of trees\n",
    "        'max_depth': [10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "        'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Random Forest analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Random Forest analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, df_results_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(df_results_5folds_1)\n",
    "\n",
    "    # Perform Random Forest analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Random Forest analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, df_results_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on larger combination:\")\n",
    "    print(df_results_5folds_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69d231",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dba2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Manual Grid Search XGBoost with Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize XGBClassifier with current hyperparameter combination\n",
    "        xgb_model = XGBClassifier(\n",
    "            eta=param_dict['eta'],\n",
    "            max_depth=param_dict['max_depth'],\n",
    "            n_estimators=param_dict['n_estimators'],\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            xgb_model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = xgb_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'eta': param_dict['eta'],\n",
    "                    'max_depth': param_dict['max_depth'],\n",
    "                    'n_estimators': param_dict['n_estimators'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return param_dict, None\n",
    "\n",
    "    # Save the results for each combination, threshold, and fold\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    outFile = f\"{combination_label}-params-xgb-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"XGBoost analysis completed. Results saved to: {outFile}\")\n",
    "    return param_dict, df_results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = {\n",
    "        'eta': [0.01, 0.1, 0.3, 0.5],  # Learning rate\n",
    "        'max_depth': [3, 5, 7, 10],    # Tree depth\n",
    "        'n_estimators': [50, 100, 200, 300],  # Number of boosting rounds\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform XGBoost analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting XGBoost analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, df_results_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(df_results_5folds_1)\n",
    "\n",
    "    # Perform XGBoost analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting XGBoost analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, df_results_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on larger combination:\")\n",
    "    print(df_results_5folds_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd4d47",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Manual Grid Search KNN with Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize KNeighborsClassifier with current hyperparameter combination\n",
    "        knn_model = KNeighborsClassifier(\n",
    "            n_neighbors=param_dict['n_neighbors'],\n",
    "            weights=param_dict['weights'],\n",
    "            algorithm=param_dict['algorithm'],\n",
    "            leaf_size=param_dict['leaf_size'],\n",
    "            p=param_dict['p'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            knn_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(knn_model, \"predict_proba\"):\n",
    "                y_pred_proba = knn_model.predict_proba(X_test)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = knn_model.kneighbors(X_test)\n",
    "                weights = knn_model._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / knn_model.n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'algorithm': param_dict['algorithm'],\n",
    "                    'leaf_size': param_dict['leaf_size'],\n",
    "                    'p': param_dict['p'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return param_dict, None\n",
    "\n",
    "    # Save the results for each combination, threshold, and fold\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    outFile = f\"{combination_label}-params-knn-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"KNN analysis completed. Results saved to: {outFile}\")\n",
    "    return param_dict, df_results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9,11,15,20],           # Number of neighbors to use\n",
    "        'weights': ['uniform', 'distance'],    # Weight function used in prediction\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n",
    "        'leaf_size': [30, 50],                 # Leaf size passed to BallTree or KDTree\n",
    "        'p': [1, 2],                           # Power parameter for the Minkowski metric\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting KNN analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, df_results_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(df_results_5folds_1)\n",
    "\n",
    "    # Perform KNN analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting KNN analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, df_results_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on larger combination:\")\n",
    "    print(df_results_5folds_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc8ceb",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Manual Grid Search SVM with Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize SVM with current hyperparameter combination\n",
    "        svm_model = SVC(\n",
    "            C=param_dict['C'],\n",
    "            kernel=param_dict['kernel'],\n",
    "            probability=True,  # Enable probability estimates\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            try:\n",
    "                svm_model.fit(X_train, y_train)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to train SVM with parameters {param_dict} on fold {fold+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            try:\n",
    "                y_pred_proba = svm_model.predict_proba(X_test)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to predict probabilities with SVM on fold {fold+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'C': param_dict['C'],\n",
    "                    'kernel': param_dict['kernel'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return param_dict, None\n",
    "\n",
    "    # Save the results for each combination, threshold, and fold\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    outFile = f\"{combination_label}-params-svm-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"SVM analysis completed. Results saved to: {outFile}\")\n",
    "    return param_dict, df_results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting SVM analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, df_results_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(df_results_5folds_1)\n",
    "\n",
    "    # Perform SVM analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting SVM analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, df_results_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on larger combination:\")\n",
    "    print(df_results_5folds_2)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74136b4",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0281b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlance without adjustment\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Manual Grid Search Naive Bayes with Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize MultinomialNB with current hyperparameter combination\n",
    "        nb_model = MultinomialNB(\n",
    "            alpha=param_dict['alpha']\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            nb_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = nb_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'alpha': param_dict['alpha'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return param_dict, None\n",
    "\n",
    "    # Save the results for each combination, threshold, and fold\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    outFile = f\"{combination_label}-params-nb-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Naive Bayes analysis completed. Results saved to: {outFile}\")\n",
    "    return param_dict, df_results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],  # Additive smoothing parameter\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Naive Bayes analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Naive Bayes analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, df_results_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(df_results_5folds_1)\n",
    "\n",
    "    # Perform Naive Bayes analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Naive Bayes analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, df_results_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on larger combination:\")\n",
    "    print(df_results_5folds_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the best results from the CSV files of each model\n",
    "def extract_best_results(model_name, combination, fold_label, csv_file):\n",
    "    \"\"\"\n",
    "    Extracts the best result from the CSV file for a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model (e.g., \"KNN\", \"SVM\")\n",
    "    - combination: The combination of flaky and non-flaky files (e.g., \"equal\", \"larger\")\n",
    "    - fold_label: Number of folds (e.g., \"5-fold\" or \"3-fold\")\n",
    "    - csv_file: The path to the CSV file containing the model's results\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the best results for the model, combination, and fold.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold_label}) does not exist: {csv_file}\")\n",
    "        return None\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"CSV file for {model_name} ({combination}, {fold_label}) is empty: {csv_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Identify the metric columns\n",
    "    metric_columns = ['accuracy', 'precision', 'recall', 'f1', 'mcc']\n",
    "    # Identify parameter columns (exclude known metric columns and 'fold' and 'threshold')\n",
    "    parameter_columns = [col for col in df.columns if col not in metric_columns + ['fold', 'threshold']]\n",
    "    \n",
    "    # Group by hyperparameters, fold, and threshold, then find the row with the highest F1 score\n",
    "    idx = df.groupby(parameter_columns + ['fold', 'threshold'])['f1'].idxmax()\n",
    "    df_best = df.loc[idx]\n",
    "    \n",
    "    # Now, find the overall best result (highest F1 score)\n",
    "    best_row = df_best.loc[df_best['f1'].idxmax()]\n",
    "    \n",
    "    # Extract metrics\n",
    "    accuracy = best_row['accuracy']\n",
    "    precision = best_row['precision']\n",
    "    recall = best_row['recall']\n",
    "    f1 = best_row['f1']\n",
    "    mcc = best_row.get('mcc', None)  # Get MCC if available\n",
    "    \n",
    "    # Extract parameters\n",
    "    parameters = {col: best_row[col] for col in parameter_columns}\n",
    "    parameters['threshold'] = best_row['threshold']\n",
    "    parameters['fold'] = int(best_row['fold'])\n",
    "    \n",
    "    # Create a combined model name\n",
    "    combined_model_name = f\"{combination} {model_name}\"\n",
    "    \n",
    "    # Collect the best results into a dictionary\n",
    "    best_results = {\n",
    "        'Model': combined_model_name,\n",
    "        'Fold': fold_label,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'MCC': mcc,\n",
    "        'Parameters': parameters\n",
    "    }\n",
    "    \n",
    "    return best_results\n",
    "\n",
    "# Function to gather and print/save the best results from a single combination\n",
    "def gather_best_results_for_combination(models_results_dir, output_file, combination):\n",
    "    \"\"\"\n",
    "    Gathers the best results from all models for a specific combination (e.g., \"equal\", \"larger\") and writes them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - models_results_dir: Directory where the model result CSV files are stored for the combination.\n",
    "    - output_file: Path to the output CSV file to store the best results for the combination.\n",
    "    - combination: The combination name (e.g., \"equal\", \"larger\").\n",
    "    \"\"\"\n",
    "    # List of models and their corresponding result file patterns\n",
    "    models = {\n",
    "        'KNN': 'params-knn',\n",
    "        'SVM': 'params-svm',\n",
    "        'Naive Bayes': 'params-nb',\n",
    "        'XGBoost': 'params-xgb',\n",
    "        'Random Forest': 'params-rf',\n",
    "        'Decision Tree': 'params-dt'\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the best results from each model and fold\n",
    "    best_results = []\n",
    "\n",
    "    # Iterate over each model and its result files for both 5-fold and 3-fold\n",
    "    for model_name, base_filename in models.items():\n",
    "        for n_splits in [5, 3]:\n",
    "            # Construct the CSV filename\n",
    "            csv_file = f\"{combination}-{base_filename}-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "            full_csv_path = os.path.join(models_results_dir, csv_file)\n",
    "            fold_label = f\"{n_splits}-fold\"\n",
    "\n",
    "            best_result = extract_best_results(model_name, combination, fold_label, full_csv_path)\n",
    "            if best_result:\n",
    "                best_results.append(best_result)\n",
    "\n",
    "    if not best_results:\n",
    "        print(f\"No best results found for combination {combination}.\")\n",
    "        return\n",
    "\n",
    "    # Convert the list of best results into a DataFrame\n",
    "    best_results_df = pd.DataFrame(best_results)\n",
    "    \n",
    "    # Reorder columns for clarity\n",
    "    columns = ['Model', 'Fold', 'Accuracy', 'Precision', 'Recall', 'F1', 'MCC', 'Parameters']\n",
    "    best_results_df = best_results_df[columns]\n",
    "    \n",
    "    # Save the best results to the output CSV file\n",
    "    best_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Best results for {combination} combination saved to: {output_file}\")\n",
    "    \n",
    "    # Print the best results as a table\n",
    "    print(f\"\\nBest Results from All Models for {combination} Combination:\")\n",
    "    print(best_results_df.to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories where the model result CSV files are stored for each combination\n",
    "    equal_results_dir = 'results/equal_flaky_nonflaky/'\n",
    "    larger_results_dir = 'results/larger_nonflaky/'\n",
    "\n",
    "    # Paths to the output CSV files where best results will be stored for each combination\n",
    "    equal_output_file = \"best_results_equal_combination.csv\"\n",
    "    larger_output_file = \"best_results_larger_combination.csv\"\n",
    "\n",
    "    # Gather and save the best results for the equal combination\n",
    "    gather_best_results_for_combination(equal_results_dir, equal_output_file, \"equal\")\n",
    "\n",
    "    # Gather and save the best results for the larger combination\n",
    "    gather_best_results_for_combination(larger_results_dir, larger_output_file, \"larger\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc70675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe051a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6163e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
