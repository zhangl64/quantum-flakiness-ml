{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56520cbc",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd7adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents (equal combination): 45\n",
      "Number of non-flaky documents (equal combination): 45\n",
      "Total number of documents (equal combination): 90\n",
      "Shape of vectorized data (equal combination): (90, 7563)\n",
      "Number of flaky documents (larger combination): 45\n",
      "Number of non-flaky documents (larger combination): 254\n",
      "Total number of documents (larger combination): 299\n",
      "Shape of vectorized data (larger combination): (299, 11986)\n",
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Paths to your datasets\n",
    "flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "# You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "nonFlakyZip_equal = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "nonFlakyZip_larger = \"compressedDataset/all_nonflaky_files.zip\"     # Unbalanced dataset\n",
    "\n",
    "# Create directories\n",
    "outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "outDirLarger = \"results/larger_nonflaky/\"\n",
    "os.makedirs(outDirEqual, exist_ok=True)\n",
    "os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "os.makedirs(extractDirEqual, exist_ok=True)\n",
    "os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "# Extract and read data for equal combination\n",
    "flakyDirEqual = os.path.join(extractDirEqual, 'flaky')\n",
    "nonFlakyDirEqual = os.path.join(extractDirEqual, 'nonFlaky')\n",
    "os.makedirs(flakyDirEqual, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirEqual, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirEqual)\n",
    "extract_zip(nonFlakyZip_equal, nonFlakyDirEqual)\n",
    "\n",
    "dataPointsFlakyEqual = getDataPoints(flakyDirEqual)\n",
    "dataPointsNonFlakyEqual = getDataPoints(nonFlakyDirEqual)\n",
    "dataPointsEqual = dataPointsFlakyEqual + dataPointsNonFlakyEqual\n",
    "\n",
    "# Print the number of datasets for equal combination\n",
    "print(f\"Number of flaky documents (equal combination): {len(dataPointsFlakyEqual)}\")\n",
    "print(f\"Number of non-flaky documents (equal combination): {len(dataPointsNonFlakyEqual)}\")\n",
    "print(f\"Total number of documents (equal combination): {len(dataPointsEqual)}\")\n",
    "\n",
    "dataLabelsListEqual = np.array([1]*len(dataPointsFlakyEqual) + [0]*len(dataPointsNonFlakyEqual))\n",
    "\n",
    "# Vectorize data\n",
    "Z_equal = flastVectorization(dataPointsEqual)\n",
    "\n",
    "print(\"Shape of vectorized data (equal combination):\", Z_equal.shape)\n",
    "\n",
    "# Extract and read data for larger non-flaky combination\n",
    "flakyDirLarger = os.path.join(extractDirLarger, 'flaky')\n",
    "nonFlakyDirLarger = os.path.join(extractDirLarger, 'nonFlaky')\n",
    "os.makedirs(flakyDirLarger, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirLarger, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirLarger)\n",
    "extract_zip(nonFlakyZip_larger, nonFlakyDirLarger)\n",
    "\n",
    "dataPointsFlakyLarger = getDataPoints(flakyDirLarger)\n",
    "dataPointsNonFlakyLarger = getDataPoints(nonFlakyDirLarger)\n",
    "dataPointsLarger = dataPointsFlakyLarger + dataPointsNonFlakyLarger\n",
    "\n",
    "# Print the number of datasets for larger combination\n",
    "print(f\"Number of flaky documents (larger combination): {len(dataPointsFlakyLarger)}\")\n",
    "print(f\"Number of non-flaky documents (larger combination): {len(dataPointsNonFlakyLarger)}\")\n",
    "print(f\"Total number of documents (larger combination): {len(dataPointsLarger)}\")\n",
    "\n",
    "dataLabelsListLarger = np.array([1]*len(dataPointsFlakyLarger) + [0]*len(dataPointsNonFlakyLarger))\n",
    "\n",
    "# Vectorize data\n",
    "Z_larger = flastVectorization(dataPointsLarger)\n",
    "\n",
    "print(\"Shape of vectorized data (larger combination):\", Z_larger.shape)\n",
    "\n",
    "print(\"Data preprocessing completed.\")\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Custom scorer function for Matthews Correlation Coefficient.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f01bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NB analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'alpha': 0.1}\n",
      "Training with parameters: {'alpha': 0.5}\n",
      "Training with parameters: {'alpha': 1.0}\n",
      "Per-fold Naive Bayes analysis completed. Results saved to: nb_thresholds-thresholds-nb-results-per-fold.csv\n",
      "Averaged results saved to: nb_thresholds-thresholds-nb-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   alpha  fold  threshold  accuracy  precision    recall        f1       mcc\n",
      "0    0.1     1        0.1      0.85        0.5  0.444444  0.470588  0.384465\n",
      "1    0.1     1        0.2      0.85        0.5  0.444444  0.470588  0.384465\n",
      "2    0.1     1        0.3      0.85        0.5  0.444444  0.470588  0.384465\n",
      "3    0.1     1        0.4      0.85        0.5  0.444444  0.470588  0.384465\n",
      "4    0.1     1        0.5      0.85        0.5  0.444444  0.470588  0.384465\n",
      "Averaged Results:\n",
      "   alpha  threshold  accuracy_mean  accuracy_std  precision_mean  \\\n",
      "0    0.1        0.1       0.889492      0.055246        0.635000   \n",
      "1    0.1        0.2       0.889492      0.055246        0.635000   \n",
      "2    0.1        0.3       0.889492      0.055246        0.635000   \n",
      "3    0.1        0.4       0.892825      0.060099        0.652778   \n",
      "4    0.1        0.5       0.892825      0.060099        0.652778   \n",
      "\n",
      "   precision_std  recall_mean  recall_std   f1_mean    f1_std  mcc_mean  \\\n",
      "0       0.186748     0.644444    0.198762  0.635648  0.184755  0.573543   \n",
      "1       0.186748     0.644444    0.198762  0.635648  0.184755  0.573543   \n",
      "2       0.186748     0.644444    0.198762  0.635648  0.184755  0.573543   \n",
      "3       0.209257     0.644444    0.198762  0.645005  0.198498  0.584581   \n",
      "4       0.209257     0.644444    0.198762  0.645005  0.198498  0.584581   \n",
      "\n",
      "    mcc_std  \n",
      "0  0.215808  \n",
      "1  0.215808  \n",
      "2  0.215808  \n",
      "3  0.231991  \n",
      "4  0.231991  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Function to run Naive Bayes analysis with threshold adjustments\n",
    "\n",
    "def flastNBWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize MultinomialNB with current hyperparameter combination\n",
    "        nb_model = MultinomialNB(\n",
    "            alpha=param_dict['alpha']\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            nb_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = nb_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'alpha': param_dict['alpha'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-nb-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold Naive Bayes analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['alpha', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-nb-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 0.5, 1.0],  # Smoothing parameter\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    # Use the same dataset for both models to ensure fairness\n",
    "    # You can choose between 'reduced_nonflaky_files.zip' (balanced) or 'all_nonflaky_files.zip' (unbalanced)\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  # Unbalanced dataset\n",
    "    # nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"  # Balanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/nb_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/nb_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform Naive Bayes analysis with threshold adjustments\n",
    "    print(\"Starting NB analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastNBWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5,\n",
    "        combination_label=\"nb_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb7b95",
   "metadata": {},
   "source": [
    "## why NB has negative and I got rid of dim red\n",
    "\n",
    "Each algorithm of NB expects different types of data.\n",
    "\n",
    "GaussianNB → When you have continuous features.<br>\n",
    "\n",
    "CategoricalNB → When you have categorical data.<br>\n",
    "\n",
    "MultinomialNB → Applied to text data.<br>\n",
    "\n",
    "Dimensionality reduction techniques like SparseRandomProjection or PCA can introduce negative values, which are incompatible with MultinomialNB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f8334",
   "metadata": {},
   "source": [
    "### SVM W/O PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a82298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 299\n",
      "Training with parameters: {'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Per-fold SVM analysis completed. Results saved to: svm_thresholds-thresholds-svm-results-per-fold.csv\n",
      "Averaged results saved to: svm_thresholds-thresholds-svm-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   svm__C svm__kernel  fold  threshold  accuracy  precision    recall   f1  \\\n",
      "0    0.01      linear     1        0.1  0.800000   0.400000  0.666667  0.5   \n",
      "1    0.01      linear     1        0.2  0.866667   0.571429  0.444444  0.5   \n",
      "2    0.01      linear     1        0.3  0.900000   1.000000  0.333333  0.5   \n",
      "3    0.01      linear     1        0.4  0.900000   1.000000  0.333333  0.5   \n",
      "4    0.01      linear     1        0.5  0.900000   1.000000  0.333333  0.5   \n",
      "\n",
      "        mcc  \n",
      "0  0.404226  \n",
      "1  0.428924  \n",
      "2  0.546119  \n",
      "3  0.546119  \n",
      "4  0.546119  \n",
      "Averaged Results:\n",
      "   svm__C svm__kernel  threshold  accuracy_mean  accuracy_std  precision_mean  \\\n",
      "0    0.01      linear        0.1       0.736384      0.158328        0.405333   \n",
      "1    0.01      linear        0.2       0.906328      0.050888        0.724286   \n",
      "2    0.01      linear        0.3       0.906271      0.056238        0.815714   \n",
      "3    0.01      linear        0.4       0.906328      0.048081        0.900000   \n",
      "4    0.01      linear        0.5       0.902994      0.029871        0.920000   \n",
      "\n",
      "   precision_std  recall_mean  recall_std   f1_mean    f1_std  mcc_mean  \\\n",
      "0       0.161105     0.911111    0.144871  0.540824  0.142117  0.487069   \n",
      "1       0.197536     0.644444    0.240883  0.665977  0.184545  0.624104   \n",
      "2       0.239387     0.511111    0.289742  0.599639  0.231042  0.586970   \n",
      "3       0.223607     0.444444    0.248452  0.568235  0.212930  0.579643   \n",
      "4       0.178885     0.400000    0.149071  0.545714  0.145476  0.562251   \n",
      "\n",
      "    mcc_std  \n",
      "0  0.157532  \n",
      "1  0.213990  \n",
      "2  0.252135  \n",
      "3  0.219505  \n",
      "4  0.147897  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def flastSVMWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization \n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Define the pipeline without PCA\n",
    "        pipeline = Pipeline([\n",
    "            ('svm', SVC(\n",
    "                C=param_dict['svm__C'],\n",
    "                kernel=param_dict['svm__kernel'],\n",
    "                probability=True,  # Enable probability estimates\n",
    "                random_state=42\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model without converting to dense format\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'svm__C': param_dict['svm__C'],\n",
    "                    'svm__kernel': param_dict['svm__kernel'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-svm-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold SVM analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['svm__C', 'svm__kernel', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-svm-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning without PCA parameters\n",
    "    param_grid = {\n",
    "        'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"  \n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/svm_thresholds_woPCA/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/svm_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis with threshold adjustments\n",
    "    print(\"Starting SVM analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastSVMWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, combination_label=\"svm_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96572f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
