{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eab9b6e-a03b-4173-855a-c334a520dc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3307_0a45dea238c813ade4e9421a553f86f1bbe5d068_qiskit_pulse_reschedule.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3307_0a45dea238c813ade4e9421a553f86f1bbe5d068_test_python_pulse_test_reschedule.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3348_d3bf2374bbe5f2abdf19d97a078bb3bdcfe7a07a_qiskit_quantum_info_synthesis_one_qubit_decompose.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3348_d3bf2374bbe5f2abdf19d97a078bb3bdcfe7a07a_qiskit_transpiler_passes_ms_basis_decomposer.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_dagcircuit__dagcircuit.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_barrier.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_ccx.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_ch.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_crz.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_cswap.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_cu1.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_cu3.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_cx.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_cxbase.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_cy.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_cz.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_h.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_iden.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_rx.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_ry.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_rz.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_s.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_swap.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_t.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit_extensions_standard_u0.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__classicalregister.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__instruction.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__measure.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__quantumcircuit.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__quantumprogram.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__quantumregister.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__register.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__reset.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR338_9364c2b98cfdda74993df973b030db9bb006477d_qiskit__result.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3393_1996bc25401d5060e23dbc23f851fd980c8cdaaf_qiskit_converters_ast_to_dag.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3393_1996bc25401d5060e23dbc23f851fd980c8cdaaf_test_python_circuit_test_circuit_load_from_qasm.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3418_b9f2de9700a1f4cde0af2a78fc9949ca03de5825_qiskit_visualization_pulse_matplotlib.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3450_3bbdf32289db063577825f28b4a82d4861272a1f_qiskit_pulse_timeslots.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR3495_12bbc237b64ce395dec92a6032e95d77334cc344_qiskit_visualization_counts_visualization.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR366_01e92859d53933b5125687a4d7dfc0434f8af956_qiskit_extensions_qiskit_simulator_noise.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_doc_conf.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_examples_python_ghz.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_examples_python_qft.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_examples_python_rippleadd-async.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_examples_python_rippleadd.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_examples_python_sympy_backends.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_examples_python_teleport.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit_backends_basebackend.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit_backends_baseprovider.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit_backends_ibmq_ibmqbackend.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit_backends_ibmq_ibmqprovider.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit_backends_local_localprovider.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit_backends_local_qasm_simulator_cpp.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit_backends_local___init__.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit__compiler.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit__jobprocessor.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit__quantumjob.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit__quantumprogram.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR435_298d42d1cc7ac0ebe7f0d8105d1271e61f95e7ca_qiskit__result.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR466_f0e3e406205e4792aaa4109023aa1b7fe9838caa_qiskit__result.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR466_f0e3e406205e4792aaa4109023aa1b7fe9838caa_test_python_test_unitary_simulator_py.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR603_1d76d534ccd92ca6be10f68c443fd0dd8ab0e195_test_python_common.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR781_09af5d7566345355d52454e7f3c5e1cc7da3cb16_qiskit_backends_ibmq_ibmqbackend.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR781_09af5d7566345355d52454e7f3c5e1cc7da3cb16_qiskit_backends_ibmq_ibmqjob.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR781_09af5d7566345355d52454e7f3c5e1cc7da3cb16_test_python_test_ibmqjob_states.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR781_09af5d7566345355d52454e7f3c5e1cc7da3cb16_test_python_test_quantumprogram.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR929_ce2fcfd32f8c3b6ee663c8a357df712d95ebea52_qiskit_transpiler__parallel.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR929_ce2fcfd32f8c3b6ee663c8a357df712d95ebea52_qiskit_transpiler___init__.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR929_ce2fcfd32f8c3b6ee663c8a357df712d95ebea52_test_python_test_parallel.txt\n",
      "Attempting to open file: dataset\\project\\nonflakyMethods\\Qiskit_qiskit_PR944_739e94c352bd8b47d9b1ab0b839e1e8da54147cf_test_python_test_qasm_simulator_cpp.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.6111111111111112\n",
      "1.0 0.4444444444444444\n",
      "1.0 0.5555555555555556\n",
      "0.9 0.5\n",
      "0.875 0.3888888888888889\n",
      "1.0 0.5\n",
      "1.0 0.4444444444444444\n",
      "1.0 0.6111111111111112\n",
      "1.0 0.5\n",
      "1.0 0.4444444444444444\n",
      "1.0 0.5555555555555556\n",
      "1.0 0.5555555555555556\n",
      "0.9 0.5\n",
      "1.0 0.3888888888888889\n",
      "1.0 0.7222222222222222\n",
      "1.0 0.5555555555555556\n",
      "1.0 0.5\n",
      "1.0 0.4444444444444444\n",
      "1.0 0.3888888888888889\n",
      "1.0 0.5555555555555556\n",
      "1.0 0.5\n",
      "0.8888888888888888 0.4444444444444444\n",
      "1.0 0.3888888888888889\n",
      "0.9 0.5\n",
      "0.8888888888888888 0.4444444444444444\n",
      "1.0 0.6666666666666666\n",
      "1.0 0.3333333333333333\n",
      "1.0 0.1111111111111111\n",
      "0.9 0.5\n",
      "1.0 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "###############################################################################\n",
    "# read data from file\n",
    "\n",
    "def getDataPoints(path):\n",
    "    dataPointsList = []\n",
    "    for dataPointName in os.listdir(path):\n",
    "        if dataPointName[0] == \".\":\n",
    "            continue\n",
    "        filePath = os.path.join(path, dataPointName)\n",
    "        print(f\"Attempting to open file: {filePath}\")\n",
    "        if not os.path.exists(filePath):\n",
    "            print(f\"File does not exist: {filePath}\")\n",
    "            continue\n",
    "        with open(filePath, encoding=\"utf-8\") as fileIn:\n",
    "            dp = fileIn.read()\n",
    "        dataPointsList.append(dp)\n",
    "    return dataPointsList\n",
    "\n",
    "def getDataPointsInfo(projectBasePath, projectName):\n",
    "    # get list of tokenized test methods\n",
    "    projectPath = os.path.join(projectBasePath, projectName)\n",
    "    flakyPath = os.path.join(projectPath, \"flakyMethods\")\n",
    "    nonFlakyPath = os.path.join(projectPath, \"nonflakyMethods\")  # Updated path\n",
    "    return getDataPoints(flakyPath), getDataPoints(nonFlakyPath)\n",
    "\n",
    "# Example usage\n",
    "projectBasePath = r\"dataset\"\n",
    "projectName = \"project\"  # Replace with the actual project name\n",
    "\n",
    "flakyMethods, nonFlakyMethods = getDataPointsInfo(projectBasePath, projectName)\n",
    "\n",
    "print(\"Flaky Methods:\")\n",
    "for method in flakyMethods:\n",
    "    print(method)\n",
    "\n",
    "print(\"\\nNon-Flaky Methods:\")\n",
    "for method in nonFlakyMethods:\n",
    "    print(method)\n",
    "\n",
    "###############################################################################\n",
    "# compute effectiveness metrics\n",
    "\n",
    "def computeResults(testLabels, predictLabels):\n",
    "    warnings.filterwarnings(\"error\")  # to catch warnings, e.g., \"prec set to 0.0\"\n",
    "    try:\n",
    "        precision = precision_score(testLabels, predictLabels)\n",
    "    except:\n",
    "        precision = \"-\"\n",
    "    try:\n",
    "        recall = recall_score(testLabels, predictLabels)\n",
    "    except:\n",
    "        recall = \"-\"\n",
    "    warnings.resetwarnings()  # warnings are no more errors\n",
    "    return precision, recall\n",
    "\n",
    "###############################################################################\n",
    "# FLAST\n",
    "\n",
    "def vectorization(dataPoints, dim=0, eps=0.3):\n",
    "    countVec = CountVectorizer()\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "def classificationRandomForest(trainData, trainLabels, testData, params):\n",
    "    # training\n",
    "    t0 = time.perf_counter()\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=params.get(\"n_estimators\", 100),\n",
    "        criterion=params.get(\"criterion\", \"gini\"),\n",
    "        max_depth=params.get(\"max_depth\"),\n",
    "        min_samples_split=params.get(\"min_samples_split\", 2),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
    "        n_jobs=params.get(\"n_jobs\", -1),\n",
    "        random_state=params.get(\"random_state\", 42)\n",
    "    )\n",
    "    clf.fit(trainData, trainLabels)\n",
    "    t1 = time.perf_counter()\n",
    "    trainTime = t1 - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    predictLabels = clf.predict(testData)\n",
    "    t1 = time.perf_counter()\n",
    "    testTime = t1 - t0\n",
    "\n",
    "    return trainTime, testTime, predictLabels\n",
    "\n",
    "def randomForest(outDir, projectBasePath, projectName, kf, dim, eps, params):\n",
    "    v0 = time.perf_counter()\n",
    "    dataPointsFlaky, dataPointsNonFlaky = getDataPointsInfo(projectBasePath, projectName)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "    Z = vectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataPointsList = np.array([Z[i].toarray() for i in range(Z.shape[0])])\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    v1 = time.perf_counter()\n",
    "    vecTime = v1 - v0\n",
    "\n",
    "    # storage\n",
    "    randomForest = (dataPointsList, dataLabelsList)\n",
    "    pickleDumpRandomForest = os.path.join(outDir, \"random-forest.pickle\")\n",
    "    with open(pickleDumpRandomForest, \"wb\") as pickleFile:\n",
    "        pickle.dump(randomForest, pickleFile)\n",
    "    storage = os.path.getsize(pickleDumpRandomForest)\n",
    "    os.remove(pickleDumpRandomForest)\n",
    "\n",
    "    avgP, avgR = 0, 0\n",
    "    avgTPrep, avgTPred = 0, 0\n",
    "    avgFlakyTrain, avgNonFlakyTrain, avgFlakyTest, avgNonFlakyTest = 0, 0, 0, 0\n",
    "    successFold, precisionFold = 0, 0\n",
    "    for (trnIdx, tstIdx) in kf.split(dataPointsList, dataLabelsList):\n",
    "        trainData, testData = dataPointsList[trnIdx], dataPointsList[tstIdx]\n",
    "        trainLabels, testLabels = dataLabelsList[trnIdx], dataLabelsList[tstIdx]\n",
    "        if sum(trainLabels) == 0 or sum(testLabels) == 0:\n",
    "            print(\"Skipping fold...\")\n",
    "            print(\" Flaky Train Tests\", sum(trainLabels))\n",
    "            print(\" Flaky Test Tests\", sum(testLabels))\n",
    "            continue\n",
    "\n",
    "        successFold += 1\n",
    "        avgFlakyTrain += sum(trainLabels)\n",
    "        avgNonFlakyTrain += len(trainLabels) - sum(trainLabels)\n",
    "        avgFlakyTest += sum(testLabels)\n",
    "        avgNonFlakyTest += len(testLabels) - sum(testLabels)\n",
    "\n",
    "        # prepare the data in the right format for Random Forest\n",
    "        nSamplesTrainData, nxTrain, nyTrain = trainData.shape\n",
    "        trainData = trainData.reshape((nSamplesTrainData, nxTrain * nyTrain))\n",
    "        nSamplesTestData, nxTest, nyTest = testData.shape\n",
    "        testData = testData.reshape((nSamplesTestData, nxTest * nyTest))\n",
    "\n",
    "        trainTime, testTime, predictLabels = classificationRandomForest(trainData, trainLabels, testData, params)\n",
    "        preparationTime = (vecTime * len(trainData) / len(dataPoints)) + trainTime\n",
    "        predictionTime = (vecTime / len(dataPoints)) + (testTime / len(testData))\n",
    "        (precision, recall) = computeResults(testLabels, predictLabels)\n",
    "\n",
    "        print(precision, recall)\n",
    "        if precision != \"-\":\n",
    "            precisionFold += 1\n",
    "            avgP += precision\n",
    "        avgR += recall\n",
    "        avgTPrep += preparationTime\n",
    "        avgTPred += predictionTime\n",
    "\n",
    "    if precisionFold == 0:\n",
    "        avgP = \"-\"\n",
    "    else:\n",
    "        avgP /= precisionFold\n",
    "    avgR /= successFold\n",
    "    avgTPrep /= successFold\n",
    "    avgTPred /= successFold\n",
    "    avgFlakyTrain /= successFold\n",
    "    avgNonFlakyTrain /= successFold\n",
    "    avgFlakyTest /= successFold\n",
    "    avgNonFlakyTest /= successFold\n",
    "\n",
    "    return (avgFlakyTrain, avgNonFlakyTrain, avgFlakyTest, avgNonFlakyTest, avgP, avgR, storage, avgTPrep, avgTPred)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    projectBasePath = \"\"\n",
    "    projectList = [\n",
    "        \"project\"  # Replace with the actual project name\n",
    "    ]\n",
    "    outDir = \"results-RandomForest\"\n",
    "    outFile = \"result_Random_Forest.csv\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"dataset,flakyTrain,nonFlakyTrain,flakyTest,nonFlakyTest,precision,recall,storage,preparationTime,f1, ,predictionTime\\n\")\n",
    "\n",
    "    numSplit = 30\n",
    "    testSetSize = 0.2\n",
    "    kf = StratifiedShuffleSplit(n_splits=numSplit, test_size=testSetSize)\n",
    "\n",
    "    # FLAST\n",
    "    dim = 0  # number of dimensions (0: JL with error eps)\n",
    "    eps = 0.3  # JL eps\n",
    "    params = {\n",
    "        \"criterion\": \"entropy\"\n",
    "        \"criterion\": \"gini\",\n",
    "        \"max_depth\": 300,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"min_samples_leaf\": 1\n",
    "    }\n",
    "    for projectName in projectList:\n",
    "        print(projectName.upper(), \"FLAST\")\n",
    "        (flakyTrain, nonFlakyTrain, flakyTest, nonFlakyTest, avgP, avgR, storage, avgTPrep, avgTPred) = randomForest(outDir, projectBasePath, projectName, kf, dim, eps, params)\n",
    "        with open(os.path.join(outDir, outFile), \"a\") as fo:\n",
    "            fo.write(\"{},{},{},{},{},{},{},{},{},{}\\n\".format(projectName, flakyTrain, nonFlakyTrain, flakyTest, nonFlakyTest, avgP, avgR, storage, avgTPrep, avgTPred))\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3020bc-234a-4ef0-82be-224b4036852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Random Forest with GridSearchCV and Multiple Scoring Metrics including MCC\n",
    "\n",
    "def flastRFWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define Random Forest model\n",
    "    rf_model = RandomForestClassifier()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 300, 500],  # Number of trees\n",
    "        'max_depth': [10, 30, 50, 100, 300, 500],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "        \"criterion\": [\"gini\", \"entropy\"],  # Function to measure the quality of a split\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions including MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  \n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(rf_model, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-rf-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_estimators,max_depth,min_samples_split,min_samples_leaf,criterion,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            mcc = grid_search.cv_results_['mean_test_mcc'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['n_estimators']},{param['max_depth']},{param['min_samples_split']},{param['min_samples_leaf']},{param['criterion']},{accuracy},{precision},{recall},{f1},{mcc},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Random Forest analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Random Forest analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Random Forest analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastRFWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastRFWithGridSearchCV(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\")\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Random Forest analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Random Forest analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastRFWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastRFWithGridSearchCV(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\")\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
