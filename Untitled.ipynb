{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e03351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fold 1: Best Threshold: 0.5, Best F1 Score: 0.631578947368421\n",
      "Fold 2: Best Threshold: 0.5, Best F1 Score: 0.7777777777777778\n",
      "Fold 3: Best Threshold: 0.5, Best F1 Score: 0.7058823529411765\n",
      "Fold 4: Best Threshold: 0.5, Best F1 Score: 0.7058823529411765\n",
      "Fold 5: Best Threshold: 0.5, Best F1 Score: 0.5714285714285714\n",
      "Decision Tree analysis completed for 5 folds. Results saved to: equal-params-dt-5-folds-Threshold.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fold 1: Best Threshold: 0.5, Best F1 Score: 0.46153846153846156\n",
      "Fold 2: Best Threshold: 0.5, Best F1 Score: 0.5945945945945946\n",
      "Fold 3: Best Threshold: 0.5, Best F1 Score: 0.5806451612903226\n",
      "Decision Tree analysis completed for 3 folds. Results saved to: equal-params-dt-3-folds-Threshold.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: 0.6785100004914246\n",
      "Best results for 3-fold on equal combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: 0.5455927391411263\n",
      "Starting Decision Tree analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fold 1: Best Threshold: 0.5, Best F1 Score: 0.5217391304347826\n",
      "Fold 2: Best Threshold: 0.5, Best F1 Score: 0.3157894736842105\n",
      "Fold 3: Best Threshold: 0.5, Best F1 Score: 0.2\n",
      "Fold 4: Best Threshold: 0.5, Best F1 Score: 0.47619047619047616\n",
      "Fold 5: Best Threshold: 0.5, Best F1 Score: 0.23529411764705882\n",
      "Decision Tree analysis completed for 5 folds. Results saved to: larger-params-dt-5-folds-Threshold.csv\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Fold 1: Best Threshold: 0.5, Best F1 Score: 0.4444444444444444\n",
      "Fold 2: Best Threshold: 0.5, Best F1 Score: 0.2777777777777778\n",
      "Fold 3: Best Threshold: 0.5, Best F1 Score: 0.3225806451612903\n",
      "Decision Tree analysis completed for 3 folds. Results saved to: larger-params-dt-3-folds-Threshold.csv\n",
      "Best results for 5-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: 0.34980263959130564\n",
      "Best results for 3-fold on larger non-flaky combination:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best F1 Score: 0.3482676224611709\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree with Manual Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, params):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define Decision Tree model with given parameters\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        criterion=params.get('criterion', 'entropy'),\n",
    "        max_depth=params.get('max_depth', None),\n",
    "        min_samples_split=params.get('min_samples_split', 2),\n",
    "        min_samples_leaf=params.get('min_samples_leaf', 1),\n",
    "        max_features=params.get('max_features', None),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize metrics\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    total_f1 = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_mcc = 0.0\n",
    "    total_preparationTime = 0.0\n",
    "    best_thresholds = []\n",
    "    successFold = 0\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "        X_train, X_test = Z[train_index], Z[test_index]\n",
    "        y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "        if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "            print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "            continue\n",
    "\n",
    "        # Train the model\n",
    "        dt_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities on test set\n",
    "        y_pred_proba = dt_model.predict_proba(X_test)\n",
    "\n",
    "        # Find the best threshold for this fold\n",
    "        best_threshold = 0.5\n",
    "        '''\n",
    "        best_f1 = 0.0\n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                \n",
    "                '''\n",
    "         # Set the threshold to 0.5 and calculate F1 score\n",
    "        threshold = 0.5  # Fixed threshold\n",
    "        y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold  # Always set to 0.5\n",
    "\n",
    "\n",
    "        best_thresholds.append(best_threshold)\n",
    "        print(f\"Fold {fold+1}: Best Threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "        # Calculate other metrics using the best threshold\n",
    "        y_pred = (y_pred_proba[:, 1] >= best_threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=1)  # Recalculate F1 score here\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        preparationTime = vecTime / len(dataPoints)\n",
    "\n",
    "        total_f1 += f1  # Accumulate the recalculated F1 score\n",
    "        total_accuracy += accuracy\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_mcc += mcc\n",
    "        total_preparationTime += preparationTime\n",
    "        successFold += 1\n",
    "\n",
    "    if successFold == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return params, None\n",
    "\n",
    "    # Compute average metrics over successful folds\n",
    "    avg_f1 = total_f1 / successFold\n",
    "    avg_accuracy = total_accuracy / successFold\n",
    "    avg_precision = total_precision / successFold\n",
    "    avg_recall = total_recall / successFold\n",
    "    avg_mcc = total_mcc / successFold\n",
    "    avg_preparationTime = total_preparationTime / successFold\n",
    "    avg_threshold = np.mean(best_thresholds)\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-dt-{n_splits}-folds-Threshold.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        # Write the header\n",
    "        fo.write(\"criterion,max_depth,min_samples_split,min_samples_leaf,max_features,threshold,accuracy,precision,recall,f1,mcc,preparationTime\\n\")\n",
    "        # Write the data row\n",
    "        fo.write(f\"{params.get('criterion', 'entropy')},{params.get('max_depth', None)},{params.get('min_samples_split', 2)},\"\n",
    "                 f\"{params.get('min_samples_leaf', 1)},{params.get('max_features', None)},{avg_threshold},\"\n",
    "                 f\"{avg_accuracy},{avg_precision},{avg_recall},{avg_f1},{avg_mcc},{avg_preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {successFold} folds. Results saved to: {outFile}\")\n",
    "    return params, avg_f1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    params = {\n",
    "        \"criterion\": \"entropy\",\n",
    "        \"max_depth\": 300,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"max_features\" : 'log2'\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, best_score_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", params=params)\n",
    "    best_params_3folds_1, best_score_3folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 3, dim=100, eps=0.3, combination_label=\"equal\", params=params)\n",
    "\n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_1}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on equal combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_1}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_1}\")\n",
    "\n",
    "    # Perform Decision Tree analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Decision Tree analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, best_score_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", params=params)\n",
    "    best_params_3folds_2, best_score_3folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 3, dim=100, eps=0.3, combination_label=\"larger\", params=params)\n",
    "\n",
    "    print(\"Best results for 5-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_5folds_2}\")\n",
    "\n",
    "    print(\"Best results for 3-fold on larger non-flaky combination:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_2}\")\n",
    "    print(f\"Best F1 Score: {best_score_3folds_2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c255f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
