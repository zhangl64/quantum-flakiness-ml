{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e54f4a5-348d-4e3f-9b6f-74f4506a9992",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20573e37-1565-44bd-a94d-5c52b8d3f678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis...\n",
      "18\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best Accuracy Score: 0.7316236670106638\n",
      "KNN analysis completed for 5-folds. Results saved to: params-knn-5-folds-.csv\n",
      "Starting 5-fold analysis with unbalanced data...\n",
      "18\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best Accuracy Score: 0.5124156545209176\n",
      "KNN analysis completed for 5-folds. Results saved to: params-knn-5-folds-imbalance.csv\n",
      "Starting 3-fold analysis...\n",
      "18\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best Accuracy Score: 0.726798637718178\n",
      "KNN analysis completed for 3-folds. Results saved to: params-knn-3-folds-.csv\n",
      "Starting 3-fold analysis with unbalanced data...\n",
      "18\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best Accuracy Score: 0.5896825396825397\n",
      "KNN analysis completed for 3-folds. Results saved to: params-knn-3-folds-imbalance.csv\n",
      "Best results for 5-fold:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best f1 Score: 0.7316236670106638\n",
      "Best results for imbalanced 5-fold:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best f1 Score: 0.5124156545209176\n",
      "Best results for imbalanced 3-fold:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best f1 Score: 0.726798637718178\n",
      "Best results for 3-fold:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3}\n",
      "Best f1 Score: 0.5896825396825397\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Vectorizes the data points using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# KNN with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "def flastKNNWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits,datasetType):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print (len(nonFlakyDir))\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(flakyDir):\n",
    "        shutil.rmtree(flakyDir)\n",
    "    if os.path.exists(nonFlakyDir):\n",
    "        shutil.rmtree(nonFlakyDir)\n",
    "    \n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the KNN model\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7],  # Hyperparameter for k in KNN\n",
    "        'metric': ['cosine', 'euclidean'],  # Distance metrics\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions for precision, recall, accuracy, and F1 score\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy Score: {best_score}\")\n",
    "    \n",
    "   \n",
    "    outFile = f\"params-knn-{n_splits}-folds-{datasetType}.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_neighbors,metric,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['n_neighbors']},{param['metric']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"KNN analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Parameters setup\n",
    "    flakyZip = \"compressedDataset/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    nonFlakyUnbalance = \"compressedDataset/non-flakyMethods.zip\"\n",
    "    \n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/resut_FlastKNN\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis for 5 folds and 3 folds\n",
    "    print(\"Starting 5-fold analysis...\")\n",
    "    best_params_5folds, best_score_5folds = flastKNNWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir,5, \"\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    print (\"Starting 5-fold analysis with unbalanced data...\")\n",
    "    best_params_5folds_imbalance,best_score_5folds_imbalance = flastKNNWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 5,\"imbalance\")\n",
    "  \n",
    "    \n",
    "    print(\"Starting 3-fold analysis...\")\n",
    "    best_params_3folds, best_score_3folds = flastKNNWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, 3,\"\")\n",
    "\n",
    "    \n",
    "    print (\"Starting 3-fold analysis with unbalanced data...\")\n",
    "    best_params_3folds_imbalance,best_score_3folds_imbalance = flastKNNWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 3,\"imbalance\")\n",
    "  \n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds}\")\n",
    "\n",
    "    \n",
    "    print(\"Best results for imbalanced 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds_imbalance}\")\n",
    "    \n",
    "    print(\"Best results for imbalanced 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds}\")\n",
    "    \n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds_imbalance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26a58c-f768-4863-ba1a-411cd77b6202",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f8789a-f8d2-40c6-b5e9-e9f6a9f34129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best Accuracy Score: 0.6942522974101921\n",
      "SVM analysis completed for 5-folds. Results saved to: params-svm-5-folds-.csv\n",
      "Starting 5-fold analysis with unbalanced data...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best Accuracy Score: 0.6353538257408227\n",
      "SVM analysis completed for 5-folds. Results saved to: params-svm-5-folds-imbalance.csv\n",
      "Starting 3-fold analysis...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best Accuracy Score: 0.6977777777777777\n",
      "SVM analysis completed for 3-folds. Results saved to: params-svm-3-folds-.csv\n",
      "Starting 3-fold analysis with unbalanced data...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best Accuracy Score: 0.632716049382716\n",
      "SVM analysis completed for 3-folds. Results saved to: params-svm-3-folds-imbalance.csv\n",
      "Best results for 5-fold:\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best f1 Score: 0.6942522974101921\n",
      "Best results for imbalanced 5-fold:\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best f1 Score: 0.6353538257408227\n",
      "Best results for imbalanced 3-fold:\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best f1 Score: 0.6977777777777777\n",
      "Best results for 3-fold:\n",
      "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best f1 Score: 0.632716049382716\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Vectorizes the data points using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# SVM with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "def flastSVMWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits,datasetType):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    \n",
    "    if os.path.exists(flakyDir):\n",
    "        shutil.rmtree(flakyDir)\n",
    "    if os.path.exists(nonFlakyDir):\n",
    "        shutil.rmtree(nonFlakyDir)\n",
    "    \n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the SVM model\n",
    "    svm = SVC()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions for precision, recall, accuracy, and F1 score\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score, zero_division=0),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=0)  \n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"params-svm-{n_splits}-folds-{datasetType}.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"C,kernel,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints) \n",
    "            fo.write(f\"{param['C']},{param['kernel']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"SVM analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup\n",
    "    flakyZip = \"compressedDataset/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    nonFlakyUnbalance = \"compressedDataset/non-flakyMethods.zip\"\n",
    "\n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/results_SVM\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis for 5 folds and 3 folds\n",
    "    print(\"Starting 5-fold analysis...\")\n",
    "    best_params_5folds, best_score_5folds = flastSVMWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, 5,\"\")\n",
    "\n",
    "    \n",
    "    \n",
    "    print (\"Starting 5-fold analysis with unbalanced data...\")\n",
    "    best_params_5folds_imbalance,best_score_5folds_imbalance = flastSVMWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 5,\"imbalance\")\n",
    "  \n",
    "    \n",
    "    \n",
    "    print(\"Starting 3-fold analysis...\")\n",
    "    best_params_3folds, best_score_3folds = flastSVMWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, 3,\"\")\n",
    "    \n",
    "    print (\"Starting 3-fold analysis with unbalanced data...\")\n",
    "    best_params_3folds_imbalance,best_score_3folds_imbalance = flastSVMWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 3,\"imbalance\")\n",
    "  \n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds}\")\n",
    "\n",
    "    \n",
    "    print(\"Best results for imbalanced 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds_imbalance}\")\n",
    "    \n",
    "    print(\"Best results for imbalanced 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds}\")\n",
    "    \n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds_imbalance}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb67b0a-56b4-4182-9c98-09be0f352ae0",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbdaa8f8-efd5-407d-853a-ebdcbaba8e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Parameters: {'alpha': 10.0}\n",
      "Best Accuracy Score: 0.7685425685425685\n",
      "NBanalysis completed for 5-folds. Results saved to: params-nb-5-folds-.csv\n",
      "Starting 5-fold analysis with unbalanced data...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Parameters: {'alpha': 0.01}\n",
      "Best Accuracy Score: 0.79828001375989\n",
      "NBanalysis completed for 5-folds. Results saved to: params-nb-5-folds-imbalance.csv\n",
      "Starting 3-fold analysis...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best Parameters: {'alpha': 1.0}\n",
      "Best Accuracy Score: 0.7674382716049383\n",
      "NBanalysis completed for 3-folds. Results saved to: params-nb-3-folds-.csv\n",
      "Starting 3-fold analysis with unbalanced data...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best Parameters: {'alpha': 0.1}\n",
      "Best Accuracy Score: 0.800260671228413\n",
      "NBanalysis completed for 3-folds. Results saved to: params-nb-3-folds-imbalance.csv\n",
      "Best results for 5-fold:\n",
      "Best Parameters: {'alpha': 10.0}\n",
      "Best f1 Score: 0.7685425685425685\n",
      "Best results for imbalanced 5-fold:\n",
      "Best Parameters: {'alpha': 0.01}\n",
      "Best f1 Score: 0.79828001375989\n",
      "Best results for imbalanced 3-fold:\n",
      "Best Parameters: {'alpha': 1.0}\n",
      "Best f1 Score: 0.7674382716049383\n",
      "Best results for 3-fold:\n",
      "Best Parameters: {'alpha': 0.1}\n",
      "Best f1 Score: 0.800260671228413\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Vectorizes the data points using CountVectorizer without dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Naive Bayes with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def flastNBWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits,datasetType):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    if os.path.exists(flakyDir):\n",
    "        shutil.rmtree(flakyDir)\n",
    "    if os.path.exists(nonFlakyDir):\n",
    "        shutil.rmtree(nonFlakyDir)\n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "    \n",
    "      \n",
    "\n",
    "    \n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization without Random Projection\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0],  # Laplace smoothing parameter\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions for precision, recall, accuracy, and F1 score\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score)  \n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(nb, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"params-nb-{n_splits}-folds-{datasetType}.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"alpha,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = vecTime / len(dataPoints)  \n",
    "            fo.write(f\"{param['alpha']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "             \n",
    "             \n",
    "\n",
    "    print(f\"NBanalysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup\n",
    "    flakyZip = \"compressedDataset/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    nonFlakyUnbalance = \"compressedDataset/non-flakyMethods.zip\"\n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    print(\"Starting 5-fold analysis...\")\n",
    "    best_params_5folds, best_score_5folds = flastNBWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, 5,\"\")\n",
    "\n",
    "    \n",
    "    \n",
    "    print (\"Starting 5-fold analysis with unbalanced data...\")\n",
    "    best_params_5folds_imbalance,best_score_5folds_imbalance = flastNBWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 5,\"imbalance\")\n",
    "  \n",
    "    \n",
    "    \n",
    "    print(\"Starting 3-fold analysis...\")\n",
    "    best_params_3folds, best_score_3folds = flastNBWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, 3,\"\")\n",
    "    \n",
    "    print (\"Starting 3-fold analysis with unbalanced data...\")\n",
    "    best_params_3folds_imbalance,best_score_3folds_imbalance = flastNBWithGridSearchCV(outDir, flakyZip,nonFlakyUnbalance, extractDir, 3,\"imbalance\")\n",
    "  \n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds}\")\n",
    "\n",
    "    \n",
    "    print(\"Best results for imbalanced 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds_imbalance}\")\n",
    "    \n",
    "    print(\"Best results for imbalanced 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds}\")\n",
    "    \n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds_imbalance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b751a1-1c35-4150-97cc-7b00124c835e",
   "metadata": {},
   "source": [
    "XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "568fb140-510b-40d7-9308-d5d36a53ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis with XGBoost...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "Best Parameters: {'colsample_bytree': 1, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best Accuracy Score: 0.786437908496732\n",
      "XGBoost analysis completed for 5-folds. Results saved to: params-xgb-5-folds.csv\n",
      "Starting 5-fold analysis with unbalanced data using XGBoost...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "Best Parameters: {'colsample_bytree': 1, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best Accuracy Score: 0.5952380952380952\n",
      "XGBoost analysis completed for 5-folds. Results saved to: params-xgb-5-folds.csv\n",
      "Starting 3-fold analysis with XGBoost...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 1, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}\n",
      "Best Accuracy Score: 0.7901305683563749\n",
      "XGBoost analysis completed for 3-folds. Results saved to: params-xgb-3-folds.csv\n",
      "Starting 3-fold analysis with unbalanced data using XGBoost...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best Accuracy Score: 0.7063492063492064\n",
      "XGBoost analysis completed for 3-folds. Results saved to: params-xgb-3-folds.csv\n",
      "Best results for 5-fold:\n",
      "Best Parameters: {'colsample_bytree': 1, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best f1 Score: 0.786437908496732\n",
      "Best results for unbalanced 5-fold:\n",
      "Best Parameters: {'colsample_bytree': 1, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best f1 Score: 0.5952380952380952\n",
      "Best results for 3-fold:\n",
      "Best Parameters: {'colsample_bytree': 1, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}\n",
      "Best f1 Score: 0.7901305683563749\n",
      "Best results for unbalanced 3-fold:\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best f1 Score: 0.7063492063492064\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "from xgboost import XGBClassifier  # Import XGBoost Classifier\n",
    "\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "###############################################################################\n",
    "# read data from file\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        \n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def vectorization(dataPoints, dim=0, eps=0.3):\n",
    "    countVec = CountVectorizer(stop_words=None)  # Disable stop words filtering\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "def xgboostWithGridSearch(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps):\n",
    "    v0 = time.perf_counter()\n",
    "    \n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    if os.path.exists(flakyDir):\n",
    "        shutil.rmtree(flakyDir)\n",
    "    if os.path.exists(nonFlakyDir):\n",
    "        shutil.rmtree(nonFlakyDir)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "    \n",
    "    \n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    if len(dataPoints) > 0:\n",
    "        print(f\"Sample document: {dataPoints[0]}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = vectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataPointsList = np.array([Z[i].toarray() for i in range(Z.shape[0])])\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    classifier = XGBClassifier(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'gamma': [0, 0.1],\n",
    "        'subsample': [0.8, 1],\n",
    "        'colsample_bytree': [0.8, 1],\n",
    "        'min_child_weight': [1, 3]\n",
    "    }\n",
    "    \n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score, zero_division=0),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(dataPointsList.reshape(len(dataPointsList), -1), dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"params-xgb-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_estimators,max_depth,learning_rate,gamma,subsample,colsample_bytree,min_child_weight,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = vecTime / dataPointsList.shape[0]  # Use shape[0] instead of len()\n",
    "            fo.write(f\"{param.get('n_estimators', '-')},{param.get('max_depth', '-')},{param.get('learning_rate', '-')},{param.get('gamma', '-')},{param.get('subsample', '-')},{param.get('colsample_bytree', '-')},{param.get('min_child_weight', '-')},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup\n",
    "    flakyZip = \"compressedDataset/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    nonFlakyUnbalance = \"compressedDataset/non-flakyMethods.zip\"  # Unbalanced dataset\n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/results_XGBoost\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform XGBoost analysis for 5 folds and 3 folds\n",
    "    dim = 100  # Example value for JL dimensionality reduction\n",
    "    eps = 0.3  # JL epsilon\n",
    "\n",
    "    print(\"Starting 5-fold analysis with XGBoost...\")\n",
    "    best_params_5folds, best_score_5folds = xgboostWithGridSearch(outDir, flakyZip, nonFlakyZip, extractDir, 5, dim, eps)\n",
    "\n",
    "    print(\"Starting 5-fold analysis with unbalanced data using XGBoost...\")\n",
    "    best_params_5folds_imbalance, best_score_5folds_imbalance = xgboostWithGridSearch(outDir, flakyZip, nonFlakyUnbalance, extractDir, 5, dim, eps)\n",
    "\n",
    "    print(\"Starting 3-fold analysis with XGBoost...\")\n",
    "    best_params_3folds, best_score_3folds = xgboostWithGridSearch(outDir, flakyZip, nonFlakyZip, extractDir, 3, dim, eps)\n",
    "\n",
    "    print(\"Starting 3-fold analysis with unbalanced data using XGBoost...\")\n",
    "    best_params_3folds_imbalance, best_score_3folds_imbalance = xgboostWithGridSearch(outDir, flakyZip, nonFlakyUnbalance, extractDir, 3, dim, eps)\n",
    "\n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds}\")\n",
    "\n",
    "    print(\"Best results for unbalanced 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds_imbalance}\")\n",
    "\n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds}\")\n",
    "\n",
    "    print(\"Best results for unbalanced 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds_imbalance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827ef23",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b095a44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500}\n",
      "Best Accuracy Score: 0.7331601731601732\n",
      "Random Forest analysis completed for 5-folds. Results saved to: params-rf-5-folds.csv\n",
      "Starting 5-fold analysis with unbalanced data...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Best Accuracy Score: 0.4606193806193807\n",
      "Random Forest analysis completed for 5-folds. Results saved to: params-rf-5-folds.csv\n",
      "Starting 3-fold analysis...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Best Accuracy Score: 0.7785714285714286\n",
      "Random Forest analysis completed for 3-folds. Results saved to: params-rf-3-folds.csv\n",
      "Starting 3-fold analysis with unbalanced data...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Accuracy Score: 0.459676860134526\n",
      "Random Forest analysis completed for 3-folds. Results saved to: params-rf-3-folds.csv\n",
      "Best results for 5-fold:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 500}\n",
      "Best f1 Score: 0.7331601731601732\n",
      "Best results for unbalanced 5-fold:\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Best f1 Score: 0.4606193806193807\n",
      "Best results for 3-fold:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Best f1 Score: 0.7785714285714286\n",
      "Best results for unbalanced 3-fold:\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best f1 Score: 0.459676860134526\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim, SparseRandomProjection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Random Forest with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "def flastRandomForestWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "            \n",
    "    if os.path.exists(flakyDir):\n",
    "        shutil.rmtree(flakyDir)\n",
    "    if os.path.exists(nonFlakyDir):\n",
    "        shutil.rmtree(nonFlakyDir)\n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    if len(dataPoints) > 0:\n",
    "        print(f\"Sample document: {dataPoints[0]}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataPointsList = np.array([Z[i].toarray() for i in range(Z.shape[0])])  # Keep this line unchanged\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the Random Forest classifier\n",
    "    classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning without 'rf__' prefixes\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [10,50, 100, 300,500],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [10, 30, 50, 100, 300, 500],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2]\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions for precision, recall, and accuracy\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score, zero_division=0),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(\n",
    "        classifier,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit='f1',\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(dataPointsList.reshape(len(dataPointsList), -1), dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"params-rf-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_estimators,max_depth,criterion,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = vecTime / len(dataPointsList)  # Estimating preparation time\n",
    "            fo.write(f\"{param['n_estimators']},{param['max_depth']},{param['criterion']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Random Forest analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup\n",
    "    flakyZip = \"compressedDataset/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    nonFlakyUnbalance = \"compressedDataset/non-flakyMethods.zip\"  # Added unbalanced dataset\n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/results_RandomForst\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform Random Forest analysis for 5 folds and 3 folds\n",
    "    dim = 100  # Example value for JL dimensionality reduction\n",
    "    eps = 0.3  # JL epsilon\n",
    "\n",
    "    print(\"Starting 5-fold analysis...\")\n",
    "    best_params_5folds, best_score_5folds = flastRandomForestWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, 5, dim, eps)\n",
    "\n",
    "    print(\"Starting 5-fold analysis with unbalanced data...\")\n",
    "    best_params_5folds_imbalance, best_score_5folds_imbalance = flastRandomForestWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyUnbalance, extractDir, 5, dim, eps)\n",
    "\n",
    "    print(\"Starting 3-fold analysis...\")\n",
    "    best_params_3folds, best_score_3folds = flastRandomForestWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, 3, dim, eps)\n",
    "\n",
    "    print(\"Starting 3-fold analysis with unbalanced data...\")\n",
    "    best_params_3folds_imbalance, best_score_3folds_imbalance = flastRandomForestWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyUnbalance, extractDir, 3, dim, eps)\n",
    "\n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds}\")\n",
    "\n",
    "    print(\"Best results for unbalanced 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds_imbalance}\")\n",
    "\n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds}\")\n",
    "\n",
    "    print(\"Best results for unbalanced 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds_imbalance}\")\n",
    "\n",
    "    '''\n",
    "    Take best parameters\n",
    "    Fit them inside model\n",
    "    Hyper-tune\n",
    "    Visualize using heatmap\n",
    "    Overleaf (writing)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92fa9ad",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35d6d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold analysis with Decision Tree...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best Accuracy Score: 0.7095906432748538\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: params-dt-5-folds.csv\n",
      "Starting 5-fold analysis with unbalanced data using Decision Tree...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Best Accuracy Score: 0.5338001867413632\n",
      "Decision Tree analysis completed for 5-folds. Results saved to: params-dt-5-folds.csv\n",
      "Starting 3-fold analysis with Decision Tree...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 3 folds for each of 504 candidates, totalling 1512 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Accuracy Score: 0.7324675324675325\n",
      "Decision Tree analysis completed for 3-folds. Results saved to: params-dt-3-folds.csv\n",
      "Starting 3-fold analysis with unbalanced data using Decision Tree...\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1134_7e2204c7f046ad324e1726ad320f0323bdd26e87_qiskit_tools___init__.py\n",
      "Empty file: extracted\\nonFlaky\\non-flakyMethods\\Qiskit_qiskit_PR1702_85b5af6100be236d97a3e0e77c8aba2ded185805_test_python_tools_visualization___init__.py\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 248\n",
      "Total number of documents: 295\n",
      "Sample document: # Copyright 2021 The NetKet Authors - All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import dataclasses\n",
      "import abc\n",
      "\n",
      "from flax.core import freeze\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class ModuleFramework(abc.ABC):\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_loaded() -> bool:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def is_my_module(module):\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap_params(variables):\n",
      "        return freeze({\"params\": variables})\n",
      "\n",
      "    @staticmethod\n",
      "    @abc.abstractmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "registered_frameworks = []\n",
      "\n",
      "\n",
      "def framework(clz):\n",
      "    \"\"\"\n",
      "    Registers a framework and it's wrapper methods to make it\n",
      "    behave like a flax framework.\n",
      "    \"\"\"\n",
      "    clz = dataclasses.dataclass(frozen=True)(clz)\n",
      "    registered_frameworks.append(clz)\n",
      "    return clz\n",
      "\n",
      "\n",
      "@dataclasses.dataclass(frozen=True)\n",
      "class UnknownFramework(ModuleFramework):\n",
      "    name: str = \"Unknown\"\n",
      "\n",
      "    @staticmethod\n",
      "    def is_loaded() -> bool:\n",
      "        return True\n",
      "\n",
      "    @staticmethod\n",
      "    def is_my_module(module):\n",
      "        return False\n",
      "\n",
      "    @staticmethod\n",
      "    def wrap(module):\n",
      "        return module\n",
      "\n",
      "    @staticmethod\n",
      "    def unwrap_params(wrapped_variables):\n",
      "        return wrapped_variables\n",
      "\n",
      "\n",
      "def identify_framework(module):\n",
      "    for _framework in registered_frameworks:\n",
      "        if _framework.is_my_module(module):\n",
      "            return _framework\n",
      "\n",
      "    return UnknownFramework\n",
      "\n",
      "\n",
      "def maybe_wrap_module(module):\n",
      "    \"\"\"\n",
      "    Passing a module from an unknown framework (might be user defined module, a jax\n",
      "    module, flax or haiku or anything else really), attempt to identify what is the\n",
      "    package/framework it comes from, and if so it correctly wraps it in order to\n",
      "    make it behave like a flax module (our default).\n",
      "\n",
      "    Also returns a function used to unpack the parameters once we are done.\n",
      "    \"\"\"\n",
      "    framewrk = identify_framework(module)\n",
      "\n",
      "    return framewrk, framewrk.wrap(module)\n",
      "Fitting 3 folds for each of 504 candidates, totalling 1512 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Accuracy Score: 0.47474747474747475\n",
      "Decision Tree analysis completed for 3-folds. Results saved to: params-dt-3-folds.csv\n",
      "Best results for 5-fold:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best f1 Score: 0.7095906432748538\n",
      "Best results for unbalanced 5-fold:\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Best f1 Score: 0.5338001867413632\n",
      "Best results for 3-fold:\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best f1 Score: 0.7324675324675325\n",
      "Best results for unbalanced 3-fold:\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best f1 Score: 0.47474747474747475\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim, SparseRandomProjection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree with GridSearchCV and Multiple Scoring Metrics\n",
    "\n",
    "def flastDecisionTreeWithGridSearchCV(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    if os.path.exists(flakyDir):\n",
    "        shutil.rmtree(flakyDir)\n",
    "    if os.path.exists(nonFlakyDir):\n",
    "        shutil.rmtree(nonFlakyDir)\n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "    \n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    if len(dataPoints) > 0:\n",
    "        print(f\"Sample document: {dataPoints[0]}\")\n",
    "\n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataPointsList = np.array([Z[i].toarray() for i in range(Z.shape[0])])\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define the Decision Tree classifier\n",
    "    classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [None, 10, 30, 50,100,300,500],\n",
    "        \"min_samples_split\": [2, 5, 10,],\n",
    "        \"min_samples_leaf\": [1, 2, 5,10],\n",
    "        \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions for precision, recall, and accuracy\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score)\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(\n",
    "        classifier,\n",
    "        param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        refit='f1',\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(dataPointsList.reshape(len(dataPointsList), -1), dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for accuracy\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Accuracy Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"params-dt-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"criterion,max_depth,min_samples_split,min_samples_leaf,max_features,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = vecTime / len(dataPointsList)  # Estimating preparation time\n",
    "            fo.write(f\"{param['criterion']},{param['max_depth']},{param['min_samples_split']},{param['min_samples_leaf']},{param['max_features']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters setup\n",
    "    flakyZip = \"compressedDataset/cleaned_flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    nonFlakyUnbalance = \"compressedDataset/non-flakyMethods.zip\"  # Added unbalanced dataset\n",
    "\n",
    "    extractDir = \"extracted\"\n",
    "    outDir = \"results/results_DecisionTree\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform Decision Tree analysis for 5 folds and 3 folds\n",
    "    dim = 100  # Example value for JL dimensionality reduction\n",
    "    eps = 0.3  # JL epsilon\n",
    "\n",
    "    print(\"Starting 5-fold analysis with Decision Tree...\")\n",
    "    best_params_5folds, best_score_5folds = flastDecisionTreeWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, 5, dim, eps)\n",
    "\n",
    "    # Added the unbalanced dataset analysis\n",
    "    print(\"Starting 5-fold analysis with unbalanced data using Decision Tree...\")\n",
    "    best_params_5folds_imbalance, best_score_5folds_imbalance = flastDecisionTreeWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyUnbalance, extractDir, 5, dim, eps)\n",
    "\n",
    "    print(\"Starting 3-fold analysis with Decision Tree...\")\n",
    "    best_params_3folds, best_score_3folds = flastDecisionTreeWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, 3, dim, eps)\n",
    "\n",
    "    # Added the unbalanced dataset analysis\n",
    "    print(\"Starting 3-fold analysis with unbalanced data using Decision Tree...\")\n",
    "    best_params_3folds_imbalance, best_score_3folds_imbalance = flastDecisionTreeWithGridSearchCV(\n",
    "        outDir, flakyZip, nonFlakyUnbalance, extractDir, 3, dim, eps)\n",
    "\n",
    "    print(\"Best results for 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds}\")\n",
    "\n",
    "    print(\"Best results for unbalanced 5-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_5folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_5folds_imbalance}\")\n",
    "\n",
    "    print(\"Best results for 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds}\")\n",
    "\n",
    "    print(\"Best results for unbalanced 3-fold:\")\n",
    "    print(f\"Best Parameters: {best_params_3folds_imbalance}\")\n",
    "    print(f\"Best f1 Score: {best_score_3folds_imbalance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726ba8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
