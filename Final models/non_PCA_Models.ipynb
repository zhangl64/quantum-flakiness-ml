{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY94WkB0xrt_",
        "outputId": "5b14d2ee-197e-454c-bb99-84774e3b9bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data points shape: (299, 11986)\n",
            "Number of flaky files: 45\n",
            "Number of non-flaky files: 254\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Preprocessing and Data Preparation\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Function to extract zip files\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Function to read .py files from the given directory\n",
        "def getDataPoints(path):\n",
        "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
        "    dataPointsList = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Directory does not exist: {path}\")\n",
        "        return dataPointsList\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for dataPointName in files:\n",
        "            if dataPointName.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, dataPointName)\n",
        "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
        "                    dp = fileIn.read().strip()\n",
        "                    if dp:\n",
        "                        dataPointsList.append(dp)\n",
        "                    else:\n",
        "                        print(f\"Empty file skipped: {file_path}\")\n",
        "    return dataPointsList\n",
        "\n",
        "# Function for vectorization using CountVectorizer\n",
        "def flastVectorization(dataPoints):\n",
        "    countVec = CountVectorizer(stop_words=None)\n",
        "    return countVec.fit_transform(dataPoints)\n",
        "\n",
        "# Paths to your zip files (you can modify these paths)\n",
        "flakyZip = \"/content/flaky_files (1).zip\"\n",
        "nonFlakyZip = \"/content/all_nonflaky_files (1).zip\"\n",
        "\n",
        "# Create directories for extraction\n",
        "extractDir = \"extracted_files\"\n",
        "flakyDir = os.path.join(extractDir, 'flaky')\n",
        "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(flakyDir, exist_ok=True)\n",
        "os.makedirs(nonFlakyDir, exist_ok=True)\n",
        "\n",
        "# Extract the zip files\n",
        "extract_zip(flakyZip, flakyDir)\n",
        "extract_zip(nonFlakyZip, nonFlakyDir)\n",
        "\n",
        "# Collect data points from flaky and non-flaky files\n",
        "dataPointsFlaky = getDataPoints(flakyDir)\n",
        "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
        "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
        "\n",
        "# Create labels: 1 for flaky, 0 for non-flaky\n",
        "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
        "\n",
        "# Vectorize the data\n",
        "Z = flastVectorization(dataPoints)\n",
        "\n",
        "print(f\"Data points shape: {Z.shape}\")\n",
        "print(f\"Number of flaky files: {len(dataPointsFlaky)}\")\n",
        "print(f\"Number of non-flaky files: {len(dataPointsNonFlaky)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "McHkV0aFx4sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "# Custom MCC scorer function\n",
        "def mcc_scorer(estimator, X, y_true):\n",
        "    y_pred = estimator.predict(X)\n",
        "    return matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "# Summarize and find best parameter combinations\n",
        "def summarize_and_find_best(df_results):\n",
        "    summary_df = df_results.groupby(['rf__n_estimators', 'rf__max_depth', 'rf__min_samples_split', 'rf__min_samples_leaf', 'rf__criterion', 'threshold']).agg({\n",
        "        'accuracy': 'mean',\n",
        "        'precision': 'mean',\n",
        "        'recall': 'mean',\n",
        "        'f1': 'mean',\n",
        "        'mcc': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Finding the best parameter set based on the highest F1 score\n",
        "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
        "\n",
        "    return summary_df, best_row\n",
        "\n",
        "# Combined SMOTE and Threshold-based Random Forest (No PCA)\n",
        "def runRFWithSMOTEAndThreshold(X, y, outDir, n_splits, param_grid, thresholds):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    metrics_per_combination = []\n",
        "\n",
        "    for params in product(*param_grid.values()):\n",
        "        param_dict = dict(zip(param_grid.keys(), params))\n",
        "\n",
        "        # Define pipeline with SMOTE and Random Forest\n",
        "        pipeline = ImbPipeline([\n",
        "            ('smote', SMOTE(random_state=42)),\n",
        "            ('rf', RandomForestClassifier(\n",
        "                n_estimators=param_dict['rf__n_estimators'],\n",
        "                max_depth=param_dict['rf__max_depth'],\n",
        "                min_samples_split=param_dict['rf__min_samples_split'],\n",
        "                min_samples_leaf=param_dict['rf__min_samples_leaf'],\n",
        "                criterion=param_dict['rf__criterion'],\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "            X_fold_train, X_fold_test = X[train_index].toarray(), X[test_index].toarray()\n",
        "            y_fold_train, y_fold_test = y[train_index], y[test_index]\n",
        "\n",
        "            # Train the pipeline\n",
        "            pipeline.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "            # Predict probabilities\n",
        "            y_pred_proba = pipeline.predict_proba(X_fold_test)[:, 1]  # Random Forest probability for positive class\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_fold_test, y_pred_threshold)\n",
        "                precision = precision_score(y_fold_test, y_pred_threshold, zero_division=1)\n",
        "                recall = recall_score(y_fold_test, y_pred_threshold, zero_division=1)\n",
        "                f1 = f1_score(y_fold_test, y_pred_threshold, zero_division=1)\n",
        "                mcc = matthews_corrcoef(y_fold_test, y_pred_threshold)\n",
        "\n",
        "                # Store results\n",
        "                metrics_per_combination.append({\n",
        "                    'rf__n_estimators': param_dict['rf__n_estimators'],\n",
        "                    'rf__max_depth': param_dict['rf__max_depth'],\n",
        "                    'rf__min_samples_split': param_dict['rf__min_samples_split'],\n",
        "                    'rf__min_samples_leaf': param_dict['rf__min_samples_leaf'],\n",
        "                    'rf__criterion': param_dict['rf__criterion'],\n",
        "                    'fold': fold + 1,\n",
        "                    'threshold': threshold,\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'mcc': mcc\n",
        "                })\n",
        "\n",
        "    # Convert metrics to DataFrame and save results\n",
        "    df_results = pd.DataFrame(metrics_per_combination)\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    outFile = os.path.join(outDir, \"rf-smote-threshold-results.csv\")\n",
        "    df_results.to_csv(outFile, index=False)\n",
        "\n",
        "    # Summarize results and return the best parameter set\n",
        "    summary_df, best_params = summarize_and_find_best(df_results)\n",
        "\n",
        "    # Save summarized results to a new CSV file\n",
        "    summary_outFile = os.path.join(outDir, \"rf-smote-threshold-summary.csv\")\n",
        "    summary_df.to_csv(summary_outFile, index=False)\n",
        "    print(f\"Summary of results saved to {summary_outFile}\")\n",
        "\n",
        "    return df_results, best_params\n",
        "\n",
        "# Hyperparameters and thresholds\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [10, 50, 100],                   # Number of trees in the forest\n",
        "    'rf__max_depth': [10, 30, 50],                       # Maximum depth of the tree\n",
        "    'rf__min_samples_split': [2, 5],                     # Minimum number of samples required to split a node\n",
        "    'rf__min_samples_leaf': [1, 2],                      # Minimum number of samples required at a leaf node\n",
        "    'rf__criterion': ['gini', 'entropy']                 # Function to measure the quality of a split\n",
        "}\n",
        "\n",
        "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
        "\n",
        "outDir = \"results/\"\n",
        "df_results, best_params = runRFWithSMOTEAndThreshold(Z, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
        "\n",
        "print(\"\\nRandom Forest with SMOTE and Threshold analysis completed.\")\n",
        "print(\"Best parameter set based on F1-score:\")\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfYjJdp_x7le",
        "outputId": "ca09f7d3-dad3-4637-a489-9661651c4285"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of results saved to results/rf-smote-threshold-summary.csv\n",
            "\n",
            "Random Forest with SMOTE and Threshold analysis completed.\n",
            "Best parameter set based on F1-score:\n",
            "rf__n_estimators               10\n",
            "rf__max_depth                  10\n",
            "rf__min_samples_split           2\n",
            "rf__min_samples_leaf            1\n",
            "rf__criterion             entropy\n",
            "threshold                     0.6\n",
            "accuracy                 0.956554\n",
            "precision                0.877273\n",
            "recall                   0.844444\n",
            "f1                       0.849412\n",
            "mcc                      0.831831\n",
            "Name: 5, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "R6_eoOI9ycjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "# Custom MCC scorer function\n",
        "def mcc_scorer(estimator, X, y_true):\n",
        "    y_pred = estimator.predict(X)\n",
        "    return matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "# Summarize and find best parameter combinations\n",
        "def summarize_and_find_best(df_results):\n",
        "    summary_df = df_results.groupby(['dt__max_depth', 'dt__min_samples_split', 'dt__min_samples_leaf', 'dt__criterion', 'threshold']).agg({\n",
        "        'accuracy': 'mean',\n",
        "        'precision': 'mean',\n",
        "        'recall': 'mean',\n",
        "        'f1': 'mean',\n",
        "        'mcc': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Finding the best parameter set based on the highest F1 score\n",
        "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
        "\n",
        "    return summary_df, best_row\n",
        "\n",
        "# Combined SMOTE and Threshold-based Decision Tree (No PCA)\n",
        "def runDTWithSMOTEAndThreshold(X, y, outDir, n_splits, param_grid, thresholds):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    metrics_per_combination = []\n",
        "\n",
        "    for params in product(*param_grid.values()):\n",
        "        param_dict = dict(zip(param_grid.keys(), params))\n",
        "\n",
        "        # Define pipeline with SMOTE and Decision Tree\n",
        "        pipeline = ImbPipeline([\n",
        "            ('smote', SMOTE(random_state=42)),\n",
        "            ('dt', DecisionTreeClassifier(\n",
        "                max_depth=param_dict['dt__max_depth'],\n",
        "                min_samples_split=param_dict['dt__min_samples_split'],\n",
        "                min_samples_leaf=param_dict['dt__min_samples_leaf'],\n",
        "                criterion=param_dict['dt__criterion'],\n",
        "                random_state=42\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "            X_fold_train, X_fold_test = X[train_index].toarray(), X[test_index].toarray()\n",
        "            y_fold_train, y_fold_test = y[train_index], y[test_index]\n",
        "\n",
        "            # Train the pipeline\n",
        "            pipeline.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "            # Predict probabilities\n",
        "            y_pred_proba = pipeline.predict_proba(X_fold_test)[:, 1]  # Decision Tree probability for positive class\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_fold_test, y_pred_threshold)\n",
        "                precision = precision_score(y_fold_test, y_pred_threshold, zero_division=1)\n",
        "                recall = recall_score(y_fold_test, y_pred_threshold, zero_division=1)\n",
        "                f1 = f1_score(y_fold_test, y_pred_threshold, zero_division=1)\n",
        "                mcc = matthews_corrcoef(y_fold_test, y_pred_threshold)\n",
        "\n",
        "                # Store results\n",
        "                metrics_per_combination.append({\n",
        "                    'dt__max_depth': param_dict['dt__max_depth'],\n",
        "                    'dt__min_samples_split': param_dict['dt__min_samples_split'],\n",
        "                    'dt__min_samples_leaf': param_dict['dt__min_samples_leaf'],\n",
        "                    'dt__criterion': param_dict['dt__criterion'],\n",
        "                    'fold': fold + 1,\n",
        "                    'threshold': threshold,\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'mcc': mcc\n",
        "                })\n",
        "\n",
        "    # Convert metrics to DataFrame and save results\n",
        "    df_results = pd.DataFrame(metrics_per_combination)\n",
        "    os.makedirs(outDir, exist_ok=True)\n",
        "    outFile = os.path.join(outDir, \"dt-smote-threshold-results.csv\")\n",
        "    df_results.to_csv(outFile, index=False)\n",
        "\n",
        "    # Summarize results and return the best parameter set\n",
        "    summary_df, best_params = summarize_and_find_best(df_results)\n",
        "\n",
        "    # Save summarized results to a new CSV file\n",
        "    summary_outFile = os.path.join(outDir, \"dt-smote-threshold-summary.csv\")\n",
        "    summary_df.to_csv(summary_outFile, index=False)\n",
        "    print(f\"Summary of results saved to {summary_outFile}\")\n",
        "\n",
        "    return df_results, best_params\n",
        "\n",
        "# Hyperparameters and thresholds\n",
        "param_grid = {\n",
        "    'dt__max_depth': [10, 20, 30],                  # Maximum depth of the decision tree\n",
        "    'dt__min_samples_split': [2, 5],                # Minimum number of samples required to split a node\n",
        "    'dt__min_samples_leaf': [1, 2],                 # Minimum number of samples required at a leaf node\n",
        "    'dt__criterion': ['gini', 'entropy']            # Function to measure the quality of a split\n",
        "}\n",
        "\n",
        "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
        "\n",
        "outDir = \"results/\"\n",
        "df_results, best_params = runDTWithSMOTEAndThreshold(Z, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
        "\n",
        "print(\"\\nDecision Tree with SMOTE and Threshold analysis completed.\")\n",
        "print(\"Best parameter set based on F1-score:\")\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoFWUzGmyecK",
        "outputId": "4a8db336-85e8-4b9d-8969-12bac228dcd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of results saved to results/dt-smote-threshold-summary.csv\n",
            "\n",
            "Decision Tree with SMOTE and Threshold analysis completed.\n",
            "Best parameter set based on F1-score:\n",
            "dt__max_depth                  10\n",
            "dt__min_samples_split           2\n",
            "dt__min_samples_leaf            1\n",
            "dt__criterion                gini\n",
            "threshold                     0.1\n",
            "accuracy                 0.963277\n",
            "precision                    0.94\n",
            "recall                   0.822222\n",
            "f1                       0.872074\n",
            "mcc                      0.856652\n",
            "Name: 9, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Obxhsyp84Ib8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}