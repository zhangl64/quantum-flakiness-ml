{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014a5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Shape of vectorized data: (288, 11847)\n",
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Paths to your datasets\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"Dataset/nonflaky_files.zip\"   \n",
    "\n",
    "# Create directories\n",
    "outDir = \"results/nonflaky/\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "extractDir = \"extracted/nonflaky/\"\n",
    "os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "# Extract and read data\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "# Print the number of datasets\n",
    "print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "\n",
    "# Vectorize data\n",
    "Z = flastVectorization(dataPoints)\n",
    "\n",
    "print(\"Shape of vectorized data:\", Z.shape)\n",
    "print(\"Data preprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2c816",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30601c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 150, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 180, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 200, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'cosine'}\n",
      "Training with parameters: {'pca__n_components': 220, 'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean'}\n",
      "Per-fold KNN analysis completed. Results saved to: knn_thresholds-thresholds-knn-results-per-fold.csv\n",
      "Averaged results saved to: knn_thresholds-thresholds-knn-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   pca__n_components  n_neighbors  weights  metric  fold  threshold  accuracy  \\\n",
      "0                150            3  uniform  cosine     1        0.1  0.758621   \n",
      "1                150            3  uniform  cosine     1        0.2  0.758621   \n",
      "2                150            3  uniform  cosine     1        0.3  0.758621   \n",
      "3                150            3  uniform  cosine     1        0.4  0.879310   \n",
      "4                150            3  uniform  cosine     1        0.5  0.879310   \n",
      "\n",
      "   precision    recall        f1       mcc  \n",
      "0   0.333333  0.555556  0.416667  0.290625  \n",
      "1   0.333333  0.555556  0.416667  0.290625  \n",
      "2   0.333333  0.555556  0.416667  0.290625  \n",
      "3   0.750000  0.333333  0.461538  0.447129  \n",
      "4   0.750000  0.333333  0.461538  0.447129  \n",
      "Averaged Results:\n",
      "   pca__n_components  n_neighbors   weights  metric  threshold  accuracy_mean  \\\n",
      "0                150            3  distance  cosine        0.1       0.854204   \n",
      "1                150            3  distance  cosine        0.2       0.864610   \n",
      "2                150            3  distance  cosine        0.3       0.868179   \n",
      "3                150            3  distance  cosine        0.4       0.878584   \n",
      "4                150            3  distance  cosine        0.5       0.875076   \n",
      "\n",
      "   accuracy_std  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0      0.031166        0.533217       0.090486     0.600000    0.168508   \n",
      "1      0.037400        0.568182       0.122390     0.600000    0.168508   \n",
      "2      0.039652        0.588889       0.133795     0.511111    0.168508   \n",
      "3      0.023753        0.734286       0.200408     0.355556    0.144871   \n",
      "4      0.022002        0.774286       0.233954     0.311111    0.144871   \n",
      "\n",
      "    f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0  0.555758  0.114102  0.476908  0.125255  \n",
      "1  0.575462  0.131355  0.501408  0.147215  \n",
      "2  0.542222  0.146228  0.470821  0.165204  \n",
      "3  0.464935  0.162247  0.448658  0.158913  \n",
      "4  0.423377  0.154581  0.427528  0.149471  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "def flastKNNWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization without PCA\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Define the pipeline with PCA and KNN\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('knn', KNeighborsClassifier(\n",
    "                n_neighbors=param_dict['n_neighbors'],\n",
    "                weights=param_dict['weights'],\n",
    "                metric=param_dict['metric'],\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            if hasattr(pipeline.named_steps['knn'], \"predict_proba\"):\n",
    "                y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "            else:\n",
    "                # If predict_proba is not available, use distance-based probabilities\n",
    "                distances, indices = pipeline.named_steps['knn'].kneighbors(X_test_dense)\n",
    "                weights = pipeline.named_steps['knn']._get_weights(distances)\n",
    "                y_pred_proba = np.zeros((X_test_dense.shape[0], 2))\n",
    "                for i, neighbors in enumerate(indices):\n",
    "                    neighbor_labels = y_train[neighbors]\n",
    "                    if weights is None:\n",
    "                        proba = np.bincount(neighbor_labels, minlength=2) / pipeline.named_steps['knn'].n_neighbors\n",
    "                    else:\n",
    "                        proba = np.bincount(neighbor_labels, weights=weights[i], minlength=2) / weights[i].sum()\n",
    "                    y_pred_proba[i] = proba\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'metric': param_dict['metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-knn-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold KNN analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['pca__n_components', 'n_neighbors', 'weights', 'metric', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-knn-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #param grid\n",
    "    param_grid = {\n",
    "        'pca__n_components': [150,180,200,220],  \n",
    "        'n_neighbors': [3, 5, 7, 9,11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['cosine', 'euclidean'],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"   \n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  \n",
    "    \n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/knn_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/knn_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform KNN analysis with threshold adjustments\n",
    "    print(\"Starting KNN analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastKNNWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, combination_label=\"knn_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52380d",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfccd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 150, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 180, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 200, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.01, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 0.1, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 1.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 10.0, 'svm__kernel': 'sigmoid'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'linear'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'rbf'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'poly'}\n",
      "Training with parameters: {'pca__n_components': 220, 'svm__C': 100.0, 'svm__kernel': 'sigmoid'}\n",
      "Per-fold SVM analysis completed. Results saved to: svm_thresholds-thresholds-svm-results-per-fold.csv\n",
      "Averaged results saved to: svm_thresholds-thresholds-svm-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   pca__n_components  svm__C svm__kernel  fold  threshold  accuracy  \\\n",
      "0                150    0.01      linear     1        0.1  0.793103   \n",
      "1                150    0.01      linear     1        0.2  0.913793   \n",
      "2                150    0.01      linear     1        0.3  0.948276   \n",
      "3                150    0.01      linear     1        0.4  0.948276   \n",
      "4                150    0.01      linear     1        0.5  0.896552   \n",
      "\n",
      "   precision    recall        f1       mcc  \n",
      "0   0.411765  0.777778  0.538462  0.456336  \n",
      "1   0.750000  0.666667  0.705882  0.657143  \n",
      "2   1.000000  0.666667  0.800000  0.792594  \n",
      "3   1.000000  0.666667  0.800000  0.792594  \n",
      "4   1.000000  0.333333  0.500000  0.544949  \n",
      "Averaged Results:\n",
      "   pca__n_components  svm__C svm__kernel  threshold  accuracy_mean  \\\n",
      "0                150    0.01      linear        0.1       0.781246   \n",
      "1                150    0.01      linear        0.2       0.909619   \n",
      "2                150    0.01      linear        0.3       0.902601   \n",
      "3                150    0.01      linear        0.4       0.909619   \n",
      "4                150    0.01      linear        0.5       0.902783   \n",
      "\n",
      "   accuracy_std  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0      0.049663        0.415755       0.054813     0.911111    0.121716   \n",
      "1      0.019698        0.702121       0.036890     0.733333    0.185924   \n",
      "2      0.042399        0.720556       0.186790     0.600000    0.230405   \n",
      "3      0.045152        0.891429       0.174262     0.488889    0.255797   \n",
      "4      0.043630        0.933333       0.149071     0.400000    0.255797   \n",
      "\n",
      "    f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0  0.567704  0.060260  0.514142  0.079142  \n",
      "1  0.707593  0.103336  0.661493  0.109769  \n",
      "2  0.642109  0.204357  0.598420  0.216152  \n",
      "3  0.595714  0.270091  0.597732  0.235823  \n",
      "4  0.526667  0.271211  0.553641  0.236713  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def flastSVMWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization \n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "\n",
    "        # Define the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('svm', SVC(\n",
    "                C=param_dict['svm__C'],\n",
    "                kernel=param_dict['svm__kernel'],\n",
    "                probability=True,  # Enable probability estimates\n",
    "                random_state=42\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'svm__C': param_dict['svm__C'],\n",
    "                    'svm__kernel': param_dict['svm__kernel'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-svm-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold SVM analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['pca__n_components', 'svm__C', 'svm__kernel', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-svm-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "    'pca__n_components': [150, 180, 200, 220],           # PCA components\n",
    "    'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0],            # Regularization parameter C for SVM\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types for SVM\n",
    "}\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  \n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/svm_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/svm_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform SVM analysis with threshold adjustments\n",
    "    print(\"Starting SVM analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastSVMWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5, combination_label=\"svm_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c77477",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1eee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c379083",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "794c3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost analysis with threshold adjustments...\n",
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 180, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 200, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 3, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 5, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 7, 'xgb__n_estimators': 300}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 50}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 100}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 200}\n",
      "Training with parameters: {'pca__n_components': 220, 'xgb__learning_rate': 0.5, 'xgb__max_depth': 10, 'xgb__n_estimators': 300}\n",
      "Per-fold XGBoost analysis completed. Results saved to: xgb_thresholds-thresholds-xgb-results-per-fold.csv\n",
      "Averaged results saved to: xgb_thresholds-thresholds-xgb-results-averaged.csv\n",
      "Analysis completed. Per-fold Results:\n",
      "   pca__n_components  xgb__learning_rate  xgb__max_depth  xgb__n_estimators  \\\n",
      "0                180                0.01               3                 50   \n",
      "1                180                0.01               3                 50   \n",
      "2                180                0.01               3                 50   \n",
      "3                180                0.01               3                 50   \n",
      "4                180                0.01               3                 50   \n",
      "\n",
      "   fold  threshold  accuracy  precision    recall        f1       mcc  \n",
      "0     1        0.1  0.155172   0.155172  1.000000  0.268657  0.000000  \n",
      "1     1        0.2  0.896552   0.714286  0.555556  0.625000  0.572101  \n",
      "2     1        0.3  0.862069   0.666667  0.222222  0.333333  0.329935  \n",
      "3     1        0.4  0.879310   1.000000  0.222222  0.363636  0.440959  \n",
      "4     1        0.5  0.844828   1.000000  0.000000  0.000000  0.000000  \n",
      "Averaged Results:\n",
      "   pca__n_components  xgb__learning_rate  xgb__max_depth  xgb__n_estimators  \\\n",
      "0                180                0.01               3                 50   \n",
      "1                180                0.01               3                 50   \n",
      "2                180                0.01               3                 50   \n",
      "3                180                0.01               3                 50   \n",
      "4                180                0.01               3                 50   \n",
      "\n",
      "   threshold  accuracy_mean  accuracy_std  precision_mean  precision_std  \\\n",
      "0        0.1       0.156261      0.001491        0.156261       0.001491   \n",
      "1        0.2       0.830067      0.081730        0.506583       0.175154   \n",
      "2        0.3       0.854265      0.018818        0.666667       0.216025   \n",
      "3        0.4       0.857592      0.014989        1.000000       0.000000   \n",
      "4        0.5       0.843739      0.001491        1.000000       0.000000   \n",
      "\n",
      "   recall_mean  recall_std   f1_mean    f1_std  mcc_mean   mcc_std  \n",
      "0     1.000000    0.000000  0.270285  0.002230  0.000000  0.000000  \n",
      "1     0.622222    0.149071  0.546874  0.143096  0.457424  0.187059  \n",
      "2     0.244444    0.164804  0.316190  0.200611  0.278431  0.183541  \n",
      "3     0.088889    0.092962  0.152727  0.154599  0.211725  0.200663  \n",
      "4     0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import product\n",
    "from xgboost import XGBClassifier  # Import XGBoost classifier\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Function to run XGBoost analysis with PCA and threshold adjustments\n",
    "\n",
    "def flastXGBWithThresholds(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization\n",
    "    Z = flastVectorization(dataPoints)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Define the pipeline with PCA and XGBoost\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('xgb', XGBClassifier(\n",
    "                learning_rate=param_dict['xgb__learning_rate'],\n",
    "                max_depth=param_dict['xgb__max_depth'],\n",
    "                n_estimators=param_dict['xgb__n_estimators'],\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                verbosity=0,\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )),\n",
    "        ])\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Convert to dense format for PCA\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "\n",
    "            # Train the model\n",
    "            pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = pipeline.predict_proba(X_test_dense)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'xgb__learning_rate': param_dict['xgb__learning_rate'],\n",
    "                    'xgb__max_depth': param_dict['xgb__max_depth'],\n",
    "                    'xgb__n_estimators': param_dict['xgb__n_estimators'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert the list of metrics into a DataFrame\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "\n",
    "    # Save the per-fold results\n",
    "    outFile = f\"{combination_label}-thresholds-xgb-results-per-fold.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Per-fold XGBoost analysis completed. Results saved to: {outFile}\")\n",
    "\n",
    "    # Compute the average metrics across folds for each combination of hyperparameters and thresholds\n",
    "    grouped_metrics = df_results.groupby(\n",
    "        ['pca__n_components', 'xgb__learning_rate', 'xgb__max_depth', 'xgb__n_estimators', 'threshold']\n",
    "    ).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'mcc': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    grouped_metrics.columns = [\n",
    "        '_'.join(col).strip('_') if col[1] else col[0]\n",
    "        for col in grouped_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    # Save the averaged results\n",
    "    outFileAvg = f\"{combination_label}-thresholds-xgb-results-averaged.csv\"\n",
    "    grouped_metrics.to_csv(os.path.join(outDir, outFileAvg), index=False)\n",
    "\n",
    "    print(f\"Averaged results saved to: {outFileAvg}\")\n",
    "\n",
    "    return df_results, grouped_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'pca__n_components': [180, 200, 220],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "        'xgb__max_depth': [3, 5, 7, 10],\n",
    "        'xgb__n_estimators': [50, 100, 200, 300],\n",
    "    }\n",
    "\n",
    "    # Paths to your datasets\n",
    "    flakyZip = \"Dataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"Dataset/nonflaky_files.zip\"  # Unbalanced dataset\n",
    "\n",
    "    # Create result and extract directories\n",
    "    outDir = \"results/xgb_thresholds/\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    extractDir = \"extracted/xgb_thresholds/\"\n",
    "    os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "    # Perform XGBoost analysis with threshold adjustments\n",
    "    print(\"Starting XGBoost analysis with threshold adjustments...\")\n",
    "    df_results, grouped_metrics = flastXGBWithThresholds(\n",
    "        outDir, flakyZip, nonFlakyZip, extractDir, n_splits=5,\n",
    "        combination_label=\"xgb_thresholds\", param_grid=param_grid)\n",
    "\n",
    "    if df_results is not None:\n",
    "        print(\"Analysis completed. Per-fold Results:\")\n",
    "        print(df_results.head())\n",
    "\n",
    "        print(\"Averaged Results:\")\n",
    "        print(grouped_metrics.head())\n",
    "    else:\n",
    "        print(\"Analysis did not produce any results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e68fb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43310b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d58f10d7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92196b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72871749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-fold results file for Naive Bayes does not exist: results\\nb_thresholds\\nb_thresholds-thresholds-nb-results-per-fold.csv\n",
      "Averaged results file for Naive Bayes does not exist: results\\nb_thresholds\\nb_thresholds-thresholds-nb-results-averaged.csv\n",
      "Best per-fold results saved to 'best_per_fold_results.csv'\n",
      "Best averaged results saved to 'best_averaged_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_per_fold['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n",
      "C:\\Users\\haha9\\AppData\\Local\\Temp\\ipykernel_69536\\1641632888.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_row_averaged['Model'] = model_name\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec077ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
