{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a655df6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778 0.7777777777777778\n",
      "0.6842105263157895 0.7222222222222222\n",
      "0.75 0.8333333333333334\n",
      "0.6 0.6666666666666666\n",
      "0.7 0.7777777777777778\n",
      "0.6875 0.6111111111111112\n",
      "0.625 0.5555555555555556\n",
      "0.6 0.5\n",
      "0.7333333333333333 0.6111111111111112\n",
      "0.6521739130434783 0.8333333333333334\n",
      "0.9166666666666666 0.6111111111111112\n",
      "0.65 0.7222222222222222\n",
      "0.7 0.7777777777777778\n",
      "0.56 0.7777777777777778\n",
      "0.5416666666666666 0.7222222222222222\n",
      "0.6086956521739131 0.7777777777777778\n",
      "0.7368421052631579 0.7777777777777778\n",
      "0.5882352941176471 0.5555555555555556\n",
      "0.7272727272727273 0.4444444444444444\n",
      "0.7 0.7777777777777778\n",
      "0.8 0.6666666666666666\n",
      "0.631578947368421 0.6666666666666666\n",
      "0.7 0.7777777777777778\n",
      "0.6111111111111112 0.6111111111111112\n",
      "0.6923076923076923 0.5\n",
      "0.6875 0.6111111111111112\n",
      "0.631578947368421 0.6666666666666666\n",
      "0.5454545454545454 0.3333333333333333\n",
      "0.7142857142857143 0.5555555555555556\n",
      "0.75 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "###############################################################################\n",
    "# read data from file\n",
    "\n",
    "def getDataPoints(path):\n",
    "    dataPointsList = []\n",
    "    for dataPointName in os.listdir(path):\n",
    "        if dataPointName[0] == \".\":\n",
    "            continue\n",
    "        filePath = os.path.join(path, dataPointName)\n",
    "        print(f\"Attempting to open file: {filePath}\")\n",
    "        if not os.path.exists(filePath):\n",
    "            print(f\"File does not exist: {filePath}\")\n",
    "            continue\n",
    "        with open(filePath, encoding=\"utf-8\") as fileIn:\n",
    "            dp = fileIn.read()\n",
    "        dataPointsList.append(dp)\n",
    "    return dataPointsList\n",
    "\n",
    "def getDataPointsInfo(projectBasePath, projectName):\n",
    "    # get list of tokenized test methods\n",
    "    projectPath = os.path.join(projectBasePath, projectName)\n",
    "    flakyPath = os.path.join(projectPath, \"flakyMethods\")\n",
    "    nonFlakyPath = os.path.join(projectPath, \"nonflakyMethods\")  # Updated path\n",
    "    return getDataPoints(flakyPath), getDataPoints(nonFlakyPath)\n",
    "\n",
    "# Example usage\n",
    "projectBasePath = r\"dataset\"\n",
    "projectName = \"project\"  # Replace with the actual project name\n",
    "\n",
    "flakyMethods, nonFlakyMethods = getDataPointsInfo(projectBasePath, projectName)\n",
    "\n",
    "print(\"Flaky Methods:\")\n",
    "for method in flakyMethods:\n",
    "    print(method)\n",
    "\n",
    "print(\"\\nNon-Flaky Methods:\")\n",
    "for method in nonFlakyMethods:\n",
    "    print(method)\n",
    "\n",
    "###############################################################################\n",
    "# compute effectiveness metrics\n",
    "\n",
    "def computeResults(testLabels, predictLabels):\n",
    "    warnings.filterwarnings(\"error\")  # to catch warnings, e.g., \"prec set to 0.0\"\n",
    "    try:\n",
    "        precision = precision_score(testLabels, predictLabels)\n",
    "    except:\n",
    "        precision = \"-\"\n",
    "    try:\n",
    "        recall = recall_score(testLabels, predictLabels)\n",
    "    except:\n",
    "        recall = \"-\"\n",
    "    warnings.resetwarnings()  # warnings are no more errors\n",
    "    return precision, recall\n",
    "\n",
    "###############################################################################\n",
    "# FLAST\n",
    "\n",
    "def vectorization(dataPoints, dim=0, eps=0.3):\n",
    "    countVec = CountVectorizer(stop_words=None)  # Disable stop words filtering\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "def classificationDecisionTree(trainData, trainLabels, testData, params):\n",
    "    # training\n",
    "    t0 = time.perf_counter()\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion=params.get(\"criterion\", \"gini\"),\n",
    "        splitter=params.get(\"splitter\", \"best\"),\n",
    "        max_depth=params.get(\"max_depth\"),\n",
    "        min_samples_split=params.get(\"min_samples_split\", 2),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
    "    )\n",
    "    clf.fit(trainData, trainLabels)\n",
    "    t1 = time.perf_counter()\n",
    "    trainTime = t1 - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    predictLabels = clf.predict(testData)\n",
    "    t1 = time.perf_counter()\n",
    "    testTime = t1 - t0\n",
    "\n",
    "    return trainTime, testTime, predictLabels\n",
    "\n",
    "def decisionTree(outDir, projectBasePath, projectName, kf, dim, eps, params):\n",
    "    v0 = time.perf_counter()\n",
    "    dataPointsFlaky, dataPointsNonFlaky = getDataPointsInfo(projectBasePath, projectName)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "    print(\"Data points before vectorization:\", dataPoints) \n",
    "    Z = vectorization(dataPoints, dim=dim, eps=eps)\n",
    "    dataPointsList = np.array([Z[i].toarray() for i in range(Z.shape[0])])\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    v1 = time.perf_counter()\n",
    "    vecTime = v1 - v0\n",
    "\n",
    "    # storage\n",
    "    decisionTree = (dataPointsList, dataLabelsList)\n",
    "    pickleDumpDecisionTree = os.path.join(outDir, \"decision-tree.pickle\")\n",
    "    with open(pickleDumpDecisionTree, \"wb\") as pickleFile:\n",
    "        pickle.dump(decisionTree, pickleFile)\n",
    "    storage = os.path.getsize(pickleDumpDecisionTree)\n",
    "    os.remove(pickleDumpDecisionTree)\n",
    "\n",
    "    avgP, avgR = 0, 0\n",
    "    avgTPrep, avgTPred = 0, 0\n",
    "    avgFlakyTrain, avgNonFlakyTrain, avgFlakyTest, avgNonFlakyTest = 0, 0, 0, 0\n",
    "    successFold, precisionFold = 0, 0\n",
    "    for (trnIdx, tstIdx) in kf.split(dataPointsList, dataLabelsList):\n",
    "        trainData, testData = dataPointsList[trnIdx], dataPointsList[tstIdx]\n",
    "        trainLabels, testLabels = dataLabelsList[trnIdx], dataLabelsList[tstIdx]\n",
    "        if sum(trainLabels) == 0 or sum(testLabels) == 0:\n",
    "            print(\"Skipping fold...\")\n",
    "            print(\" Flaky Train Tests\", sum(trainLabels))\n",
    "            print(\" Flaky Test Tests\", sum(testLabels))\n",
    "            continue\n",
    "\n",
    "        successFold += 1\n",
    "        avgFlakyTrain += sum(trainLabels)\n",
    "        avgNonFlakyTrain += len(trainLabels) - sum(trainLabels)\n",
    "        avgFlakyTest += sum(testLabels)\n",
    "        avgNonFlakyTest += len(testLabels) - sum(testLabels)\n",
    "\n",
    "        # prepare the data in the right format for Decision Tree\n",
    "        nSamplesTrainData, nxTrain, nyTrain = trainData.shape\n",
    "        trainData = trainData.reshape((nSamplesTrainData, nxTrain * nyTrain))\n",
    "        nSamplesTestData, nxTest, nyTest = testData.shape\n",
    "        testData = testData.reshape((nSamplesTestData, nxTest * nyTest))\n",
    "\n",
    "        trainTime, testTime, predictLabels = classificationDecisionTree(trainData, trainLabels, testData, params)\n",
    "        preparationTime = (vecTime * len(trainData) / len(dataPoints)) + trainTime\n",
    "        predictionTime = (vecTime / len(dataPoints)) + (testTime / len(testData))\n",
    "        (precision, recall) = computeResults(testLabels, predictLabels)\n",
    "\n",
    "        print(precision, recall)\n",
    "        if precision != \"-\":\n",
    "            precisionFold += 1\n",
    "            avgP += precision\n",
    "        avgR += recall\n",
    "        avgTPrep += preparationTime\n",
    "        avgTPred += predictionTime\n",
    "\n",
    "    if precisionFold == 0:\n",
    "        avgP = \"-\"\n",
    "    else:\n",
    "        avgP /= precisionFold\n",
    "    avgR /= successFold\n",
    "    avgTPrep /= successFold\n",
    "    avgTPred /= successFold\n",
    "    avgFlakyTrain /= successFold\n",
    "    avgNonFlakyTrain /= successFold\n",
    "    avgFlakyTest /= successFold\n",
    "    avgNonFlakyTest /= successFold\n",
    "\n",
    "    return (avgFlakyTrain, avgNonFlakyTrain, avgFlakyTest, avgNonFlakyTest, avgP, avgR, storage, avgTPrep, avgTPred)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    projectBasePath = \"dataset\"\n",
    "    projectList = [\n",
    "        \"project\"  # Replace with the actual project name\n",
    "    ]\n",
    "    outDir = \"results-DecisionTree\"\n",
    "    outFile = \"result_Decision_Tree.csv\"\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"dataset,flakyTrain,nonFlakyTrain,flakyTest,nonFlakyTest,precision,recall,storage,preparationTime,predictionTime\\n\")\n",
    "\n",
    "    numSplit = 30\n",
    "    testSetSize = 0.2\n",
    "    kf = StratifiedShuffleSplit(n_splits=numSplit, test_size=testSetSize)\n",
    "\n",
    "    # FLAST\n",
    "    dim = 0  # number of dimensions (0: JL with error eps)\n",
    "    eps = 0.3  # JL eps\n",
    "    params = {\n",
    "        \"criterion\": \"gini\",\n",
    "        \"splitter\": \"best\",\n",
    "        \"max_depth\": None,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"min_samples_leaf\": 1\n",
    "    }\n",
    "    for projectName in projectList:\n",
    "        print(projectName.upper(), \"FLAST\")\n",
    "        (flakyTrain, nonFlakyTrain, flakyTest, nonFlakyTest, avgP, avgR, storage, avgTPrep, avgTPred) = decisionTree(outDir, projectBasePath, projectName, kf, dim, eps, params)\n",
    "        with open(os.path.join(outDir, outFile), \"a\") as fo:\n",
    "            fo.write(\"{},{},{},{},{},{},{},{},{},{}\\n\".format(projectName, flakyTrain, nonFlakyTrain, flakyTest, nonFlakyTest, avgP, avgR, storage, avgTPrep, avgTPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40150e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cc3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
