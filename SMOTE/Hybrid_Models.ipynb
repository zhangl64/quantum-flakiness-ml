{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xY94WkB0xrt_",
    "outputId": "7efe2c67-0bd6-4ae6-9508-39c7dcfca5c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents ( combination): 45\n",
      "Number of non-flaky documents ( combination): 243\n",
      "Total number of documents ( combination): 288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer) \n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "\n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "\n",
    "    return dataPointsList\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction\n",
    "\n",
    "# Parameters setup\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"Dataset/nonflaky_files.zip\"\n",
    "extractDir = \"extracted\"\n",
    "os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "best_hyperparameter = 'results/best_hyperparameter'\n",
    "os.makedirs(best_hyperparameter,exist_ok=True)\n",
    "# Extract the zip files\n",
    "# Extract and read data once for  non-flaky combination\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "# Print the number of datasets for  combination\n",
    "print(f\"Number of flaky documents ( combination): {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky documents ( combination): {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of documents ( combination): {len(dataPoints)}\")\n",
    "\n",
    "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxF2lhn6Rwqa"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OqX-b_8Rxvb",
    "outputId": "c8de935d-78ad-43d9-a54e-82ea8b9da1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting KNN analysis with SMOTE, PCA, and Threshold adjustment for 5-fold cross-validation...\n",
      "Results saved to: results/hybrid\\knn-smote-threshold-results.csv\n",
      "\n",
      "Best Parameters, Threshold, and Metrics:\n",
      "Best Parameters: {'pca__n_components': 200, 'knn__n_neighbors': 7, 'knn__weights': 'distance', 'knn__metric': 'cosine'}\n",
      "Best Threshold: 0.7000000000000001\n",
      "Best F1 Score: 0.6229509671614935 (Std Dev: 0.1556934194689674)\n",
      "Final MCC: 0.5801774825249082 (Std Dev: 0.16143406592047685)\n",
      "\n",
      "Best results for KNN with SMOTE, PCA, and Threshold adjustment 5-fold cross-validation:\n",
      "Best Parameters: {'pca__n_components': 200, 'knn__n_neighbors': 7, 'knn__weights': 'distance', 'knn__metric': 'cosine'}\n",
      "Best Threshold: 0.7000000000000001\n",
      "Best F1 Score: 0.6229509671614935 (Std Dev: 0.1556934194689674)\n",
      "Final MCC: 0.5801774825249082 (Std Dev: 0.16143406592047685)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json  # Import json to load best hyperparameters\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import product\n",
    "\n",
    "def KNNwithSMOTEThreshold(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    # Define parameter grid\n",
    "\n",
    "    # Load best hyperparameters from JSON file\n",
    "    with open(os.path.join(best_hyperparameter, 'best_params_knn.json'), 'r') as f:\n",
    "        best_params_knn = json.load(f)\n",
    "\n",
    "    param_grid = {\n",
    "        'pca__n_components': [best_params_knn['pca__n_components']],             \n",
    "        'knn__n_neighbors': [best_params_knn['knn__n_neighbors']],                  \n",
    "        'knn__weights': [best_params_knn['knn__weights']],               \n",
    "        'knn__metric': [best_params_knn['knn__metric']]                 \n",
    "    }\n",
    "    \n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    \n",
    "    # Prepare to store metrics\n",
    "    metrics_per_combination = []\n",
    "    \n",
    "    # Define thresholds to evaluate\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "    \n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Loop over each hyperparameter combination\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        \n",
    "        # Initialize lists to store metrics per threshold\n",
    "        threshold_metrics_list = []\n",
    "        \n",
    "        # For each fold in cross-validation\n",
    "        for fold_idx, (train_index, test_index) in enumerate(skf.split(dataPoints, dataLabelsList)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train = [dataLabelsList[i] for i in train_index]\n",
    "            y_test = [dataLabelsList[i] for i in test_index]\n",
    "            \n",
    "            # Define a pipeline with CountVectorizer, PCA, SMOTE, and KNN with current params\n",
    "            pipeline = ImbPipeline([\n",
    "                ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "                ('knn', KNeighborsClassifier(\n",
    "                    n_neighbors=param_dict['knn__n_neighbors'],\n",
    "                    weights=param_dict['knn__weights'],\n",
    "                    metric=param_dict['knn__metric']))\n",
    "            ])\n",
    "            \n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Iterate over thresholds\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "                \n",
    "                # Store fold metrics\n",
    "                threshold_metrics_list.append({\n",
    "                    **param_dict,\n",
    "                    'threshold': threshold,\n",
    "                    'fold': fold_idx + 1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "        \n",
    "        # Calculate average metrics and standard deviations over folds for each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Filter metrics for the current threshold\n",
    "            threshold_metrics = [tm for tm in threshold_metrics_list if tm['threshold'] == threshold]\n",
    "            \n",
    "            # Extract metrics per fold\n",
    "            accuracies = [tm['accuracy'] for tm in threshold_metrics]\n",
    "            precisions = [tm['precision'] for tm in threshold_metrics]\n",
    "            recalls = [tm['recall'] for tm in threshold_metrics]\n",
    "            f1s = [tm['f1'] for tm in threshold_metrics]\n",
    "            mccs = [tm['mcc'] for tm in threshold_metrics]\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            avg_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies)\n",
    "            avg_precision = np.mean(precisions)\n",
    "            std_precision = np.std(precisions)\n",
    "            avg_recall = np.mean(recalls)\n",
    "            std_recall = np.std(recalls)\n",
    "            avg_f1 = np.mean(f1s)\n",
    "            std_f1 = np.std(f1s)\n",
    "            avg_mcc = np.mean(mccs)\n",
    "            std_mcc = np.std(mccs)\n",
    "            \n",
    "            # Store the metrics along with parameters and threshold\n",
    "            metrics_per_combination.append({\n",
    "                **param_dict,\n",
    "                'threshold': threshold,\n",
    "                'accuracy': avg_accuracy,\n",
    "                'std_accuracy': std_accuracy,\n",
    "                'precision': avg_precision,\n",
    "                'std_precision': std_precision,\n",
    "                'recall': avg_recall,\n",
    "                'std_recall': std_recall,\n",
    "                'f1': avg_f1,\n",
    "                'std_f1': std_f1,\n",
    "                'mcc': avg_mcc,\n",
    "                'std_mcc': std_mcc\n",
    "            })\n",
    "    \n",
    "    # Now, find the parameter combination with the best F1 score\n",
    "    best_result = max(metrics_per_combination, key=lambda x: x['f1'])\n",
    "    \n",
    "    # Save the results to CSV\n",
    "    df_metrics = pd.DataFrame(metrics_per_combination)\n",
    "    outFile_metrics = os.path.join(outDir, f\"knn-smote-threshold-results.csv\")\n",
    "    df_metrics.to_csv(outFile_metrics, index=False)\n",
    "    \n",
    "    print(f\"Results saved to: {outFile_metrics}\")\n",
    "    \n",
    "    # Extract the best parameters, threshold, and metrics\n",
    "    best_params = {key: best_result[key] for key in param_keys}\n",
    "    best_threshold = best_result['threshold']\n",
    "    best_f1 = best_result['f1']\n",
    "    std_f1 = best_result['std_f1']\n",
    "    final_mcc = best_result['mcc']\n",
    "    std_mcc = best_result['std_mcc']\n",
    "    \n",
    "    print(\"\\nBest Parameters, Threshold, and Metrics:\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score: {best_f1} (Std Dev: {std_f1})\")\n",
    "    print(f\"Final MCC: {final_mcc} (Std Dev: {std_mcc})\")\n",
    "    \n",
    "    return best_params, best_threshold, best_f1, std_f1, final_mcc, std_mcc\n",
    "\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "outDir = \"results/hybrid\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run KNN with SMOTE, PCA, and Threshold adjustment using 5-fold cross-validation\n",
    "print(\"\\nStarting KNN analysis with SMOTE, PCA, and Threshold adjustment for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_threshold_5folds, best_f1_5folds, std_f1_5folds, final_mcc_5folds, std_mcc_5folds = KNNwithSMOTEThreshold(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for KNN with SMOTE, PCA, and Threshold adjustment 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best Threshold: {best_threshold_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds} (Std Dev: {std_f1_5folds})\")\n",
    "print(f\"Final MCC: {final_mcc_5folds} (Std Dev: {std_mcc_5folds})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdlyyndUW-0b"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qVQMEQqXAEk",
    "outputId": "a6abde80-dd22-405a-ab92-0267a323a5ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SVM analysis with SMOTE, PCA, and Threshold adjustment for 5-fold cross-validation...\n",
      "Results saved to: results/hybrid\\svm-smote-threshold-results.csv\n",
      "\n",
      "Best Parameters, Threshold, and Metrics:\n",
      "Best Parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best Threshold: 0.1\n",
      "Best F1 Score: 0.7191387559808613 (Std Dev: 0.05468294599571738)\n",
      "Final MCC: 0.6671247721541482 (Std Dev: 0.06638462910356159)\n",
      "\n",
      "Best results for SVM with SMOTE, PCA, and Threshold adjustment 5-fold cross-validation:\n",
      "Best Parameters: {'pca__n_components': 180, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best Threshold: 0.1\n",
      "Best F1 Score: 0.7191387559808613 (Std Dev: 0.05468294599571738)\n",
      "Final MCC: 0.6671247721541482 (Std Dev: 0.06638462910356159)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json  # Import json to load best hyperparameters\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import product\n",
    "\n",
    "def SVM_SMOTE_Threshold(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "    \n",
    "    # Load best hyperparameters from JSON file\n",
    "    with open(os.path.join(best_hyperparameter, 'best_params_svm.json'), 'r') as f:\n",
    "        best_params_svm = json.load(f)\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'pca__n_components': [best_params_svm['pca__n_components']],          # PCA components\n",
    "        'svm__C': [best_params_svm['svm__C']],                    # Regularization parameter\n",
    "        'svm__kernel': [best_params_svm['svm__kernel']]            # Kernel types\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    \n",
    "    # Prepare to store metrics\n",
    "    metrics_per_combination = []\n",
    "    \n",
    "    # Define thresholds to evaluate\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "    \n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Loop over each hyperparameter combination\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        \n",
    "        # Initialize lists to store metrics per threshold\n",
    "        threshold_metrics_list = []\n",
    "        \n",
    "        # For each fold in cross-validation\n",
    "        for fold_idx, (train_index, test_index) in enumerate(skf.split(dataPoints, dataLabelsList)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train = [dataLabelsList[i] for i in train_index]\n",
    "            y_test = [dataLabelsList[i] for i in test_index]\n",
    "            \n",
    "            # Define a pipeline with CountVectorizer, PCA, SMOTE, and SVM with current params\n",
    "            pipeline = ImbPipeline([\n",
    "                ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "                ('svm', SVC(\n",
    "                    C=param_dict['svm__C'],\n",
    "                    kernel=param_dict['svm__kernel'],\n",
    "                    probability=True,\n",
    "                    random_state=42))\n",
    "            ])\n",
    "            \n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Iterate over thresholds\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "                \n",
    "                # Store fold metrics\n",
    "                threshold_metrics_list.append({\n",
    "                    **param_dict,\n",
    "                    'threshold': threshold,\n",
    "                    'fold': fold_idx + 1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "        \n",
    "        # Calculate average metrics and standard deviations over folds for each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Filter metrics for the current threshold\n",
    "            threshold_metrics = [tm for tm in threshold_metrics_list if tm['threshold'] == threshold]\n",
    "            \n",
    "            # Extract metrics per fold\n",
    "            accuracies = [tm['accuracy'] for tm in threshold_metrics]\n",
    "            precisions = [tm['precision'] for tm in threshold_metrics]\n",
    "            recalls = [tm['recall'] for tm in threshold_metrics]\n",
    "            f1s = [tm['f1'] for tm in threshold_metrics]\n",
    "            mccs = [tm['mcc'] for tm in threshold_metrics]\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            avg_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies)\n",
    "            avg_precision = np.mean(precisions)\n",
    "            std_precision = np.std(precisions)\n",
    "            avg_recall = np.mean(recalls)\n",
    "            std_recall = np.std(recalls)\n",
    "            avg_f1 = np.mean(f1s)\n",
    "            std_f1 = np.std(f1s)\n",
    "            avg_mcc = np.mean(mccs)\n",
    "            std_mcc = np.std(mccs)\n",
    "            \n",
    "            # Store the metrics along with parameters and threshold\n",
    "            metrics_per_combination.append({\n",
    "                **param_dict,\n",
    "                'threshold': threshold,\n",
    "                'accuracy': avg_accuracy,\n",
    "                'std_accuracy': std_accuracy,\n",
    "                'precision': avg_precision,\n",
    "                'std_precision': std_precision,\n",
    "                'recall': avg_recall,\n",
    "                'std_recall': std_recall,\n",
    "                'f1': avg_f1,\n",
    "                'std_f1': std_f1,\n",
    "                'mcc': avg_mcc,\n",
    "                'std_mcc': std_mcc\n",
    "            })\n",
    "    \n",
    "    # Now, find the parameter combination with the best F1 score\n",
    "    best_result = max(metrics_per_combination, key=lambda x: x['f1'])\n",
    "    \n",
    "    # Save the results to CSV\n",
    "    df_metrics = pd.DataFrame(metrics_per_combination)\n",
    "    outFile_metrics = os.path.join(outDir, f\"svm-smote-threshold-results.csv\")\n",
    "    df_metrics.to_csv(outFile_metrics, index=False)\n",
    "    \n",
    "    print(f\"Results saved to: {outFile_metrics}\")\n",
    "    \n",
    "    # Extract the best parameters, threshold, and metrics\n",
    "    best_params = {key: best_result[key] for key in param_keys}\n",
    "    best_threshold = best_result['threshold']\n",
    "    best_f1 = best_result['f1']\n",
    "    std_f1 = best_result['std_f1']\n",
    "    final_mcc = best_result['mcc']\n",
    "    std_mcc = best_result['std_mcc']\n",
    "    \n",
    "    print(\"\\nBest Parameters, Threshold, and Metrics:\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score: {best_f1} (Std Dev: {std_f1})\")\n",
    "    print(f\"Final MCC: {final_mcc} (Std Dev: {std_mcc})\")\n",
    "    \n",
    "    return best_params, best_threshold, best_f1, std_f1, final_mcc, std_mcc\n",
    "\n",
    "# Main Execution\n",
    "outDir = \"results/hybrid\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run SVM with SMOTE, PCA, and Threshold adjustment using 5-fold cross-validation\n",
    "print(\"\\nStarting SVM analysis with SMOTE, PCA, and Threshold adjustment for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_threshold_5folds, best_f1_5folds, std_f1_5folds, final_mcc_5folds, std_mcc_5folds = SVM_SMOTE_Threshold(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for SVM with SMOTE, PCA, and Threshold adjustment 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best Threshold: {best_threshold_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds} (Std Dev: {std_f1_5folds})\")\n",
    "print(f\"Final MCC: {final_mcc_5folds} (Std Dev: {std_mcc_5folds})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsT3CWD_bxWg"
   },
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npy2L6bqbydZ",
    "outputId": "a35ec835-af6a-4092-e877-4f0c41cc1e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGBoost analysis with SMOTE, PCA, and Threshold adjustment for 5-fold cross-validation...\n",
      "Results saved to: results/hybrid\\xgb-smote-pca-threshold-results.csv\n",
      "\n",
      "Best Parameters, Threshold, and Metrics:\n",
      "Best Parameters: {'xgb__n_estimators': 200, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.01}\n",
      "Best Threshold: 0.5\n",
      "Best F1 Score: 0.8839 ± 0.1068\n",
      "Final MCC: 0.8765 ± 0.1051\n",
      "\n",
      "Best results for XGBoost with SMOTE, PCA, and Threshold adjustment 5-fold cross-validation:\n",
      "Best Parameters: {'xgb__n_estimators': 200, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.01}\n",
      "Best Threshold: 0.5\n",
      "Best F1 Score: 0.8839 ± 0.1068\n",
      "Final MCC: 0.8765 ± 0.1051\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import product\n",
    "\n",
    "def runXGBWithSMOTE_PCA(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "    with open(os.path.join(best_hyperparameter, 'best_params_xgb.json'), 'r') as f:\n",
    "        best_params_xgb = json.load(f)\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'xgb__n_estimators': [best_params_xgb['xgb__n_estimators']],     # Number of boosting rounds\n",
    "        'xgb__max_depth': [best_params_xgb['xgb__max_depth']],           # Maximum depth of a tree\n",
    "        'xgb__learning_rate': [best_params_xgb['xgb__learning_rate']],   # Learning rate\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    \n",
    "    # Prepare to store metrics\n",
    "    metrics_per_combination = []\n",
    "    \n",
    "    # Define thresholds to evaluate\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "    \n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Loop over each hyperparameter combination\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        \n",
    "        # Initialize lists to store metrics per threshold\n",
    "        threshold_metrics_list = []\n",
    "        \n",
    "        # For each fold in cross-validation\n",
    "        for fold_idx, (train_index, test_index) in enumerate(skf.split(dataPoints, dataLabelsList)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train = [dataLabelsList[i] for i in train_index]\n",
    "            y_test = [dataLabelsList[i] for i in test_index]\n",
    "            \n",
    "            # Define a pipeline with CountVectorizer, PCA, SMOTE, and XGBClassifier with current params\n",
    "            pipeline = ImbPipeline([\n",
    "                ('vectorizer', CountVectorizer(stop_words=None)),                                \n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('xgb', XGBClassifier(\n",
    "                    n_estimators=param_dict['xgb__n_estimators'],\n",
    "                    max_depth=param_dict['xgb__max_depth'],\n",
    "                    learning_rate=param_dict['xgb__learning_rate'],\n",
    "                    eval_metric='logloss',\n",
    "                    use_label_encoder=False,\n",
    "                    random_state=42))\n",
    "            ])\n",
    "            \n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Iterate over thresholds\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "                \n",
    "                # Store fold metrics\n",
    "                threshold_metrics_list.append({\n",
    "                    **param_dict,\n",
    "                    'threshold': threshold,\n",
    "                    'fold': fold_idx + 1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "        \n",
    "        # Calculate average metrics and standard deviations over folds for each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Filter metrics for the current threshold\n",
    "            threshold_metrics = [tm for tm in threshold_metrics_list if tm['threshold'] == threshold]\n",
    "            \n",
    "            # Extract metrics per fold\n",
    "            accuracies = [tm['accuracy'] for tm in threshold_metrics]\n",
    "            precisions = [tm['precision'] for tm in threshold_metrics]\n",
    "            recalls = [tm['recall'] for tm in threshold_metrics]\n",
    "            f1s = [tm['f1'] for tm in threshold_metrics]\n",
    "            mccs = [tm['mcc'] for tm in threshold_metrics]\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            avg_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "            avg_precision = np.mean(precisions)\n",
    "            std_precision = np.std(precisions, ddof=1)\n",
    "            avg_recall = np.mean(recalls)\n",
    "            std_recall = np.std(recalls, ddof=1)\n",
    "            avg_f1 = np.mean(f1s)\n",
    "            std_f1 = np.std(f1s, ddof=1)\n",
    "            avg_mcc = np.mean(mccs)\n",
    "            std_mcc = np.std(mccs, ddof=1)\n",
    "            \n",
    "            # Store the metrics along with parameters and threshold\n",
    "            metrics_per_combination.append({\n",
    "                **param_dict,\n",
    "                'threshold': threshold,\n",
    "                'accuracy': avg_accuracy,\n",
    "                'std_accuracy': std_accuracy,\n",
    "                'precision': avg_precision,\n",
    "                'std_precision': std_precision,\n",
    "                'recall': avg_recall,\n",
    "                'std_recall': std_recall,\n",
    "                'f1': avg_f1,\n",
    "                'std_f1': std_f1,\n",
    "                'mcc': avg_mcc,\n",
    "                'std_mcc': std_mcc\n",
    "            })\n",
    "    \n",
    "    # Now, find the parameter combination with the best F1 score\n",
    "    best_result = max(metrics_per_combination, key=lambda x: x['f1'])\n",
    "    \n",
    "    # Save the results to CSV\n",
    "    df_metrics = pd.DataFrame(metrics_per_combination)\n",
    "    outFile_metrics = os.path.join(outDir, f\"xgb-smote-pca-threshold-results.csv\")\n",
    "    df_metrics.to_csv(outFile_metrics, index=False)\n",
    "    \n",
    "    print(f\"Results saved to: {outFile_metrics}\")\n",
    "    \n",
    "    # Extract the best parameters, threshold, and metrics\n",
    "    best_params = {key: best_result[key] for key in param_keys}\n",
    "    best_threshold = best_result['threshold']\n",
    "    best_f1 = best_result['f1']\n",
    "    std_f1 = best_result['std_f1']\n",
    "    final_mcc = best_result['mcc']\n",
    "    std_mcc = best_result['std_mcc']\n",
    "    \n",
    "    print(\"\\nBest Parameters, Threshold, and Metrics:\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score: {best_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(f\"Final MCC: {final_mcc:.4f} ± {std_mcc:.4f}\")\n",
    "    \n",
    "    return best_params, best_threshold, best_f1, std_f1, final_mcc, std_mcc\n",
    "\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "outDir = \"results/hybrid\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run XGBoost with SMOTE, PCA, and Threshold adjustment using 5-fold cross-validation\n",
    "print(\"\\nStarting XGBoost analysis with SMOTE, PCA, and Threshold adjustment for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_threshold_5folds, best_f1_5folds, std_f1_5folds, final_mcc_5folds, std_mcc_5folds = runXGBWithSMOTE_PCA(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost with SMOTE, PCA, and Threshold adjustment 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best Threshold: {best_threshold_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds:.4f} ± {std_f1_5folds:.4f}\")\n",
    "print(f\"Final MCC: {final_mcc_5folds:.4f} ± {std_mcc_5folds:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McHkV0aFx4sL"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfYjJdp_x7le",
    "outputId": "43462ef3-896b-40be-89bb-a00cdc545308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Random Forest analysis with SMOTE and Threshold adjustment for 5-fold cross-validation...\n",
      "Results saved to: results/hybrid\\rf-smote-threshold-results.csv\n",
      "\n",
      "Best Parameters, Threshold, and Metrics:\n",
      "Best Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Best Threshold: 0.5\n",
      "Best F1 Score: 0.822015823873409 (Std Dev: 0.04193302466637172)\n",
      "Final MCC: 0.7898138504372108 (Std Dev: 0.04984274546216977)\n",
      "\n",
      "Best results for Random Forest with SMOTE and Threshold adjustment 5-fold cross-validation:\n",
      "Best Parameters: {'rf__n_estimators': 100, 'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__criterion': 'entropy'}\n",
      "Best Threshold: 0.5\n",
      "Best F1 Score: 0.822015823873409 (Std Dev: 0.04193302466637172)\n",
      "Final MCC: 0.7898138504372108 (Std Dev: 0.04984274546216977)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json  # Import json to load best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import product\n",
    "\n",
    "def runRFWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "    with open(os.path.join(\"results/best_hyperparameter\", 'best_params_rf.json'), 'r') as f:\n",
    "        best_params_rf = json.load(f)\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'rf__n_estimators': [best_params_rf['rf__n_estimators']],          # Number of trees in the forest\n",
    "        'rf__max_depth': [best_params_rf['rf__max_depth']],             # Maximum depth of the tree\n",
    "        'rf__min_samples_split': [best_params_rf['rf__min_samples_split']],      # Minimum number of samples required to split a node\n",
    "        'rf__min_samples_leaf': [best_params_rf['rf__min_samples_leaf']],       # Minimum number of samples required at a leaf node\n",
    "        'rf__criterion': [best_params_rf['rf__criterion']]         # Function to measure the quality of a split\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    \n",
    "    # Prepare to store metrics\n",
    "    metrics_per_combination = []\n",
    "    \n",
    "    # Define thresholds to evaluate\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "    \n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Loop over each hyperparameter combination\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        \n",
    "        # Initialize lists to store metrics per threshold\n",
    "        threshold_metrics_list = []\n",
    "        \n",
    "        # For each fold in cross-validation\n",
    "        for fold_idx, (train_index, test_index) in enumerate(skf.split(dataPoints, dataLabelsList)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train = [dataLabelsList[i] for i in train_index]\n",
    "            y_test = [dataLabelsList[i] for i in test_index]\n",
    "            \n",
    "            # Define a pipeline with CountVectorizer, SMOTE, and Random Forest with current params\n",
    "            pipeline = ImbPipeline([\n",
    "                ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('rf', RandomForestClassifier(\n",
    "                    n_estimators=param_dict['rf__n_estimators'],\n",
    "                    max_depth=param_dict['rf__max_depth'],\n",
    "                    min_samples_split=param_dict['rf__min_samples_split'],\n",
    "                    min_samples_leaf=param_dict['rf__min_samples_leaf'],\n",
    "                    criterion=param_dict['rf__criterion'],\n",
    "                    random_state=42))\n",
    "            ])\n",
    "            \n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Iterate over thresholds\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "                \n",
    "                # Store fold metrics\n",
    "                threshold_metrics_list.append({\n",
    "                    **param_dict,\n",
    "                    'threshold': threshold,\n",
    "                    'fold': fold_idx + 1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "        \n",
    "        # Calculate average metrics and standard deviations over folds for each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Filter metrics for the current threshold\n",
    "            threshold_metrics = [tm for tm in threshold_metrics_list if tm['threshold'] == threshold]\n",
    "            \n",
    "            # Extract metrics per fold\n",
    "            accuracies = [tm['accuracy'] for tm in threshold_metrics]\n",
    "            precisions = [tm['precision'] for tm in threshold_metrics]\n",
    "            recalls = [tm['recall'] for tm in threshold_metrics]\n",
    "            f1s = [tm['f1'] for tm in threshold_metrics]\n",
    "            mccs = [tm['mcc'] for tm in threshold_metrics]\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            avg_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies)\n",
    "            avg_precision = np.mean(precisions)\n",
    "            std_precision = np.std(precisions)\n",
    "            avg_recall = np.mean(recalls)\n",
    "            std_recall = np.std(recalls)\n",
    "            avg_f1 = np.mean(f1s)\n",
    "            std_f1 = np.std(f1s)\n",
    "            avg_mcc = np.mean(mccs)\n",
    "            std_mcc = np.std(mccs)\n",
    "            \n",
    "            # Store the metrics along with parameters and threshold\n",
    "            metrics_per_combination.append({\n",
    "                **param_dict,\n",
    "                'threshold': threshold,\n",
    "                'accuracy': avg_accuracy,\n",
    "                'std_accuracy': std_accuracy,\n",
    "                'precision': avg_precision,\n",
    "                'std_precision': std_precision,\n",
    "                'recall': avg_recall,\n",
    "                'std_recall': std_recall,\n",
    "                'f1': avg_f1,\n",
    "                'std_f1': std_f1,\n",
    "                'mcc': avg_mcc,\n",
    "                'std_mcc': std_mcc\n",
    "            })\n",
    "    \n",
    "    # Now, find the parameter combination with the best F1 score\n",
    "    best_result = max(metrics_per_combination, key=lambda x: x['f1'])\n",
    "    \n",
    "    # Save the results to CSV\n",
    "    df_metrics = pd.DataFrame(metrics_per_combination)\n",
    "    outFile_metrics = os.path.join(outDir, f\"rf-smote-threshold-results.csv\")\n",
    "    df_metrics.to_csv(outFile_metrics, index=False)\n",
    "    \n",
    "    print(f\"Results saved to: {outFile_metrics}\")\n",
    "    \n",
    "    # Extract the best parameters, threshold, and metrics\n",
    "    best_params = {key: best_result[key] for key in param_keys}\n",
    "    best_threshold = best_result['threshold']\n",
    "    best_f1 = best_result['f1']\n",
    "    std_f1 = best_result['std_f1']\n",
    "    final_mcc = best_result['mcc']\n",
    "    std_mcc = best_result['std_mcc']\n",
    "    \n",
    "    print(\"\\nBest Parameters, Threshold, and Metrics:\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score: {best_f1} (Std Dev: {std_f1})\")\n",
    "    print(f\"Final MCC: {final_mcc} (Std Dev: {std_mcc})\")\n",
    "    \n",
    "    return best_params, best_threshold, best_f1, std_f1, final_mcc, std_mcc\n",
    "\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "outDir = \"results/hybrid\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Random Forest with SMOTE and Threshold adjustment using 5-fold cross-validation\n",
    "print(\"\\nStarting Random Forest analysis with SMOTE and Threshold adjustment for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_threshold_5folds, best_f1_5folds, std_f1_5folds, final_mcc_5folds, std_mcc_5folds = runRFWithSMOTE(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for Random Forest with SMOTE and Threshold adjustment 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best Threshold: {best_threshold_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds} (Std Dev: {std_f1_5folds})\")\n",
    "print(f\"Final MCC: {final_mcc_5folds} (Std Dev: {std_mcc_5folds})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6_eoOI9ycjk"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoFWUzGmyecK",
    "outputId": "985059d6-3723-44cc-cce9-8aea26dca652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Decision Tree analysis with SMOTE and Threshold adjustment for 5-fold cross-validation...\n",
      "Results saved to: results/hybrid\\dt-smote-threshold-results-5-folds.csv\n",
      "\n",
      "Best Parameters, Threshold, and Metrics:\n",
      "Best Parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Best Threshold: 0.1\n",
      "Best F1 Score: 0.8760130718954249 (Std Dev: 0.09104075452318809)\n",
      "Final MCC: 0.8639675781078721 (Std Dev: 0.09751197920410426)\n",
      "\n",
      "Best results for Decision Tree with SMOTE and Threshold adjustment 5-fold cross-validation:\n",
      "Best Parameters: {'dt__max_depth': 10, 'dt__min_samples_split': 5, 'dt__min_samples_leaf': 2, 'dt__criterion': 'gini', 'dt__max_features': None}\n",
      "Best Threshold: 0.1\n",
      "Best F1 Score: 0.8760130718954249 (Std Dev: 0.09104075452318809)\n",
      "Final MCC: 0.8639675781078721 (Std Dev: 0.09751197920410426)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json  # Import json to load best hyperparameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import product\n",
    "\n",
    "def runDTWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "    with open(os.path.join(\"results/best_hyperparameter\", 'best_params_dt.json'), 'r') as f:\n",
    "        best_params_dt = json.load(f)\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'dt__max_depth': [best_params_dt['dt__max_depth']],\n",
    "        'dt__min_samples_split': [best_params_dt['dt__min_samples_split']],\n",
    "        'dt__min_samples_leaf': [best_params_dt['dt__min_samples_leaf']],\n",
    "        'dt__criterion': [best_params_dt['dt__criterion']],\n",
    "        'dt__max_features': [best_params_dt['dt__max_features']]\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    \n",
    "    # Prepare to store metrics\n",
    "    metrics_per_combination = []\n",
    "    \n",
    "    # Define thresholds to evaluate\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "    \n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Loop over each hyperparameter combination\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "        \n",
    "        # Initialize lists to store metrics per threshold\n",
    "        threshold_metrics_list = []\n",
    "        \n",
    "        # For each fold in cross-validation\n",
    "        for fold_idx, (train_index, test_index) in enumerate(skf.split(dataPoints, dataLabelsList)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train = [dataLabelsList[i] for i in train_index]\n",
    "            y_test = [dataLabelsList[i] for i in test_index]\n",
    "            \n",
    "            # Define a pipeline with CountVectorizer, SMOTE, and Decision Tree with current params\n",
    "            pipeline = ImbPipeline([\n",
    "                ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('dt', DecisionTreeClassifier(\n",
    "                    max_depth=param_dict['dt__max_depth'],\n",
    "                    min_samples_split=param_dict['dt__min_samples_split'],\n",
    "                    min_samples_leaf=param_dict['dt__min_samples_leaf'],\n",
    "                    criterion=param_dict['dt__criterion'],\n",
    "                    max_features=param_dict['dt__max_features'],\n",
    "                    random_state=42))\n",
    "            ])\n",
    "            \n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Iterate over thresholds\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "                \n",
    "                # Store fold metrics\n",
    "                threshold_metrics_list.append({\n",
    "                    **param_dict,\n",
    "                    'threshold': threshold,\n",
    "                    'fold': fold_idx + 1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "        \n",
    "        # Calculate average metrics and standard deviations over folds for each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Filter metrics for the current threshold\n",
    "            threshold_metrics = [tm for tm in threshold_metrics_list if tm['threshold'] == threshold]\n",
    "            \n",
    "            # Extract metrics per fold\n",
    "            accuracies = [tm['accuracy'] for tm in threshold_metrics]\n",
    "            precisions = [tm['precision'] for tm in threshold_metrics]\n",
    "            recalls = [tm['recall'] for tm in threshold_metrics]\n",
    "            f1s = [tm['f1'] for tm in threshold_metrics]\n",
    "            mccs = [tm['mcc'] for tm in threshold_metrics]\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            avg_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies)\n",
    "            avg_precision = np.mean(precisions)\n",
    "            std_precision = np.std(precisions)\n",
    "            avg_recall = np.mean(recalls)\n",
    "            std_recall = np.std(recalls)\n",
    "            avg_f1 = np.mean(f1s)\n",
    "            std_f1 = np.std(f1s)\n",
    "            avg_mcc = np.mean(mccs)\n",
    "            std_mcc = np.std(mccs)\n",
    "            \n",
    "            # Store the metrics along with parameters and threshold\n",
    "            metrics_per_combination.append({\n",
    "                **param_dict,\n",
    "                'threshold': threshold,\n",
    "                'accuracy': avg_accuracy,\n",
    "                'std_accuracy': std_accuracy,\n",
    "                'precision': avg_precision,\n",
    "                'std_precision': std_precision,\n",
    "                'recall': avg_recall,\n",
    "                'std_recall': std_recall,\n",
    "                'f1': avg_f1,\n",
    "                'std_f1': std_f1,\n",
    "                'mcc': avg_mcc,\n",
    "                'std_mcc': std_mcc\n",
    "            })\n",
    "    \n",
    "    # Now, find the parameter combination with the best F1 score\n",
    "    best_result = max(metrics_per_combination, key=lambda x: x['f1'])\n",
    "    \n",
    "    # Save the results to CSV\n",
    "    df_metrics = pd.DataFrame(metrics_per_combination)\n",
    "    outFile_metrics = os.path.join(outDir, f\"dt-smote-threshold-results-{n_splits}-folds.csv\")\n",
    "    df_metrics.to_csv(outFile_metrics, index=False)\n",
    "    \n",
    "    print(f\"Results saved to: {outFile_metrics}\")\n",
    "    \n",
    "    # Extract the best parameters, threshold, and metrics\n",
    "    best_params = {key: best_result[key] for key in param_keys}\n",
    "    best_threshold = best_result['threshold']\n",
    "    best_f1 = best_result['f1']\n",
    "    std_f1 = best_result['std_f1']\n",
    "    final_mcc = best_result['mcc']\n",
    "    std_mcc = best_result['std_mcc']\n",
    "    \n",
    "    print(\"\\nBest Parameters, Threshold, and Metrics:\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score: {best_f1} (Std Dev: {std_f1})\")\n",
    "    print(f\"Final MCC: {final_mcc} (Std Dev: {std_mcc})\")\n",
    "    \n",
    "    return best_params, best_threshold, best_f1, std_f1, final_mcc, std_mcc\n",
    "\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "outDir = \"results/hybrid\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Decision Tree with SMOTE and Threshold adjustment using 5-fold cross-validation\n",
    "print(\"\\nStarting Decision Tree analysis with SMOTE and Threshold adjustment for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_threshold_5folds, best_f1_5folds, std_f1_5folds, final_mcc_5folds, std_mcc_5folds = runDTWithSMOTE(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for Decision Tree with SMOTE and Threshold adjustment 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best Threshold: {best_threshold_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds} (Std Dev: {std_f1_5folds})\")\n",
    "print(f\"Final MCC: {final_mcc_5folds} (Std Dev: {std_mcc_5folds})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Obxhsyp84Ib8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
