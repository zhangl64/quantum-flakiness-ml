{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0cedc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Naive Bayes analysis for flaky vs smaller non-flaky files (47 each)...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 47\n",
      "Total number of documents: 94\n",
      "Training with parameters: {'alpha': 0.001}\n",
      "Training with parameters: {'alpha': 0.01}\n",
      "Training with parameters: {'alpha': 0.1}\n",
      "Training with parameters: {'alpha': 1.0}\n",
      "Training with parameters: {'alpha': 10.0}\n",
      "Naive Bayes analysis completed. Results saved to: equal-params-nb-5-folds-Threshold-allKfold-wogridsearch.csv\n",
      "Best results for 5-fold on equal combination:\n",
      "      alpha  fold  threshold  accuracy  precision    recall        f1  \\\n",
      "0     0.001     1        0.1  0.736842   0.857143  0.600000  0.705882   \n",
      "1     0.001     1        0.2  0.736842   0.857143  0.600000  0.705882   \n",
      "2     0.001     1        0.3  0.736842   0.857143  0.600000  0.705882   \n",
      "3     0.001     1        0.4  0.736842   0.857143  0.600000  0.705882   \n",
      "4     0.001     1        0.5  0.736842   0.857143  0.600000  0.705882   \n",
      "..      ...   ...        ...       ...        ...       ...       ...   \n",
      "220  10.000     5        0.5  0.722222   0.666667  0.888889  0.761905   \n",
      "221  10.000     5        0.6  0.722222   0.666667  0.888889  0.761905   \n",
      "222  10.000     5        0.7  0.777778   0.727273  0.888889  0.800000   \n",
      "223  10.000     5        0.8  0.777778   0.727273  0.888889  0.800000   \n",
      "224  10.000     5        0.9  0.777778   0.727273  0.888889  0.800000   \n",
      "\n",
      "          mcc  \n",
      "0    0.506048  \n",
      "1    0.506048  \n",
      "2    0.506048  \n",
      "3    0.506048  \n",
      "4    0.506048  \n",
      "..        ...  \n",
      "220  0.471405  \n",
      "221  0.471405  \n",
      "222  0.569803  \n",
      "223  0.569803  \n",
      "224  0.569803  \n",
      "\n",
      "[225 rows x 8 columns]\n",
      "Starting Naive Bayes analysis for flaky vs larger non-flaky files...\n",
      "Number of flaky documents: 47\n",
      "Number of non-flaky documents: 254\n",
      "Total number of documents: 301\n",
      "Training with parameters: {'alpha': 0.001}\n",
      "Training with parameters: {'alpha': 0.01}\n",
      "Training with parameters: {'alpha': 0.1}\n",
      "Training with parameters: {'alpha': 1.0}\n",
      "Training with parameters: {'alpha': 10.0}\n",
      "Naive Bayes analysis completed. Results saved to: larger-params-nb-5-folds-Threshold-allKfold-wogridsearch.csv\n",
      "Best results for 5-fold on larger combination:\n",
      "      alpha  fold  threshold  accuracy  precision    recall        f1  \\\n",
      "0     0.001     1        0.1  0.918033       1.00  0.500000  0.666667   \n",
      "1     0.001     1        0.2  0.918033       1.00  0.500000  0.666667   \n",
      "2     0.001     1        0.3  0.918033       1.00  0.500000  0.666667   \n",
      "3     0.001     1        0.4  0.918033       1.00  0.500000  0.666667   \n",
      "4     0.001     1        0.5  0.918033       1.00  0.500000  0.666667   \n",
      "..      ...   ...        ...       ...        ...       ...       ...   \n",
      "220  10.000     5        0.5  0.883333       0.75  0.333333  0.461538   \n",
      "221  10.000     5        0.6  0.883333       0.75  0.333333  0.461538   \n",
      "222  10.000     5        0.7  0.883333       0.75  0.333333  0.461538   \n",
      "223  10.000     5        0.8  0.883333       0.75  0.333333  0.461538   \n",
      "224  10.000     5        0.9  0.883333       0.75  0.333333  0.461538   \n",
      "\n",
      "          mcc  \n",
      "0    0.674802  \n",
      "1    0.674802  \n",
      "2    0.674802  \n",
      "3    0.674802  \n",
      "4    0.674802  \n",
      "..        ...  \n",
      "220  0.449089  \n",
      "221  0.449089  \n",
      "222  0.449089  \n",
      "223  0.449089  \n",
      "224  0.449089  \n",
      "\n",
      "[225 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from itertools import product\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints, dim=0, eps=0.3):\n",
    "    \"\"\"Performs vectorization using CountVectorizer with optional dimensionality reduction.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z_full = countVec.fit_transform(dataPoints)\n",
    "    if eps == 0:\n",
    "        Z = Z_full\n",
    "    else:\n",
    "        if dim <= 0:\n",
    "            from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "            dim = johnson_lindenstrauss_min_dim(Z_full.shape[0], eps=eps)\n",
    "        srp = SparseRandomProjection(n_components=dim)\n",
    "        Z = srp.fit_transform(Z_full)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Manual Grid Search Naive Bayes with Cross-Validation\n",
    "\n",
    "def flastThreshold(outDir, flakyZip, nonFlakyZip, extractDir, n_splits, dim, eps, combination_label, param_grid):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Extract the zip files\n",
    "    flakyDir = os.path.join(extractDir, 'flaky')\n",
    "    nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "    os.makedirs(flakyDir, exist_ok=True)\n",
    "    os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "    \n",
    "    extract_zip(flakyZip, flakyDir)\n",
    "    extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "    dataPointsFlaky = getDataPoints(flakyDir)\n",
    "    dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "    dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "    print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "    print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "    print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "    \n",
    "    if len(dataPoints) == 0:\n",
    "        raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "    # Vectorization without dimensionality reduction\n",
    "    Z = flastVectorization(dataPoints, dim=0, eps=0)\n",
    "    dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "    vecTime = time.perf_counter() - v0\n",
    "\n",
    "    # Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize storage for metrics per fold and threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    # Manually iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Convert params from tuple to dictionary\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {param_dict}\")\n",
    "        \n",
    "        # Initialize MultinomialNB with current hyperparameter combination\n",
    "        nb_model = MultinomialNB(\n",
    "            alpha=param_dict['alpha']\n",
    "        )\n",
    "        \n",
    "        # Cross-validation\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(Z, dataLabelsList)):\n",
    "            X_train, X_test = Z[train_index], Z[test_index]\n",
    "            y_train, y_test = dataLabelsList[train_index], dataLabelsList[test_index]\n",
    "\n",
    "            if sum(y_train) == 0 or sum(y_test) == 0:\n",
    "                print(f\"Skipping fold {fold+1} due to no positive samples in train or test set\")\n",
    "                continue\n",
    "\n",
    "            # Train the model\n",
    "            nb_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities on test set\n",
    "            y_pred_proba = nb_model.predict_proba(X_test)\n",
    "\n",
    "            # Calculate metrics for each threshold\n",
    "            for threshold in thresholds:\n",
    "                y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics for this threshold\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "                metrics_per_combination.append({\n",
    "                    'alpha': param_dict['alpha'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    if len(metrics_per_combination) == 0:\n",
    "        print(\"No valid folds. Exiting.\")\n",
    "        return param_dict, None\n",
    "\n",
    "    # Save the results for each combination, threshold, and fold\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    outFile = f\"{combination_label}-params-nb-{n_splits}-folds-Threshold-allKfold-wogridsearch.csv\"\n",
    "    df_results.to_csv(os.path.join(outDir, outFile), index=False)\n",
    "\n",
    "    print(f\"Naive Bayes analysis completed. Results saved to: {outFile}\")\n",
    "    return param_dict, df_results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],  # Additive smoothing parameter\n",
    "    }\n",
    "\n",
    "    # Parameters setup for the first combination\n",
    "    flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "    nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "    largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "    # Create separate result directories for equal and larger non-flaky combinations\n",
    "    outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "    outDirLarger = \"results/larger_nonflaky/\"\n",
    "    os.makedirs(outDirEqual, exist_ok=True)\n",
    "    os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "    # Create separate extract directories for each combination to avoid file confusion\n",
    "    extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "    extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "    os.makedirs(extractDirEqual, exist_ok=True)\n",
    "    os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "    # Perform Naive Bayes analysis for the first combination (flaky vs smaller non-flaky)\n",
    "    print(\"Starting Naive Bayes analysis for flaky vs smaller non-flaky files (47 each)...\")\n",
    "    best_params_5folds_1, df_results_5folds_1 = flastThreshold(\n",
    "        outDirEqual, flakyZip, nonFlakyZip, extractDirEqual, 5, dim=100, eps=0.3, combination_label=\"equal\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on equal combination:\")\n",
    "    print(df_results_5folds_1)\n",
    "\n",
    "    # Perform Naive Bayes analysis for the second combination (flaky vs larger non-flaky)\n",
    "    print(\"Starting Naive Bayes analysis for flaky vs larger non-flaky files...\")\n",
    "    best_params_5folds_2, df_results_5folds_2 = flastThreshold(\n",
    "        outDirLarger, flakyZip, largerNonFlakyZip, extractDirLarger, 5, dim=100, eps=0.3, combination_label=\"larger\", param_grid=param_grid)\n",
    "    \n",
    "    print(\"Best results for 5-fold on larger combination:\")\n",
    "    print(df_results_5folds_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a031a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
