{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84acd571",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2d4796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty or invalid file skipped: {file_path}\")\n",
    "\n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "\n",
    "    return dataPointsList\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Parameters setup\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"Dataset/all_nonflaky_files.zip\"\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "extractDir = \"smote-extracted\"\n",
    "os.makedirs(extractDir, exist_ok=True)\n",
    "\n",
    "# Extract the zip files\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "if len(dataPoints) == 0:\n",
    "    raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "# Create labels: 1 for flaky, 0 for non-flaky\n",
    "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf94de6",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f791afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents: 45\n",
      "Number of non-flaky documents: 243\n",
      "Total number of documents: 288\n",
      "Starting KNN analysis with SMOTE and PCA for 3-fold cross-validation...\n",
      "Fitting 3 folds for each of 112 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "168 fits failed out of a total of 336.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "168 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 255, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1104, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 474, in fit_transform\n",
      "    U, S, _, X, x_is_centered, xp = self._fit(X)\n",
      "                                    ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 549, in _fit\n",
      "    return self._fit_truncated(X, n_components, xp)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 724, in _fit_truncated\n",
      "    raise ValueError(\n",
      "ValueError: n_components=200 must be between 1 and min(n_samples, n_features)=192 with svd_solver='arpack'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.41879208 0.39442039        nan        nan 0.48897059 0.46165577\n",
      "        nan        nan 0.4317315  0.40970696        nan        nan\n",
      " 0.50187049 0.47476534        nan        nan 0.39660581 0.38202899\n",
      "        nan        nan 0.48910675 0.47348928        nan        nan\n",
      " 0.38164251 0.37367521        nan        nan 0.49259259 0.48253968\n",
      "        nan        nan 0.3754321  0.38162393        nan        nan\n",
      " 0.46704545 0.48618421        nan        nan 0.40333333 0.38587302\n",
      "        nan        nan 0.50535714 0.4756607         nan        nan\n",
      " 0.46332786 0.43532029        nan        nan 0.56100251 0.50119048\n",
      "        nan        nan 0.54035088 0.46968326        nan        nan\n",
      " 0.62222222 0.55512821        nan        nan 0.4624183  0.45340177\n",
      "        nan        nan 0.60470085 0.59325397        nan        nan\n",
      " 0.40331384 0.39896036        nan        nan 0.54603175 0.53238796\n",
      "        nan        nan 0.36643678 0.36903226        nan        nan\n",
      " 0.53846154 0.54586895        nan        nan 0.36812865 0.41258318\n",
      "        nan        nan 0.51811152 0.56825397        nan        nan\n",
      " 0.38657513 0.40644283        nan        nan 0.58391608 0.60772561\n",
      "        nan        nan 0.44137453 0.43294347        nan        nan\n",
      " 0.56837607 0.58226496        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [0.6701303  0.6631137         nan        nan 1.         1.\n",
      "        nan        nan 0.58633912 0.58517975        nan        nan\n",
      " 1.         1.                nan        nan 0.54143519 0.53662677\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.50841446 0.50164835        nan        nan 1.         1.\n",
      "        nan        nan 0.50054879 0.49475548        nan        nan\n",
      " 1.         1.                nan        nan 0.48341837 0.46597554\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.5041161  0.48206687        nan        nan 1.         1.\n",
      "        nan        nan 0.79658883 0.78166278        nan        nan\n",
      " 1.         1.                nan        nan 0.64557338 0.63468013\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.61015218 0.59866577        nan        nan 1.         1.\n",
      "        nan        nan 0.54695818 0.53258145        nan        nan\n",
      " 1.         1.                nan        nan 0.53477444 0.51532938\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.52292121 0.49987668        nan        nan 1.         1.\n",
      "        nan        nan 0.55594542 0.52665764        nan        nan\n",
      " 1.         1.                nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.53333333 0.53333333        nan        nan 0.51111111 0.51111111\n",
      "        nan        nan 0.57777778 0.6               nan        nan\n",
      " 0.57777778 0.6               nan        nan 0.57777778 0.6\n",
      "        nan        nan 0.57777778 0.6               nan        nan\n",
      " 0.57777778 0.6               nan        nan 0.57777778 0.6\n",
      "        nan        nan 0.6        0.6               nan        nan\n",
      " 0.6        0.6               nan        nan 0.64444444 0.64444444\n",
      "        nan        nan 0.64444444 0.64444444        nan        nan\n",
      " 0.66666667 0.73333333        nan        nan 0.71111111 0.73333333\n",
      "        nan        nan 0.51111111 0.51111111        nan        nan\n",
      " 0.48888889 0.48888889        nan        nan 0.53333333 0.57777778\n",
      "        nan        nan 0.51111111 0.55555556        nan        nan\n",
      " 0.55555556 0.57777778        nan        nan 0.53333333 0.55555556\n",
      "        nan        nan 0.6        0.62222222        nan        nan\n",
      " 0.53333333 0.55555556        nan        nan 0.55555556 0.62222222\n",
      "        nan        nan 0.51111111 0.57777778        nan        nan\n",
      " 0.53333333 0.62222222        nan        nan 0.51111111 0.6\n",
      "        nan        nan 0.55555556 0.64444444        nan        nan\n",
      " 0.51111111 0.62222222        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [0.98888889 1.                nan        nan 1.         1.\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 1.         1.                nan        nan 1.         1.\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.95555556 0.95555556        nan        nan 1.         1.\n",
      "        nan        nan 0.91111111 0.9               nan        nan\n",
      " 1.         1.                nan        nan 0.9        0.9\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.84444444 0.86666667        nan        nan 1.         1.\n",
      "        nan        nan 0.98888889 1.                nan        nan\n",
      " 1.         1.                nan        nan 1.         1.\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 1.         1.                nan        nan 1.         1.\n",
      "        nan        nan 0.95555556 0.96666667        nan        nan\n",
      " 1.         1.                nan        nan 0.91111111 0.88888889\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.88888889 0.87777778        nan        nan 1.         1.\n",
      "        nan        nan 0.78888889 0.81111111        nan        nan\n",
      " 1.         1.                nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.8125     0.79861111        nan        nan 0.84027778 0.82986111\n",
      "        nan        nan 0.8125     0.79861111        nan        nan\n",
      " 0.84375    0.83333333        nan        nan 0.79513889 0.78472222\n",
      "        nan        nan 0.84027778 0.83333333        nan        nan\n",
      " 0.78819444 0.78125           nan        nan 0.84027778 0.83680556\n",
      "        nan        nan 0.78125    0.78819444        nan        nan\n",
      " 0.82986111 0.84027778        nan        nan 0.79861111 0.78819444\n",
      "        nan        nan 0.84722222 0.83680556        nan        nan\n",
      " 0.82291667 0.80555556        nan        nan 0.86458333 0.84027778\n",
      "        nan        nan 0.84722222 0.82986111        nan        nan\n",
      " 0.87152778 0.85763889        nan        nan 0.82986111 0.82638889\n",
      "        nan        nan 0.87152778 0.87152778        nan        nan\n",
      " 0.80208333 0.79861111        nan        nan 0.85763889 0.85416667\n",
      "        nan        nan 0.77430556 0.77430556        nan        nan\n",
      " 0.85416667 0.85763889        nan        nan 0.77777778 0.80555556\n",
      "        nan        nan 0.84722222 0.86458333        nan        nan\n",
      " 0.79513889 0.80208333        nan        nan 0.86458333 0.875\n",
      "        nan        nan 0.81944444 0.8125            nan        nan\n",
      " 0.86111111 0.86805556        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [0.921875   0.92013889        nan        nan 1.         1.\n",
      "        nan        nan 0.88715278 0.88715278        nan        nan\n",
      " 1.         1.                nan        nan 0.86458333 0.86284722\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.84548611 0.84201389        nan        nan 1.         1.\n",
      "        nan        nan 0.83854167 0.83680556        nan        nan\n",
      " 1.         1.                nan        nan 0.83159722 0.82118056\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.84375    0.83159722        nan        nan 1.         1.\n",
      "        nan        nan 0.95659722 0.953125          nan        nan\n",
      " 1.         1.                nan        nan 0.91319444 0.90798611\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.89930556 0.89409722        nan        nan 1.         1.\n",
      "        nan        nan 0.86805556 0.86111111        nan        nan\n",
      " 1.         1.                nan        nan 0.86111111 0.85069444\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.85590278 0.84375           nan        nan 1.         1.\n",
      "        nan        nan 0.86805556 0.85590278        nan        nan\n",
      " 1.         1.                nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.46827342 0.45272545        nan        nan 0.49895717 0.4844697\n",
      "        nan        nan 0.49329205 0.48563686        nan        nan\n",
      " 0.53656705 0.5292756         nan        nan 0.47018071 0.46666667\n",
      "        nan        nan 0.52967172 0.52908921        nan        nan\n",
      " 0.45936422 0.45971093        nan        nan 0.53015873 0.53227513\n",
      "        nan        nan 0.46031746 0.46520742        nan        nan\n",
      " 0.52325321 0.53597181        nan        nan 0.49471545 0.48118002\n",
      "        nan        nan 0.56482335 0.54533208        nan        nan\n",
      " 0.53904638 0.53754941        nan        nan 0.61920216 0.58604651\n",
      "        nan        nan 0.51557242 0.48571429        nan        nan\n",
      " 0.54511707 0.51887125        nan        nan 0.49365079 0.50513625\n",
      "        nan        nan 0.55379189 0.57180132        nan        nan\n",
      " 0.46494355 0.46872082        nan        nan 0.5394636  0.54277901\n",
      "        nan        nan 0.45238095 0.45962733        nan        nan\n",
      " 0.53456221 0.54805195        nan        nan 0.43834423 0.49108734\n",
      "        nan        nan 0.51219087 0.56627136        nan        nan\n",
      " 0.44753059 0.48716578        nan        nan 0.54200638 0.59218559\n",
      "        nan        nan 0.49044499 0.5110715         nan        nan\n",
      " 0.53624054 0.58641975        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [0.79855305 0.79704953        nan        nan 1.         1.\n",
      "        nan        nan 0.73762392 0.73701299        nan        nan\n",
      " 1.         1.                nan        nan 0.7008612  0.69726146\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.66223802 0.65669164        nan        nan 1.         1.\n",
      "        nan        nan 0.64273226 0.63607595        nan        nan\n",
      " 1.         1.                nan        nan 0.62729317 0.61272311\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.62944886 0.6180006         nan        nan 1.         1.\n",
      "        nan        nan 0.87946081 0.87398206        nan        nan\n",
      " 1.         1.                nan        nan 0.78385052 0.77503218\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.75728631 0.74819893        nan        nan 1.         1.\n",
      "        nan        nan 0.69468142 0.68586895        nan        nan\n",
      " 1.         1.                nan        nan 0.67306853 0.65165755\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.65811966 0.6369145         nan        nan 1.         1.\n",
      "        nan        nan 0.65124183 0.63744231        nan        nan\n",
      " 1.         1.                nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.36099243 0.33907457        nan        nan 0.40478209 0.38414721\n",
      "        nan        nan 0.38823906 0.37766343        nan        nan\n",
      " 0.44524665 0.43473539        nan        nan 0.35833049 0.35385784\n",
      "        nan        nan 0.43651168 0.43415363        nan        nan\n",
      " 0.34581723 0.34712066        nan        nan 0.43802593 0.44077175\n",
      "        nan        nan 0.34803553 0.35591387        nan        nan\n",
      " 0.42826143 0.44520973        nan        nan 0.39456762 0.37848073\n",
      "        nan        nan 0.48028774 0.45789795        nan        nan\n",
      " 0.45210239 0.4565998         nan        nan 0.55061953 0.51367753\n",
      "        nan        nan 0.43210848 0.38752545        nan        nan\n",
      " 0.47772351 0.43764075        nan        nan 0.39483969 0.40850117\n",
      "        nan        nan 0.48182178 0.49813264        nan        nan\n",
      " 0.35613901 0.36174742        nan        nan 0.45543271 0.45692813\n",
      "        nan        nan 0.33872099 0.35035249        nan        nan\n",
      " 0.4490749  0.46554721        nan        nan 0.32160792 0.39389238\n",
      "        nan        nan 0.42345439 0.49111708        nan        nan\n",
      " 0.33267381 0.38838161        nan        nan 0.46650452 0.52706047\n",
      "        nan        nan 0.38752561 0.41935395        nan        nan\n",
      " 0.4571046  0.52025729        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [0.77451854 0.7747294         nan        nan 1.         1.\n",
      "        nan        nan 0.71236813 0.71171934        nan        nan\n",
      " 1.         1.                nan        nan 0.67390685 0.67016272\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.62354714 0.61768551        nan        nan 1.         1.\n",
      "        nan        nan 0.5946505  0.58527342        nan        nan\n",
      " 1.         1.                nan        nan 0.57573609 0.55974885\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.5690627  0.55962559        nan        nan 1.         1.\n",
      "        nan        nan 0.86311923 0.85823931        nan        nan\n",
      " 1.         1.                nan        nan 0.76084824 0.75164919\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.73290157 0.72339174        nan        nan 1.         1.\n",
      "        nan        nan 0.65854763 0.65148792        nan        nan\n",
      " 1.         1.                nan        nan 0.62722125 0.59989008\n",
      "        nan        nan 1.         1.                nan        nan\n",
      " 0.60731376 0.58200015        nan        nan 1.         1.\n",
      "        nan        nan 0.58711888 0.57379058        nan        nan\n",
      " 1.         1.                nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters with SMOTE and PCA: {'knn__metric': 'euclidean', 'knn__n_neighbors': 20, 'knn__weights': 'distance', 'pca__n_components': 150}\n",
      "Best F1 Score from cross-validation: 0.6192021636240703\n",
      "Per-fold metrics saved to: smote-results\\knn-smote-pca-fold-results-3-folds.csv\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.5610025062656642\n",
      "Final Recall: 0.7111111111111111\n",
      "Final Accuracy: 0.8645833333333334\n",
      "Final F1 Score: 0.6192021636240703\n",
      "Final MCC: 0.5506195279704212\n",
      "KNN analysis completed for 3-folds with SMOTE and PCA. Results saved to: smote-results\\knn-results-3-folds.csv\n",
      "\n",
      "Starting KNN analysis with SMOTE and PCA for 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n",
      "Best Parameters with SMOTE and PCA: {'knn__metric': 'cosine', 'knn__n_neighbors': 15, 'knn__weights': 'distance', 'pca__n_components': 200}\n",
      "Best F1 Score from cross-validation: 0.6066511266511266\n",
      "Per-fold metrics saved to: smote-results\\knn-smote-pca-fold-results-5-folds.csv\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.6165112665112665\n",
      "Final Recall: 0.6444444444444445\n",
      "Final Accuracy: 0.8785238959467634\n",
      "Final F1 Score: 0.6066511266511266\n",
      "Final MCC: 0.5512693059318502\n",
      "KNN analysis completed for 5-folds with SMOTE and PCA. Results saved to: smote-results\\knn-results-5-folds.csv\n",
      "\n",
      "Best results for KNN with SMOTE and PCA 3-fold cross-validation:\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 20, 'knn__weights': 'distance', 'pca__n_components': 150}\n",
      "Best F1 Score: 0.6192021636240703\n",
      "Best MCC: 0.5506195279704212\n",
      "\n",
      "Best results for KNN with SMOTE and PCA 5-fold cross-validation:\n",
      "Best Parameters: {'knn__metric': 'cosine', 'knn__n_neighbors': 15, 'knn__weights': 'distance', 'pca__n_components': 200}\n",
      "Best F1 Score: 0.6066511266511266\n",
      "Best MCC: 0.5512693059318502\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score,\n",
    "    matthews_corrcoef, make_scorer\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD  # Suitable for sparse data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def runKNNWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    # Define scoring metrics with MCC\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': make_scorer(matthews_corrcoef)  # Directly using matthews_corrcoef\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Define a pipeline with CountVectorizer, TruncatedSVD, SMOTE, and KNN\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "        ('pca', PCA()),\n",
    "        ('smote', SMOTE(random_state=42)),  # SMOTE for oversampling\n",
    "        ('knn', KNeighborsClassifier()),  # KNN classifier\n",
    "        \n",
    "\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'pca__n_components': [150,180,200,200],  # Variance ratios\n",
    "        'knn__n_neighbors': [3, 5, 7, 9, 11, 15, 20],  # Number of neighbors for KNN\n",
    "        'knn__weights': ['uniform', 'distance'],  # Uniform or distance-based weighting\n",
    "        'knn__metric': ['euclidean', 'cosine']  # Distance metrics\n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring,\n",
    "        refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE and PCA: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'svd__n_components': results['params'][idx].get('svd__n_components'),\n",
    "            'n_neighbors': results['params'][idx].get('knn__n_neighbors'),\n",
    "            'weights': results['params'][idx].get('knn__weights'),\n",
    "            'metric': results['params'][idx].get('knn__metric'),\n",
    "            'accuracy': results['mean_test_accuracy'][idx],\n",
    "            'precision': results['mean_test_precision'][idx],\n",
    "            'recall': results['mean_test_recall'][idx],\n",
    "            'f1': results['mean_test_f1'][idx],\n",
    "            'mcc': results['mean_test_mcc'][idx],  # Include MCC in fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"knn-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    best_index = grid_search.best_index_\n",
    "    final_f1 = results['mean_test_f1'][best_index]\n",
    "    final_precision = results['mean_test_precision'][best_index]\n",
    "    final_recall = results['mean_test_recall'][best_index]\n",
    "    final_accuracy = results['mean_test_accuracy'][best_index]\n",
    "    final_mcc = results['mean_test_mcc'][best_index]  # Include final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the final results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"knn-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC\n",
    "\n",
    "    print(f\"KNN analysis completed for {n_splits}-folds with SMOTE and PCA. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC as well\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "extractDir = \"smote-extracted\"\n",
    "os.makedirs(extractDir, exist_ok=True)  # Corrected 'Tarue' to 'True'\n",
    "\n",
    "# Extract the zip files\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "print(f\"Number of flaky documents: {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky documents: {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of documents: {len(dataPoints)}\")\n",
    "\n",
    "if len(dataPoints) == 0:\n",
    "    raise ValueError(\"No documents available for vectorization. Please check the input directories.\")\n",
    "\n",
    "# Create labels: 1 for flaky, 0 for non-flaky\n",
    "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run KNN with SMOTE and PCA using 5-fold cross-validation\n",
    "print(\"\\nStarting KNN analysis with SMOTE and PCA for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runKNNWithSMOTE(\n",
    "    dataPoints, dataLabelsList, outDir, n_splits=5\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nBest results for KNN with SMOTE and PCA 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Best MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c826fa",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c857480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM analysis with SMOTE for 3-fold cross-validation...\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 255, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1104, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 474, in fit_transform\n",
      "    U, S, _, X, x_is_centered, xp = self._fit(X)\n",
      "                                    ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 549, in _fit\n",
      "    return self._fit_truncated(X, n_components, xp)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 724, in _fit_truncated\n",
      "    raise ValueError(\n",
      "ValueError: n_components=200 must be between 1 and min(n_samples, n_features)=192 with svd_solver='arpack'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 255, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1104, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 474, in fit_transform\n",
      "    U, S, _, X, x_is_centered, xp = self._fit(X)\n",
      "                                    ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 549, in _fit\n",
      "    return self._fit_truncated(X, n_components, xp)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 724, in _fit_truncated\n",
      "    raise ValueError(\n",
      "ValueError: n_components=220 must be between 1 and min(n_samples, n_features)=192 with svd_solver='arpack'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.79259259 0.44166667 0.05072464 0.28246032 0.75925926 0.44166667\n",
      " 0.10243391 0.31954202 0.75925926 0.85185185 0.38697318 0.27478632\n",
      " 0.75925926 0.77380952 0.55555556 0.28226022 0.75925926 0.70075758\n",
      " 0.67222222 0.25511364 0.83611111 0.4398374  0.05072464 0.28084416\n",
      " 0.83611111 0.4398374  0.10243391 0.31366975 0.83611111 0.85185185\n",
      " 0.38697318 0.26451613 0.83611111 0.77248677 0.55555556 0.26590301\n",
      " 0.83611111 0.77272727 0.69450549 0.25511364        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [1.         0.61546232 0.72191529 0.37876964 1.         0.61546232\n",
      " 0.44672149 0.42481021 1.         0.91938646 0.72583826 0.45406178\n",
      " 1.         0.98924731 1.         0.35010293 1.         1.\n",
      " 0.97690163 0.31429071 1.         0.61300505 0.72191529 0.37766504\n",
      " 1.         0.61300505 0.44672149 0.40921409 1.         0.92119805\n",
      " 0.72583826 0.45406178 1.         0.98924731 1.         0.34668188\n",
      " 1.         1.         0.97690163 0.31429071        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.6        0.33333333 0.31111111 0.42222222 0.57777778 0.33333333\n",
      " 0.62222222 0.37777778 0.57777778 0.42222222 0.33333333 0.35555556\n",
      " 0.57777778 0.4        0.11111111 0.62222222 0.57777778 0.48888889\n",
      " 0.35555556 0.6        0.55555556 0.33333333 0.31111111 0.42222222\n",
      " 0.55555556 0.33333333 0.62222222 0.37777778 0.55555556 0.42222222\n",
      " 0.33333333 0.35555556 0.55555556 0.4        0.11111111 0.57777778\n",
      " 0.55555556 0.46666667 0.4        0.6               nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [1.         0.47777778 0.42222222 0.53333333 1.         0.47777778\n",
      " 0.75555556 0.54444444 1.         0.8        0.57777778 0.61111111\n",
      " 1.         0.98888889 0.47777778 0.75555556 1.         1.\n",
      " 0.82222222 0.68888889 1.         0.47777778 0.42222222 0.53333333\n",
      " 1.         0.47777778 0.75555556 0.54444444 1.         0.81111111\n",
      " 0.57777778 0.61111111 1.         1.         0.47777778 0.72222222\n",
      " 1.         1.         0.82222222 0.68888889        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.91319444 0.77083333 0.61805556 0.66319444 0.90625    0.77083333\n",
      " 0.40625    0.70486111 0.90625    0.89583333 0.64236111 0.73611111\n",
      " 0.90625    0.88888889 0.84722222 0.68055556 0.90625    0.88888889\n",
      " 0.875      0.64583333 0.91319444 0.76736111 0.61805556 0.65625\n",
      " 0.91319444 0.76736111 0.40625    0.68402778 0.91319444 0.89583333\n",
      " 0.64236111 0.73611111 0.91319444 0.88888889 0.84722222 0.67708333\n",
      " 0.91319444 0.89583333 0.88194444 0.64583333        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [1.         0.8125     0.64756944 0.71006944 1.         0.8125\n",
      " 0.453125   0.74305556 1.         0.95833333 0.69270833 0.81944444\n",
      " 1.         0.99652778 0.91840278 0.72916667 1.         1.\n",
      " 0.96875    0.703125   1.         0.80902778 0.64756944 0.70659722\n",
      " 1.         0.80902778 0.453125   0.72743056 1.         0.96006944\n",
      " 0.69270833 0.81944444 1.         0.99826389 0.91840278 0.73090278\n",
      " 1.         1.         0.96875    0.703125          nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.67444444 0.304329   0.08722741 0.25126629 0.64777778 0.304329\n",
      " 0.17588551 0.25966431 0.64777778 0.55555556 0.13316993 0.3026455\n",
      " 0.64777778 0.52700922 0.1834652  0.38642043 0.64777778 0.57227796\n",
      " 0.44920635 0.35682654 0.66314546 0.30238095 0.08722741 0.24901186\n",
      " 0.66314546 0.30238095 0.17588551 0.25196213 0.66314546 0.55555556\n",
      " 0.13316993 0.29436393 0.66314546 0.52525253 0.1834652  0.36166063\n",
      " 0.66314546 0.57859532 0.48961039 0.35682654        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [1.         0.45127974 0.24756451 0.3748921  1.         0.45127974\n",
      " 0.33414751 0.40664413 1.         0.85367079 0.45764537 0.52038921\n",
      " 1.         0.9888858  0.6426299  0.47435332 1.         1.\n",
      " 0.88196242 0.4286379  1.         0.44869837 0.24756451 0.3734337\n",
      " 1.         0.44869837 0.33414751 0.39651416 1.         0.86135773\n",
      " 0.45764537 0.52038921 1.         0.99453552 0.6426299  0.46480172\n",
      " 1.         1.         0.88196242 0.4286379         nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [ 0.63936668  0.23031509 -0.03266309  0.14665315  0.60806175  0.23031509\n",
      " -0.00502653  0.16113828  0.60806175  0.54860878  0.09280023  0.15271556\n",
      "  0.60806175  0.50479283  0.19599381  0.24286275  0.60806175  0.52435599\n",
      "  0.42250246  0.19511073  0.63541407  0.22711667 -0.03266309  0.14113535\n",
      "  0.63541407  0.22711667 -0.00502653  0.14397942  0.63541407  0.54860878\n",
      "  0.09280023  0.14742257  0.63541407  0.50367613  0.19599381  0.2116205\n",
      "  0.63541407  0.5473055   0.46187617  0.19511073         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the train scores are non-finite: [1.         0.40929967 0.25395578 0.27832851 1.         0.40929967\n",
      " 0.24579068 0.32472245 1.         0.83355963 0.43448614 0.41988018\n",
      " 1.         0.98695645 0.65805509 0.37299981 1.         1.\n",
      " 0.87562121 0.30589315 1.         0.40586866 0.25395578 0.27579605\n",
      " 1.         0.40586866 0.24579068 0.3086868  1.         0.8413747\n",
      " 0.43448614 0.41988018 1.         0.99356594 0.65805509 0.35661877\n",
      " 1.         1.         0.87562121 0.30589315        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters with SMOTE: {'pca__n_components': 150, 'svm__C': 0.01, 'svm__kernel': 'linear'}\n",
      "Best F1 Score from cross-validation: 0.6744444444444445\n",
      "Per-fold metrics saved to: smote-results\\svm-smote-fold-results-3-folds.csv\n",
      "\n",
      "Final Cross-Validation Metrics:\n",
      "Final Precision: 0.7925925925925926\n",
      "Final Recall: 0.6000000000000001\n",
      "Final Accuracy: 0.9131944444444445\n",
      "Final F1 Score: 0.6744444444444445\n",
      "SVM analysis completed for 3-folds with SMOTE. Results saved to: smote-results\\svm-results-3-folds.csv\n",
      "\n",
      "Starting SVM analysis with SMOTE for 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\haha9\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Run SVM with SMOTE using 5-fold cross-validation\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting SVM analysis with SMOTE for 5-fold cross-validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m best_params_5folds, best_f1_5folds \u001b[38;5;241m=\u001b[39m runSVMWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest results for SVM with SMOTE 3-fold cross-validation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m, in \u001b[0;36mrunSVMWithSMOTE\u001b[1;34m(dataPoints, dataLabelsList, outDir, n_splits)\u001b[0m\n\u001b[0;32m     45\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39mskf, scoring\u001b[38;5;241m=\u001b[39mscoring, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV to the data\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(dataPoints, dataLabelsList)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Retrieve the best parameters and score from cross-validation\u001b[39;00m\n\u001b[0;32m     51\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    966\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    967\u001b[0m         clone(base_estimator),\n\u001b[0;32m    968\u001b[0m         X,\n\u001b[0;32m    969\u001b[0m         y,\n\u001b[0;32m    970\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    971\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    972\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    973\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    974\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    975\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    976\u001b[0m     )\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    979\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    981\u001b[0m )\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py:329\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03mFit all the transforms/samplers one after the other and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    This estimator.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 329\u001b[0m Xt, yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py:255\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m    253\u001b[0m     cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m ):\n\u001b[1;32m--> 255\u001b[0m     X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    256\u001b[0m         cloned_transformer,\n\u001b[0;32m    257\u001b[0m         X,\n\u001b[0;32m    258\u001b[0m         y,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    260\u001b[0m         message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    261\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    262\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    263\u001b[0m     )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_resample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    265\u001b[0m     X, y, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_resample_one_cached(\n\u001b[0;32m    266\u001b[0m         cloned_transformer,\n\u001b[0;32m    267\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    272\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py:1104\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1104\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1107\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1108\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:549\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components, xp, is_array_api_compliant)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:755\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[1;34m(self, X, n_components, xp)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    754\u001b[0m     v0 \u001b[38;5;241m=\u001b[39m _init_arpack_v0(\u001b[38;5;28mmin\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape), random_state)\n\u001b[1;32m--> 755\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m svds(X_centered, k\u001b[38;5;241m=\u001b[39mn_components, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol, v0\u001b[38;5;241m=\u001b[39mv0)\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;66;03m# svds doesn't abide by scipy.linalg.svd/randomized_svd\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# conventions, so reverse its outputs.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     S \u001b[38;5;241m=\u001b[39m S[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\_svds.py:548\u001b[0m, in \u001b[0;36msvds\u001b[1;34m(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors, solver, random_state, options)\u001b[0m\n\u001b[0;32m    545\u001b[0m jobv \u001b[38;5;241m=\u001b[39m return_singular_vectors \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvh\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[1;32m--> 548\u001b[0m     u_tmp \u001b[38;5;241m=\u001b[39m eigvec \u001b[38;5;241m@\u001b[39m _herm(vh) \u001b[38;5;28;01mif\u001b[39;00m jobu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     vh \u001b[38;5;241m=\u001b[39m _herm(u) \u001b[38;5;28;01mif\u001b[39;00m jobv \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    550\u001b[0m     u \u001b[38;5;241m=\u001b[39m u_tmp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\_svds.py:21\u001b[0m, in \u001b[0;36m_herm\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m arpack_int \u001b[38;5;241m=\u001b[39m _arpack\u001b[38;5;241m.\u001b[39mtiming\u001b[38;5;241m.\u001b[39mnbx\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvds\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_herm\u001b[39m(x):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mconj()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iv\u001b[39m(A, k, ncv, tol, which, v0, maxiter,\n\u001b[0;32m     26\u001b[0m         return_singular, solver, random_state):\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# input validation/standardization for `solver`\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# out of order because it's needed for other parameters\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "def runSVMWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1',\n",
    "        'mcc': make_scorer(matthews_corrcoef)  \n",
    "\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Define a pipeline with SMOTE and SVM \n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
    "        ('pca', PCA()),\n",
    "        ('smote', SMOTE(random_state=42)),    # SMOTE for oversampling\n",
    "        ('svm', SVC(probability=True))        # SVM classifier\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'pca__n_components': [150,180, 200,220],              # PCA components\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],               # Regularization parameter C\n",
    "        'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid']   # Kernel types\n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'C': results['params'][idx].get('svm__C'),\n",
    "            'kernel': results['params'][idx].get('svm__kernel'),\n",
    "            'accuracy': results['mean_test_accuracy'][idx],\n",
    "            'precision': results['mean_test_precision'][idx],\n",
    "            'recall': results['mean_test_recall'][idx],\n",
    "            'f1': results['mean_test_f1'][idx],\n",
    "            'mcc': results['mean_test_mcc'][idx],\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile = os.path.join(outDir, f\"svm-smote-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile = os.path.join(outDir, f\"svm-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1\\n\")\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1}\\n\")\n",
    "\n",
    "    print(f\"SVM analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
    "\n",
    "    return best_params, final_f1\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for Both 3-Fold and 5-Fold\n",
    "\n",
    "\n",
    "# Run SVM with SMOTE using 5-fold cross-validation\n",
    "print(\"\\nStarting SVM analysis with SMOTE for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds = runSVMWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "\n",
    "print(\"\\nBest results for SVM with SMOTE 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fd5f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032a4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6bbf78",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb569e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "###############################################################################\n",
    "# XGBoost with SMOTE, PCA, and CountVectorizer\n",
    "\n",
    "def runXGBWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer  # Add MCC to scoring\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with CountVectorizer, PCA, SMOTE, and XGBoost\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
    "        ('pca', PCA()),                                    # PCA for dimensionality reduction\n",
    "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
    "        ('xgb', XGBClassifier(eval_metric='logloss', random_state=42))  # XGBoost classifier\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'pca__n_components': [150,180, 200, 220],              # PCA components\n",
    "        'xgb__n_estimators': [100, 150, 200, 300],         # Number of boosting rounds\n",
    "        'xgb__max_depth': [3, 5, 7, 10],                  # Maximum depth of a tree\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5],      # Learning rate\n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE, PCA, and CountVectorizer: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combination\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'n_components': results['params'][idx].get('pca__n_components'),\n",
    "            'n_estimators': results['params'][idx].get('xgb__n_estimators'),\n",
    "            'max_depth': results['params'][idx].get('xgb__max_depth'),\n",
    "            'learning_rate': results['params'][idx].get('xgb__learning_rate'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],  # Add MCC to fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"xgb-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]  # Extract final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"xgb-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in the header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC in the data\n",
    "\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds with SMOTE, PCA, and CountVectorizer. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run XGBoost with SMOTE, PCA, and CountVectorizer using 5-fold cross-validation\n",
    "print(\"\\nStarting XGBoost analysis with SMOTE, PCA, and CountVectorizer for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runXGBWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost with SMOTE, PCA, and CountVectorizer 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Final MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb851b8",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd465761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "###############################################################################\n",
    "# Random Forest with SMOTE and PCA\n",
    "\n",
    "def runRFWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer  # Add MCC to scoring\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with PCA, SMOTE, and Random Forest\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
    "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
    "        ('rf', RandomForestClassifier(random_state=42))    # Random Forest classifier\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'rf__n_estimators': [10, 50, 100, 300, 500],  \n",
    "        'rf__criterion': ['gini', 'entropy'],   \n",
    "        'rf__max_depth': [10, 30, 50, 100, 300, 500],  \n",
    "        'rf__min_samples_split': [2, 5],       \n",
    "        'rf__min_samples_leaf': [1, 2],        \n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE and PCA: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'n_components': results['params'][idx].get('pca__n_components'),\n",
    "            'n_estimators': results['params'][idx].get('rf__n_estimators'),\n",
    "            'max_depth': results['params'][idx].get('rf__max_depth'),\n",
    "            'min_samples_split': results['params'][idx].get('rf__min_samples_split'),\n",
    "            'min_samples_leaf': results['params'][idx].get('rf__min_samples_leaf'),\n",
    "            'criterion': results['params'][idx].get('rf__criterion'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],  # Using dynamic access to avoid KeyError\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],  # Add MCC to fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"rf-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]  # Extract final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"rf-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC in data\n",
    "\n",
    "    print(f\"Random Forest analysis completed for {n_splits}-folds with SMOTE and PCA. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Random Forest with SMOTE and PCA using 5-fold cross-validation\n",
    "print(\"\\nStarting Random Forest analysis with SMOTE and PCA for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runRFWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for Random Forest with SMOTE and PCA 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Final MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef3f6e",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6504957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer  # Include MCC\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "def runDTWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics, include MCC\n",
    "    scoring = {\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1',\n",
    "        'mcc': make_scorer(matthews_corrcoef)  # Add MCC to scoring\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with SMOTE and Decision Tree\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
    "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
    "        ('dt', DecisionTreeClassifier(random_state=42))    # Decision Tree classifier\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for GridSearchCV\n",
    "      param_grid = {\n",
    "        'dt__criterion': ['gini', 'entropy'], \n",
    "        'dt__max_depth': [None, 10, 30, 50, 100, 300, 500],  \n",
    "        'dt__min_samples_split': [2, 5, 10], \n",
    "        'dt__min_samples_leaf': [1, 2, 5, 10],  \n",
    "        'dt__max_features': [None, 'sqrt', 'log2'],  \n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring,\n",
    "        refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'max_depth': results['params'][idx].get('dt__max_depth'),\n",
    "            'min_samples_split': results['params'][idx].get('dt__min_samples_split'),\n",
    "            'min_samples_leaf': results['params'][idx].get('dt__min_samples_leaf'),\n",
    "            'criterion': results['params'][idx].get('dt__criterion'),\n",
    "            'accuracy': results['mean_test_accuracy'][idx],\n",
    "            'precision': results['mean_test_precision'][idx],\n",
    "            'recall': results['mean_test_recall'][idx],\n",
    "            'f1': results['mean_test_f1'][idx],\n",
    "            'mcc': results['mean_test_mcc'][idx],  # Add MCC to fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile = os.path.join(outDir, f\"dt-smote-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = grid_search.best_score_\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]  # Extract final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile = os.path.join(outDir, f\"dt-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC in data\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {n_splits}-folds with SMOTE. Results saved to: {outFile}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "###############################################################################\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "###############################################################################\n",
    "# Decision Tree with SMOTE and PCA\n",
    "\n",
    "def runDTWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "        'mcc': mcc_scorer  # Add MCC to scoring\n",
    "    }\n",
    "\n",
    "    # Define a pipeline with PCA, SMOTE, and Decision Tree\n",
    "    pipeline = ImbPipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=None)),  # Include vectorizer in pipeline\n",
    "        ('pca', PCA()),                                    # PCA for dimensionality reduction\n",
    "        ('smote', SMOTE(random_state=42)),                 # SMOTE for oversampling\n",
    "        ('dt', DecisionTreeClassifier(random_state=42))    # Decision Tree classifier\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'pca__n_components': [180,200,220],  \n",
    "        'dt__criterion': ['gini', 'entropy'], \n",
    "        'dt__max_depth': [None, 10, 30, 50, 100, 300, 500],  \n",
    "        'dt__min_samples_split': [2, 5, 10], \n",
    "        'dt__min_samples_leaf': [1, 2, 5, 10],  \n",
    "        'dt__max_features': [None, 'sqrt', 'log2'],  \n",
    "    }\n",
    "\n",
    "    # Setup cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Setup GridSearchCV with the pipeline and parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=skf, scoring=scoring,\n",
    "        refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV to the data\n",
    "    grid_search.fit(dataPoints, dataLabelsList)\n",
    "\n",
    "    # Retrieve the best parameters and score from cross-validation\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_cv = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters with SMOTE and PCA: {best_params}\")\n",
    "    print(f\"Best F1 Score from cross-validation: {best_f1_cv}\")\n",
    "\n",
    "    # Extract the cross-validation results and print final metrics\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # Prepare per-fold metrics for CSV along with the parameter combinations\n",
    "    fold_metrics = []\n",
    "    for idx in range(len(results['params'])):\n",
    "        fold_metrics.append({\n",
    "            'n_components': results['params'][idx].get('pca__n_components'),\n",
    "            'max_depth': results['params'][idx].get('dt__max_depth'),\n",
    "            'min_samples_split': results['params'][idx].get('dt__min_samples_split'),\n",
    "            'min_samples_leaf': results['params'][idx].get('dt__min_samples_leaf'),\n",
    "            'criterion': results['params'][idx].get('dt__criterion'),\n",
    "            'accuracy': results.get('mean_test_accuracy', [None])[idx],  # Using dynamic access to avoid KeyError\n",
    "            'precision': results.get('mean_test_precision', [None])[idx],\n",
    "            'recall': results.get('mean_test_recall', [None])[idx],\n",
    "            'f1': results.get('mean_test_f1', [None])[idx],\n",
    "            'mcc': results.get('mean_test_mcc', [None])[idx],  # Add MCC to fold metrics\n",
    "        })\n",
    "\n",
    "    # Save fold-wise metrics to CSV\n",
    "    df_folds = pd.DataFrame(fold_metrics)\n",
    "    outFile_folds = os.path.join(outDir, f\"dt-smote-pca-fold-results-{n_splits}-folds.csv\")\n",
    "    df_folds.to_csv(outFile_folds, index=False)\n",
    "\n",
    "    print(f\"Per-fold metrics saved to: {outFile_folds}\")\n",
    "\n",
    "    # Extract final metrics based on the cross-validation results\n",
    "    final_f1 = results['mean_test_f1'][grid_search.best_index_]\n",
    "    final_precision = results['mean_test_precision'][grid_search.best_index_]\n",
    "    final_recall = results['mean_test_recall'][grid_search.best_index_]\n",
    "    final_accuracy = results['mean_test_accuracy'][grid_search.best_index_]\n",
    "    final_mcc = results['mean_test_mcc'][grid_search.best_index_]  # Extract final MCC\n",
    "\n",
    "    # Print final metrics (cross-validation averages)\n",
    "    print(\"\\nFinal Cross-Validation Metrics:\")\n",
    "    print(f\"Final Precision: {final_precision}\")\n",
    "    print(f\"Final Recall: {final_recall}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy}\")\n",
    "    print(f\"Final F1 Score: {final_f1}\")\n",
    "    print(f\"Final MCC: {final_mcc}\")  # Print final MCC\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    outFile_final = os.path.join(outDir, f\"dt-results-{n_splits}-folds.csv\")\n",
    "    with open(outFile_final, \"w\") as f:\n",
    "        f.write(\"Accuracy,Precision,Recall,F1,MCC\\n\")  # Include MCC in header\n",
    "        f.write(f\"{final_accuracy},{final_precision},{final_recall},{final_f1},{final_mcc}\\n\")  # Include MCC in data\n",
    "\n",
    "    print(f\"Decision Tree analysis completed for {n_splits}-folds with SMOTE and PCA. Results saved to: {outFile_final}\")\n",
    "\n",
    "    return best_params, final_f1, final_mcc  # Return final MCC\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution for 5-Fold Cross-Validation\n",
    "\n",
    "outDir = \"smote-results\"\n",
    "os.makedirs(outDir, exist_ok=True)\n",
    "\n",
    "# Run Decision Tree with SMOTE and PCA using 5-fold cross-validation\n",
    "print(\"\\nStarting Decision Tree analysis with SMOTE and PCA for 5-fold cross-validation...\")\n",
    "best_params_5folds, best_f1_5folds, final_mcc_5folds = runDTWithSMOTE(dataPoints, dataLabelsList, outDir, n_splits=5)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for Decision Tree with SMOTE and PCA 5-fold cross-validation:\")\n",
    "print(f\"Best Parameters: {best_params_5folds}\")\n",
    "print(f\"Best F1 Score: {best_f1_5folds}\")\n",
    "print(f\"Final MCC: {final_mcc_5folds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55b949ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results saved to: best_results_smote.csv\n",
      "\n",
      "Best Results from All SMOTE Models\n",
      "        Model        Fold  Best Accuracy  Best Precision  Best Recall  Best F1 Score                                                                                                                                                                                                                        Best Parameters\n",
      "          KNN 5-fold-fold       0.849661        0.508939     0.644444       0.560588                                                 {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine', 'accuracy': 0.8496610169491525, 'precision': 0.5089393939393939, 'recall': 0.6444444444444444, 'f1': 0.5605882352941176}\n",
      "          KNN 3-fold-fold       0.822963        0.450617     0.644444       0.526455                                                  {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine', 'accuracy': 0.822962962962963, 'precision': 0.4506172839506173, 'recall': 0.6444444444444444, 'f1': 0.5264550264550265}\n",
      "          SVM 5-fold-fold       0.902938        0.724747     0.577778       0.623419                                                                                {'C': 0.1, 'kernel': 'linear', 'accuracy': 0.9029378531073448, 'precision': 0.7247474747474747, 'recall': 0.5777777777777777, 'f1': 0.6234188034188035}\n",
      "          SVM 3-fold-fold       0.902938        0.724747     0.577778       0.623419                                                                                {'C': 0.1, 'kernel': 'linear', 'accuracy': 0.9029378531073448, 'precision': 0.7247474747474747, 'recall': 0.5777777777777777, 'f1': 0.6234188034188035}\n",
      "  Naive Bayes 5-fold-fold       0.916384        0.795000     0.577778       0.658904                                                                                                           {'alpha': 0.001, 'accuracy': 0.9163841807909604, 'precision': 0.795, 'recall': 0.5777777777777777, 'f1': 0.6589043309631546}\n",
      "  Naive Bayes 3-fold-fold       0.909731        0.778535     0.577778       0.652471                                                                                                {'alpha': 1.0, 'accuracy': 0.9097306397306396, 'precision': 0.7785353535353535, 'recall': 0.5777777777777778, 'f1': 0.6524712002972873}\n",
      "      XGBoost 5-fold-fold       0.966497        1.000000     0.777778       0.866092                                                                {'n_estimators': 300.0, 'max_depth': 7.0, 'learning_rate': 0.1, 'accuracy': 0.9664971751412428, 'precision': 1.0, 'recall': 0.7777777777777778, 'f1': 0.86609243697479}\n",
      "      XGBoost 3-fold-fold       0.969899        1.000000     0.800000       0.884802                                                                 {'n_estimators': 300.0, 'max_depth': 3.0, 'learning_rate': 0.1, 'accuracy': 0.96989898989899, 'precision': 1.0, 'recall': 0.8000000000000002, 'f1': 0.884802043422733}\n",
      "Random Forest 5-fold-fold       0.949774        0.857540     0.800000       0.825817 {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'criterion': 'entropy', 'accuracy': 0.9497740112994352, 'precision': 0.8575396825396824, 'recall': 0.7999999999999999, 'f1': 0.8258169934640522}\n",
      "Random Forest 3-fold-fold       0.943064        0.820635     0.800000       0.809962  {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy', 'accuracy': 0.943063973063973, 'precision': 0.8206349206349207, 'recall': 0.7999999999999999, 'f1': 0.8099616858237547}\n",
      "Decision Tree 5-fold-fold       0.953107        0.877222     0.800000       0.831304                                        {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini', 'accuracy': 0.9531073446327684, 'precision': 0.8772222222222222, 'recall': 0.8, 'f1': 0.8313037495700035}\n",
      "Decision Tree 3-fold-fold       0.959865        0.941026     0.777778       0.847553                      {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'criterion': 'entropy', 'accuracy': 0.9598653198653198, 'precision': 0.9410256410256408, 'recall': 0.7777777777777777, 'f1': 0.8475533661740559}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the best results from the CSV files of each model\n",
    "def extract_best_results(model_name, fold, csv_file):\n",
    "    \"\"\"\n",
    "    Extracts the best result from the CSV file for a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model (e.g., \"XGBoost\", \"Random Forest\", \"Decision Tree\")\n",
    "    - fold: Number of folds (e.g., 5 or 3)\n",
    "    - csv_file: The path to the CSV file containing the model's results\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the best results for the model and fold.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"CSV file for {model_name} ({fold}-fold) does not exist: {csv_file}\")\n",
    "        return None\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Get the row with the best F1 score\n",
    "    best_row = df.loc[df['f1'].idxmax()]\n",
    "\n",
    "    # Collect the best results into a dictionary\n",
    "    best_results = {\n",
    "        'Model': model_name,\n",
    "        'Fold': f\"{fold}-fold\",\n",
    "        'Best Accuracy': best_row['accuracy'],\n",
    "        'Best Precision': best_row['precision'],\n",
    "        'Best Recall': best_row['recall'],\n",
    "        'Best F1 Score': best_row['f1'],\n",
    "        'Best Parameters': best_row.to_dict()  # Including all parameters\n",
    "    }\n",
    "\n",
    "    return best_results\n",
    "\n",
    "# Function to gather and print/save the best results from all SMOTE models\n",
    "def gather_best_results(models_results_dir, output_file):\n",
    "    \"\"\"\n",
    "    Gathers the best results from all SMOTE models and writes them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - models_results_dir: Directory where the model result CSV files are stored for the SMOTE models.\n",
    "    - output_file: Path to the output CSV file to store the best results.\n",
    "    \"\"\"\n",
    "    # List of models and their corresponding result files for both 5-fold and 3-fold\n",
    "    models = {\n",
    "        'KNN': {'5-fold': 'knn-smote-fold-results-5-folds.csv'},\n",
    "        'SVM': {'5-fold': 'svm-smote-fold-results-5-folds.csv'},\n",
    "        'Naive Bayes': {'5-fold': 'nb-smote-fold-results-5-folds.csv'},\n",
    "        'XGBoost': {'5-fold': 'xgb-smote-fold-results-5-folds.csv'},\n",
    "        'Random Forest': {'5-fold': 'rf-smote-fold-results-5-folds.csv'},\n",
    "        'Decision Tree': {'5-fold': 'dt-smote-fold-results-5-folds.csv',}\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store the best results from each model and fold\n",
    "    best_results = []\n",
    "\n",
    "    # Iterate over each model and its result files for both 5-fold and 3-fold\n",
    "    for model_name, folds in models.items():\n",
    "        for fold, csv_file in folds.items():\n",
    "            full_csv_path = os.path.join(models_results_dir, csv_file)\n",
    "            best_result = extract_best_results(model_name, fold, full_csv_path)\n",
    "            if best_result:\n",
    "                best_results.append(best_result)\n",
    "\n",
    "    # Convert the list of best results into a DataFrame\n",
    "    best_results_df = pd.DataFrame(best_results)\n",
    "\n",
    "    # Save the best results to the output CSV file\n",
    "    best_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Best results saved to: {output_file}\")\n",
    "\n",
    "    # Print the best results as a table\n",
    "    print(f\"\\nBest Results from All SMOTE Models\")\n",
    "    print(best_results_df.to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories where the model result CSV files are stored\n",
    "    results_dir = 'smote-results'\n",
    "\n",
    "    # Path to the output CSV file where best results will be stored\n",
    "    output_file = \"best_results_smote.csv\"\n",
    "\n",
    "    # Gather and save the best results\n",
    "    gather_best_results(results_dir, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf999d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
