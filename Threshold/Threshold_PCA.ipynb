{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky files: 45\n",
      "Number of non-flaky files: 243\n",
      "Total number of data points: 288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# Function to extract zip files\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "# Function to read .py files from the given directory\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file skipped: {file_path}\")\n",
    "    return dataPointsList\n",
    "\n",
    "# Custom MCC scorer function\n",
    "def mcc_scorer(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Paths to your zip files (you can modify these paths)\n",
    "flakyZip = \"Dataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"Dataset/nonflaky_files.zip\"\n",
    "\n",
    "# Create directories for extraction\n",
    "extractDir = \"extracted_files\"\n",
    "flakyDir = os.path.join(extractDir, 'flaky')\n",
    "nonFlakyDir = os.path.join(extractDir, 'nonFlaky')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(flakyDir, exist_ok=True)\n",
    "os.makedirs(nonFlakyDir, exist_ok=True)\n",
    "\n",
    "# Extract the zip files\n",
    "extract_zip(flakyZip, flakyDir)\n",
    "extract_zip(nonFlakyZip, nonFlakyDir)\n",
    "\n",
    "# Collect data points from flaky and non-flaky files\n",
    "dataPointsFlaky = getDataPoints(flakyDir)\n",
    "dataPointsNonFlaky = getDataPoints(nonFlakyDir)\n",
    "dataPoints = dataPointsFlaky + dataPointsNonFlaky\n",
    "\n",
    "# Create labels: 1 for flaky, 0 for non-flaky\n",
    "dataLabelsList = np.array([1]*len(dataPointsFlaky) + [0]*len(dataPointsNonFlaky))\n",
    "\n",
    "print(f\"Number of flaky files: {len(dataPointsFlaky)}\")\n",
    "print(f\"Number of non-flaky files: {len(dataPointsNonFlaky)}\")\n",
    "print(f\"Total number of data points: {len(dataPoints)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2c816",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30601c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of results saved to results/knn-smote-threshold-summary.csv\n",
      "\n",
      "KNN  SMOTE and Threshold analysis completed.\n",
      "Best parameter set based on F1-score:\n",
      "pca__n_components          220\n",
      "n_neighbors                  9\n",
      "weights               distance\n",
      "metric               euclidean\n",
      "threshold                  0.2\n",
      "accuracy              0.888808\n",
      "precision             0.685873\n",
      "recall                0.577778\n",
      "f1                    0.613068\n",
      "mcc                   0.560598\n",
      "Name: 658, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Summarize and find best parameter combinations\n",
    "def summarize_and_find_best(df_results):\n",
    "    summary_df = df_results.groupby(['pca__n_components', 'n_neighbors', 'weights', 'metric', 'threshold']).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'f1': 'mean',\n",
    "        'mcc': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Finding the best parameter set based on the highest F1 score\n",
    "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
    "\n",
    "    return summary_df, best_row\n",
    "\n",
    "# Combined SMOTE and Threshold-based KNN function\n",
    "def KNNThreshold(dataPoints, y, outDir, n_splits, param_grid, thresholds):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Define pipeline  CountVectorizer, SMOTE, PCA, and KNN\n",
    "        pipeline = ImbPipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('knn', KNeighborsClassifier(\n",
    "                n_neighbors=param_dict['n_neighbors'],\n",
    "                weights=param_dict['weights'],\n",
    "                metric=param_dict['metric'],\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(dataPoints, y)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "\n",
    "                # Store results\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'n_neighbors': param_dict['n_neighbors'],\n",
    "                    'weights': param_dict['weights'],\n",
    "                    'metric': param_dict['metric'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    # Convert metrics to DataFrame and save results\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    outFile = os.path.join(outDir, \"knn-smote-threshold-results.csv\")\n",
    "    df_results.to_csv(outFile, index=False)\n",
    "\n",
    "    # Summarize results and return the best parameter set\n",
    "    summary_df, best_params = summarize_and_find_best(df_results)\n",
    "\n",
    "    # Save summarized results to a new CSV file\n",
    "    summary_outFile = os.path.join(outDir, \"knn-smote-threshold-summary.csv\")\n",
    "    summary_df.to_csv(summary_outFile, index=False)\n",
    "    print(f\"Summary of results saved to {summary_outFile}\")\n",
    "\n",
    "    return df_results, best_params\n",
    "\n",
    "# Hyperparameters and thresholds\n",
    "param_grid = {\n",
    "    'pca__n_components': [150, 180, 200, 220],\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['cosine', 'euclidean']\n",
    "}\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "\n",
    "outDir = \"results/\"\n",
    "df_results, best_params = KNNThreshold(dataPoints, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
    "\n",
    "print(\"\\nKNN  SMOTE and Threshold analysis completed.\")\n",
    "print(\"Best parameter set based on F1-score:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52380d",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfccd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of results saved to results/svm-smote-threshold-summary.csv\n",
      "\n",
      "SVM with SMOTE and Threshold analysis completed.\n",
      "Best parameter set based on F1-score:\n",
      "pca__n_components         200\n",
      "kernel                 linear\n",
      "C                        0.01\n",
      "threshold                 0.2\n",
      "accuracy             0.923654\n",
      "precision            0.778312\n",
      "recall               0.755556\n",
      "f1                   0.746654\n",
      "mcc                  0.715542\n",
      "Name: 361, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "# Summarize and find best parameter combinations\n",
    "def summarize_and_find_best(df_results):\n",
    "    summary_df = df_results.groupby(['pca__n_components', 'kernel', 'C', 'threshold']).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'f1': 'mean',\n",
    "        'mcc': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Finding the best parameter set based on the highest F1 score\n",
    "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
    "\n",
    "    return summary_df, best_row\n",
    "\n",
    "# Combined SMOTE and Threshold-based SVM\n",
    "def SVMThreshold(dataPoints, y, outDir, n_splits, param_grid, thresholds):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Define pipeline with Vectorizer, SMOTE, PCA, and SVM\n",
    "        pipeline = ImbPipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words=None)),\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('svm', SVC(\n",
    "                kernel=param_dict['kernel'],\n",
    "                C=param_dict['C'],\n",
    "                probability=True,  # Enable probability predictions for threshold adjustment\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(dataPoints, y)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # SVM probability for positive class\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "\n",
    "                # Store results\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'kernel': param_dict['kernel'],\n",
    "                    'C': param_dict['C'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    # Convert metrics to DataFrame and save results\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    outFile = os.path.join(outDir, \"svm-smote-threshold-results.csv\")\n",
    "    df_results.to_csv(outFile, index=False)\n",
    "\n",
    "    # Summarize results and return the best parameter set\n",
    "    summary_df, best_params = summarize_and_find_best(df_results)\n",
    "\n",
    "    # Save summarized results to a new CSV file\n",
    "    summary_outFile = os.path.join(outDir, \"svm-smote-threshold-summary.csv\")\n",
    "    summary_df.to_csv(summary_outFile, index=False)\n",
    "    print(f\"Summary of results saved to {summary_outFile}\")\n",
    "\n",
    "    return df_results, best_params\n",
    "\n",
    "# Hyperparameters and thresholds\n",
    "param_grid = {\n",
    "    'pca__n_components': [150, 180, 200, 220],         # Number of PCA components to use\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],    # SVM kernel types\n",
    "    'C': [0.01, 0.1, 1.0, 10.0, 100.0]                 # Regularization parameter C\n",
    "}\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "\n",
    "outDir = \"results/\"\n",
    "df_results, best_params = SVMThreshold(dataPoints, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
    "\n",
    "print(\"\\nSVM with SMOTE and Threshold analysis completed.\")\n",
    "print(\"Best parameter set based on F1-score:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c77477",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c379083",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of results saved to results/xgb-smote-threshold-summary.csv\n",
      "\n",
      "XGBoost with SMOTE and Threshold analysis completed.\n",
      "Best parameter set based on F1-score:\n",
      "pca__n_components    150.000000\n",
      "max_depth              3.000000\n",
      "learning_rate          0.300000\n",
      "n_estimators         100.000000\n",
      "threshold              0.100000\n",
      "accuracy               0.913370\n",
      "precision              0.725455\n",
      "recall                 0.800000\n",
      "f1                     0.751476\n",
      "mcc                    0.707661\n",
      "Name: 54, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "# Summarize and find best parameter combinations\n",
    "def summarize_and_find_best(df_results):\n",
    "    summary_df = df_results.groupby(['pca__n_components', 'max_depth', 'learning_rate', 'n_estimators', 'threshold']).agg({\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean',\n",
    "        'f1': 'mean',\n",
    "        'mcc': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Finding the best parameter set based on the highest F1 score\n",
    "    best_row = summary_df.loc[summary_df['f1'].idxmax()]\n",
    "\n",
    "    return summary_df, best_row\n",
    "\n",
    "# Combined SMOTE and Threshold-based XGB\n",
    "def XGBThreshold(dataPoints, y, outDir, n_splits, param_grid, thresholds):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics_per_combination = []\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        # Define pipeline with Vectorizer, SMOTE, PCA, and XGBoost\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', CountVectorizer(stop_words=None)),  # Vectorization step\n",
    "            ('pca', PCA(n_components=param_dict['pca__n_components'], random_state=42)),\n",
    "            ('xgb', xgb.XGBClassifier(\n",
    "                max_depth=param_dict['max_depth'],\n",
    "                learning_rate=param_dict['learning_rate'],\n",
    "                n_estimators=param_dict['n_estimators'],\n",
    "                eval_metric='logloss',  # Set eval metric to avoid warnings\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(dataPoints, y)):\n",
    "            X_train = [dataPoints[i] for i in train_index]\n",
    "            X_test = [dataPoints[i] for i in test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Predict probabilities\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # XGBoost probability for positive class\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "                precision = precision_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                recall = recall_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                f1 = f1_score(y_test, y_pred_threshold, zero_division=1)\n",
    "                mcc = matthews_corrcoef(y_test, y_pred_threshold)\n",
    "\n",
    "                # Store results\n",
    "                metrics_per_combination.append({\n",
    "                    'pca__n_components': param_dict['pca__n_components'],\n",
    "                    'max_depth': param_dict['max_depth'],\n",
    "                    'learning_rate': param_dict['learning_rate'],\n",
    "                    'n_estimators': param_dict['n_estimators'],\n",
    "                    'fold': fold + 1,\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'mcc': mcc\n",
    "                })\n",
    "\n",
    "    # Convert metrics to DataFrame and save results\n",
    "    df_results = pd.DataFrame(metrics_per_combination)\n",
    "    os.makedirs(outDir, exist_ok=True)\n",
    "    outFile = os.path.join(outDir, \"xgb-smote-threshold-results.csv\")\n",
    "    df_results.to_csv(outFile, index=False)\n",
    "\n",
    "    # Summarize results and return the best parameter set\n",
    "    summary_df, best_params = summarize_and_find_best(df_results)\n",
    "\n",
    "    # Save summarized results to a new CSV file\n",
    "    summary_outFile = os.path.join(outDir, \"xgb-smote-threshold-summary.csv\")\n",
    "    summary_df.to_csv(summary_outFile, index=False)\n",
    "    print(f\"Summary of results saved to {summary_outFile}\")\n",
    "\n",
    "    return df_results, best_params\n",
    "\n",
    "# Hyperparameters and thresholds\n",
    "param_grid = {\n",
    "    'pca__n_components': [150, 180, 200, 220],    # PCA components to use\n",
    "    'max_depth': [3, 5, 7, 10],                   # Max depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],       # Learning rate for boosting\n",
    "    'n_estimators': [100, 150, 200]               # Number of boosting rounds\n",
    "}\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)  # Thresholds from 0.1 to 0.9\n",
    "\n",
    "outDir = \"results/\"\n",
    "df_results, best_params = XGBThreshold(dataPoints, dataLabelsList, outDir, n_splits=5, param_grid=param_grid, thresholds=thresholds)\n",
    "\n",
    "print(\"\\nXGBoost with SMOTE and Threshold analysis completed.\")\n",
    "print(\"Best parameter set based on F1-score:\")\n",
    "print(best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
