{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a42537f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flaky documents (equal combination): 45\n",
      "Number of non-flaky documents (equal combination): 47\n",
      "Total number of documents (equal combination): 92\n",
      "Number of flaky documents (larger combination): 45\n",
      "Number of non-flaky documents (larger combination): 254\n",
      "Total number of documents (larger combination): 299\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "\n",
    "def extract_zip(zip_file, extract_to):\n",
    "    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "def getDataPoints(path):\n",
    "    \"\"\"Collects content of all .py files within the given directory.\"\"\"\n",
    "    dataPointsList = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory does not exist: {path}\")\n",
    "        return dataPointsList\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dataPointName in files:\n",
    "            if dataPointName.endswith(\".py\"):  # Only consider Python files\n",
    "                file_path = os.path.join(root, dataPointName)\n",
    "                with open(file_path, encoding=\"utf-8\") as fileIn:\n",
    "                    dp = fileIn.read().strip()\n",
    "                    if dp:  # Ensure the document is not empty\n",
    "                        dataPointsList.append(dp)\n",
    "                    else:\n",
    "                        print(f\"Empty file: {file_path}\")\n",
    "    \n",
    "    if len(dataPointsList) == 0:\n",
    "        print(f\"No valid documents found in directory: {path}\")\n",
    "    \n",
    "    return dataPointsList\n",
    "\n",
    "def flastVectorization(dataPoints):\n",
    "    \"\"\"Performs vectorization using CountVectorizer.\"\"\"\n",
    "    countVec = CountVectorizer(stop_words=None)  # No stop word removal\n",
    "    Z = countVec.fit_transform(dataPoints)\n",
    "    return Z\n",
    "\n",
    "###############################################################################\n",
    "# Data Extraction and Vectorization\n",
    "\n",
    "# Parameters setup\n",
    "flakyZip = \"compressedDataset/flaky_files.zip\"\n",
    "nonFlakyZip = \"compressedDataset/reduced_nonflaky_files.zip\"\n",
    "largerNonFlakyZip = \"compressedDataset/all_nonflaky_files.zip\"\n",
    "\n",
    "# Create directories\n",
    "outDirEqual = \"results/equal_flaky_nonflaky/\"\n",
    "outDirLarger = \"results/larger_nonflaky/\"\n",
    "os.makedirs(outDirEqual, exist_ok=True)\n",
    "os.makedirs(outDirLarger, exist_ok=True)\n",
    "\n",
    "extractDirEqual = \"extracted/equal_flaky_nonflaky/\"\n",
    "extractDirLarger = \"extracted/larger_nonflaky/\"\n",
    "os.makedirs(extractDirEqual, exist_ok=True)\n",
    "os.makedirs(extractDirLarger, exist_ok=True)\n",
    "\n",
    "# Extract and read data once for equal combination\n",
    "flakyDirEqual = os.path.join(extractDirEqual, 'flaky')\n",
    "nonFlakyDirEqual = os.path.join(extractDirEqual, 'nonFlaky')\n",
    "os.makedirs(flakyDirEqual, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirEqual, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirEqual)\n",
    "extract_zip(nonFlakyZip, nonFlakyDirEqual)\n",
    "\n",
    "dataPointsFlakyEqual = getDataPoints(flakyDirEqual)\n",
    "dataPointsNonFlakyEqual = getDataPoints(nonFlakyDirEqual)\n",
    "dataPointsEqual = dataPointsFlakyEqual + dataPointsNonFlakyEqual\n",
    "\n",
    "# Print the number of datasets for equal combination\n",
    "print(f\"Number of flaky documents (equal combination): {len(dataPointsFlakyEqual)}\")\n",
    "print(f\"Number of non-flaky documents (equal combination): {len(dataPointsNonFlakyEqual)}\")\n",
    "print(f\"Total number of documents (equal combination): {len(dataPointsEqual)}\")\n",
    "\n",
    "dataLabelsListEqual = np.array([1]*len(dataPointsFlakyEqual) + [0]*len(dataPointsNonFlakyEqual))\n",
    "\n",
    "# Vectorize data once\n",
    "Z_equal = flastVectorization(dataPointsEqual)\n",
    "\n",
    "# Extract and read data once for larger non-flaky combination\n",
    "flakyDirLarger = os.path.join(extractDirLarger, 'flaky')\n",
    "nonFlakyDirLarger = os.path.join(extractDirLarger, 'nonFlaky')\n",
    "os.makedirs(flakyDirLarger, exist_ok=True)\n",
    "os.makedirs(nonFlakyDirLarger, exist_ok=True)\n",
    "\n",
    "extract_zip(flakyZip, flakyDirLarger)\n",
    "extract_zip(largerNonFlakyZip, nonFlakyDirLarger)\n",
    "\n",
    "dataPointsFlakyLarger = getDataPoints(flakyDirLarger)\n",
    "dataPointsNonFlakyLarger = getDataPoints(nonFlakyDirLarger)\n",
    "dataPointsLarger = dataPointsFlakyLarger + dataPointsNonFlakyLarger\n",
    "\n",
    "# Print the number of datasets for larger combination\n",
    "print(f\"Number of flaky documents (larger combination): {len(dataPointsFlakyLarger)}\")\n",
    "print(f\"Number of non-flaky documents (larger combination): {len(dataPointsNonFlakyLarger)}\")\n",
    "print(f\"Total number of documents (larger combination): {len(dataPointsLarger)}\")\n",
    "\n",
    "dataLabelsListLarger = np.array([1]*len(dataPointsFlakyLarger) + [0]*len(dataPointsNonFlakyLarger))\n",
    "\n",
    "Z_larger = flastVectorization(dataPointsLarger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304d035",
   "metadata": {},
   "source": [
    "## KNN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "540d06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting KNN analysis for flaky vs smaller non-flaky files (equal combination)...\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 Score: 0.7811111111111111\n",
      "KNN analysis completed for 5-folds. Results saved to: equal-params-knn-5-folds.csv\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best F1 Score: 0.7119673436877738\n",
      "KNN analysis completed for 3-folds. Results saved to: equal-params-knn-3-folds.csv\n",
      "\n",
      "Best results for KNN 5-fold on equal combination:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 Score: 0.7811111111111111\n",
      "\n",
      "Best results for KNN 3-fold on equal combination:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best F1 Score: 0.7119673436877738\n",
      "\n",
      "Starting KNN analysis for flaky vs larger non-flaky files (larger combination)...\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 Score: 0.5696886446886447\n",
      "KNN analysis completed for 5-folds. Results saved to: larger-params-knn-5-folds.csv\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 Score: 0.5818070818070818\n",
      "KNN analysis completed for 3-folds. Results saved to: larger-params-knn-3-folds.csv\n",
      "\n",
      "Best results for KNN 5-fold on larger combination:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 Score: 0.5696886446886447\n",
      "\n",
      "Best results for KNN 3-fold on larger combination:\n",
      "Best Parameters: {'metric': 'cosine', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 Score: 0.5818070818070818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def runKNN(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "    \n",
    "    # Define the KNN model\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'metric': ['cosine', 'euclidean'],  # Distance metrics\n",
    "        'weights': ['uniform', 'distance'],  # Neighbor weighting schemes\n",
    "    }\n",
    "\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  \n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-knn-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"n_neighbors,metric,weights,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)  \n",
    "            fo.write(f\"{param['n_neighbors']},{param['metric']},{param['weights']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"KNN analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# Run KNN on equal combination\n",
    "print(\"\\nStarting KNN analysis for flaky vs smaller non-flaky files (equal combination)...\")\n",
    "best_params_5folds_knn_equal, best_score_5folds_knn_equal = runKNN(Z_equal, dataLabelsListEqual, outDirEqual, 5, \"equal\")\n",
    "best_params_3folds_knn_equal, best_score_3folds_knn_equal = runKNN(Z_equal, dataLabelsListEqual, outDirEqual, 3, \"equal\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for KNN 5-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_knn_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_knn_equal}\")\n",
    "\n",
    "print(\"\\nBest results for KNN 3-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_knn_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_knn_equal}\")\n",
    "\n",
    "# Run KNN on larger non-flaky combination\n",
    "print(\"\\nStarting KNN analysis for flaky vs larger non-flaky files (larger combination)...\")\n",
    "best_params_5folds_knn_larger, best_score_5folds_knn_larger = runKNN(Z_larger, dataLabelsListLarger, outDirLarger, 5, \"larger\")\n",
    "best_params_3folds_knn_larger, best_score_3folds_knn_larger = runKNN(Z_larger, dataLabelsListLarger, outDirLarger, 3, \"larger\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for KNN 5-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_knn_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_knn_larger}\")\n",
    "\n",
    "print(\"\\nBest results for KNN 3-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_knn_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_knn_larger}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1a814",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# SVM with GridSearchCV\n",
    "\n",
    "def runSVM(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define the SVM model\n",
    "    svm = SVC()\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions (without MCC)\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-svm-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"C,kernel,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)  \n",
    "            fo.write(f\"{param['C']},{param['kernel']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"SVM analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4436c1",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba4000e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SVM analysis for flaky vs smaller non-flaky files (equal combination)...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7786800334168754\n",
      "SVM analysis completed for 5-folds. Results saved to: equal-params-svm-5-folds.csv\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7776541836608577\n",
      "SVM analysis completed for 3-folds. Results saved to: equal-params-svm-3-folds.csv\n",
      "\n",
      "Best results for SVM 5-fold on equal combination:\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7786800334168754\n",
      "\n",
      "Best results for SVM 3-fold on equal combination:\n",
      "Best Parameters: {'C': 100.0, 'kernel': 'rbf'}\n",
      "Best F1 Score: 0.7776541836608577\n",
      "\n",
      "Starting SVM analysis for flaky vs larger non-flaky files (larger combination)...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.6994444444444444\n",
      "SVM analysis completed for 5-folds. Results saved to: larger-params-svm-5-folds.csv\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best Parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.6067620650953984\n",
      "SVM analysis completed for 3-folds. Results saved to: larger-params-svm-3-folds.csv\n",
      "\n",
      "Best results for SVM 5-fold on larger combination:\n",
      "Best Parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.6994444444444444\n",
      "\n",
      "Best results for SVM 3-fold on larger combination:\n",
      "Best Parameters: {'C': 0.01, 'kernel': 'linear'}\n",
      "Best F1 Score: 0.6067620650953984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def runSVM(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define the SVM model\n",
    "    svm = SVC()\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # Kernel types\n",
    "    }\n",
    "\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-svm-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"C,kernel,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)  \n",
    "            fo.write(f\"{param['C']},{param['kernel']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"SVM analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# Run SVM on equal combination\n",
    "print(\"\\nStarting SVM analysis for flaky vs smaller non-flaky files (equal combination)...\")\n",
    "best_params_5folds_svm_equal, best_score_5folds_svm_equal = runSVM(Z_equal, dataLabelsListEqual, outDirEqual, 5, \"equal\")\n",
    "best_params_3folds_svm_equal, best_score_3folds_svm_equal = runSVM(Z_equal, dataLabelsListEqual, outDirEqual, 3, \"equal\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for SVM 5-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_svm_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_svm_equal}\")\n",
    "\n",
    "print(\"\\nBest results for SVM 3-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_svm_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_svm_equal}\")\n",
    "\n",
    "# Run SVM on larger non-flaky combination\n",
    "print(\"\\nStarting SVM analysis for flaky vs larger non-flaky files (larger combination)...\")\n",
    "best_params_5folds_svm_larger, best_score_5folds_svm_larger = runSVM(Z_larger, dataLabelsListLarger, outDirLarger, 5, \"larger\")\n",
    "best_params_3folds_svm_larger, best_score_3folds_svm_larger = runSVM(Z_larger, dataLabelsListLarger, outDirLarger, 3, \"larger\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for SVM 5-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_svm_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_svm_larger}\")\n",
    "\n",
    "print(\"\\nBest results for SVM 3-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_svm_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_svm_larger}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2dd8f",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "554dd8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting NB analysis for flaky vs smaller non-flaky files (equal combination)...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'alpha': 0.001}\n",
      "Best F1 Score: 0.8187134502923976\n",
      "Naive Bayes analysis completed for 5-folds. Results saved to: equal-params-nb-5-folds.csv\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best Parameters: {'alpha': 0.001}\n",
      "Best F1 Score: 0.7354132457580733\n",
      "Naive Bayes analysis completed for 3-folds. Results saved to: equal-params-nb-3-folds.csv\n",
      "\n",
      "Best results for NB 5-fold on equal combination:\n",
      "Best Parameters: {'alpha': 0.001}\n",
      "Best F1 Score: 0.8187134502923976\n",
      "\n",
      "Best results for NB 3-fold on equal combination:\n",
      "Best Parameters: {'alpha': 0.001}\n",
      "Best F1 Score: 0.7354132457580733\n",
      "\n",
      "Starting NB analysis for flaky vs larger non-flaky files (larger combination)...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Parameters: {'alpha': 0.01}\n",
      "Best F1 Score: 0.7763575605680868\n",
      "Naive Bayes analysis completed for 5-folds. Results saved to: larger-params-nb-5-folds.csv\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best Parameters: {'alpha': 0.001}\n",
      "Best F1 Score: 0.7561973998755608\n",
      "Naive Bayes analysis completed for 3-folds. Results saved to: larger-params-nb-3-folds.csv\n",
      "\n",
      "Best results for NB 5-fold on larger combination:\n",
      "Best Parameters: {'alpha': 0.01}\n",
      "Best F1 Score: 0.7763575605680868\n",
      "\n",
      "Best results for NB 3-fold on larger combination:\n",
      "Best Parameters: {'alpha': 0.001}\n",
      "Best F1 Score: 0.7561973998755608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def runNB(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define the NB model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0], \n",
    "    }\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(nb, param_grid, cv=skf, scoring=scoring, refit='f1', verbose=1, return_train_score=True)\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for f1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-nb-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"alpha,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)  \n",
    "            fo.write(f\"{param['alpha']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"Naive Bayes analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# Run NB on equal combination\n",
    "print(\"\\nStarting NB analysis for flaky vs smaller non-flaky files (equal combination)...\")\n",
    "best_params_5folds_nb_equal, best_score_5folds_nb_equal = runNB(Z_equal, dataLabelsListEqual, outDirEqual, 5, \"equal\")\n",
    "best_params_3folds_nb_equal, best_score_3folds_nb_equal = runNB(Z_equal, dataLabelsListEqual, outDirEqual, 3, \"equal\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for NB 5-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_nb_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_nb_equal}\")\n",
    "\n",
    "print(\"\\nBest results for NB 3-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_nb_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_nb_equal}\")\n",
    "\n",
    "# Run NB on larger non-flaky combination\n",
    "print(\"\\nStarting NB analysis for flaky vs larger non-flaky files (larger combination)...\")\n",
    "best_params_5folds_nb_larger, best_score_5folds_nb_larger = runNB(Z_larger, dataLabelsListLarger, outDirLarger, 5, \"larger\")\n",
    "best_params_3folds_nb_larger, best_score_3folds_nb_larger = runNB(Z_larger, dataLabelsListLarger, outDirLarger, 3, \"larger\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for NB 5-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_nb_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_nb_larger}\")\n",
    "\n",
    "print(\"\\nBest results for NB 3-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_nb_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_nb_larger}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8185e97",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac1b2a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGBoost analysis for flaky vs smaller non-flaky files (equal combination)...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best Parameters: {'eta': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best F1 Score: 0.9342483660130719\n",
      "XGBoost analysis completed for 5-folds. Results saved to: equal-params-xgb-5-folds.csv\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best F1 Score: 0.9331797235023043\n",
      "XGBoost analysis completed for 3-folds. Results saved to: equal-params-xgb-3-folds.csv\n",
      "\n",
      "Best results for XGBoost 5-fold on equal combination:\n",
      "Best Parameters: {'eta': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best F1 Score: 0.9342483660130719\n",
      "\n",
      "Best results for XGBoost 3-fold on equal combination:\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best F1 Score: 0.9331797235023043\n",
      "\n",
      "Starting XGBoost analysis for flaky vs larger non-flaky files (larger combination)...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best Parameters: {'eta': 0.3, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best F1 Score: 0.8877089783281733\n",
      "XGBoost analysis completed for 5-folds. Results saved to: larger-params-xgb-5-folds.csv\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 5, 'n_estimators': 50}\n",
      "Best F1 Score: 0.8878713878713879\n",
      "XGBoost analysis completed for 3-folds. Results saved to: larger-params-xgb-3-folds.csv\n",
      "\n",
      "Best results for XGBoost 5-fold on larger combination:\n",
      "Best Parameters: {'eta': 0.3, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best F1 Score: 0.8877089783281733\n",
      "\n",
      "Best results for XGBoost 3-fold on larger combination:\n",
      "Best Parameters: {'eta': 0.5, 'max_depth': 5, 'n_estimators': 50}\n",
      "Best F1 Score: 0.8878713878713879\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def runXGB(Z, dataLabelsList, outDir, n_splits, combination_label):\n",
    "    v0 = time.perf_counter()\n",
    "\n",
    "    # Define XGBoost model\n",
    "    xgb_model = XGBClassifier(eval_metric=\"logloss\")\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'eta': [0.01, 0.1, 0.3, 0.5],  # Learning rate\n",
    "        'max_depth': [3, 5, 7, 10],    # Tree depth\n",
    "        'n_estimators': [50, 100, 200, 300],  # Number of boosting rounds\n",
    "    }\n",
    "\n",
    "    # Custom scoring functions (without MCC)\n",
    "    scoring = {\n",
    "        'precision': make_scorer(precision_score, zero_division=1),  # Handle undefined precision\n",
    "        'recall': make_scorer(recall_score, zero_division=1),\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score, zero_division=1),  # Handle undefined F1 score\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV with multiple scoring metrics\n",
    "    grid_search = GridSearchCV(\n",
    "        xgb_model, param_grid, cv=skf, scoring=scoring,\n",
    "        refit='f1', verbose=1, return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the GridSearchCV on training data\n",
    "    grid_search.fit(Z, dataLabelsList)\n",
    "\n",
    "    # Get the best parameters and the best score for F1\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_score}\")\n",
    "\n",
    "    # Save the results\n",
    "    outFile = f\"{combination_label}-params-xgb-{n_splits}-folds.csv\"\n",
    "    with open(os.path.join(outDir, outFile), \"w\") as fo:\n",
    "        fo.write(\"eta,max_depth,n_estimators,accuracy,precision,recall,f1,preparationTime\\n\")\n",
    "        for idx, param in enumerate(grid_search.cv_results_['params']):\n",
    "            accuracy = grid_search.cv_results_['mean_test_accuracy'][idx]\n",
    "            precision = grid_search.cv_results_['mean_test_precision'][idx]\n",
    "            recall = grid_search.cv_results_['mean_test_recall'][idx]\n",
    "            f1 = grid_search.cv_results_['mean_test_f1'][idx]\n",
    "            preparationTime = (time.perf_counter() - v0) / len(dataLabelsList)  \n",
    "            fo.write(f\"{param['eta']},{param['max_depth']},{param['n_estimators']},{accuracy},{precision},{recall},{f1},{preparationTime}\\n\")\n",
    "\n",
    "    print(f\"XGBoost analysis completed for {n_splits}-folds. Results saved to: {outFile}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# Run XGBoost on equal combination\n",
    "print(\"\\nStarting XGBoost analysis for flaky vs smaller non-flaky files (equal combination)...\")\n",
    "best_params_5folds_xgb_equal, best_score_5folds_xgb_equal = runXGB(Z_equal, dataLabelsListEqual, outDirEqual, 5, \"equal\")\n",
    "best_params_3folds_xgb_equal, best_score_3folds_xgb_equal = runXGB(Z_equal, dataLabelsListEqual, outDirEqual, 3, \"equal\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost 5-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_xgb_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_xgb_equal}\")\n",
    "\n",
    "print(\"\\nBest results for XGBoost 3-fold on equal combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_xgb_equal}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_xgb_equal}\")\n",
    "\n",
    "# Run XGBoost on larger non-flaky combination\n",
    "print(\"\\nStarting XGBoost analysis for flaky vs larger non-flaky files (larger combination)...\")\n",
    "best_params_5folds_xgb_larger, best_score_5folds_xgb_larger = runXGB(Z_larger, dataLabelsListLarger, outDirLarger, 5, \"larger\")\n",
    "best_params_3folds_xgb_larger, best_score_3folds_xgb_larger = runXGB(Z_larger, dataLabelsListLarger, outDirLarger, 3, \"larger\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBest results for XGBoost 5-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_5folds_xgb_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_5folds_xgb_larger}\")\n",
    "\n",
    "print(\"\\nBest results for XGBoost 3-fold on larger combination:\")\n",
    "print(f\"Best Parameters: {best_params_3folds_xgb_larger}\")\n",
    "print(f\"Best F1 Score: {best_score_3folds_xgb_larger}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f670af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
